{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Backpack Prediction Data Exploration\"\n",
    "format: html\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Material</th>\n",
       "      <th>Size</th>\n",
       "      <th>Compartments</th>\n",
       "      <th>Laptop Compartment</th>\n",
       "      <th>Waterproof</th>\n",
       "      <th>Style</th>\n",
       "      <th>Color</th>\n",
       "      <th>Weight Capacity (kg)</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Jansport</td>\n",
       "      <td>Leather</td>\n",
       "      <td>Medium</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Tote</td>\n",
       "      <td>Black</td>\n",
       "      <td>11.611723</td>\n",
       "      <td>112.15875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Jansport</td>\n",
       "      <td>Canvas</td>\n",
       "      <td>Small</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Messenger</td>\n",
       "      <td>Green</td>\n",
       "      <td>27.078537</td>\n",
       "      <td>68.88056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Under Armour</td>\n",
       "      <td>Leather</td>\n",
       "      <td>Small</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Messenger</td>\n",
       "      <td>Red</td>\n",
       "      <td>16.643760</td>\n",
       "      <td>39.17320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Nike</td>\n",
       "      <td>Nylon</td>\n",
       "      <td>Small</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Messenger</td>\n",
       "      <td>Green</td>\n",
       "      <td>12.937220</td>\n",
       "      <td>80.60793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Adidas</td>\n",
       "      <td>Canvas</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Messenger</td>\n",
       "      <td>Green</td>\n",
       "      <td>17.749338</td>\n",
       "      <td>86.02312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id         Brand Material    Size  Compartments Laptop Compartment  \\\n",
       "0   0      Jansport  Leather  Medium           7.0                Yes   \n",
       "1   1      Jansport   Canvas   Small          10.0                Yes   \n",
       "2   2  Under Armour  Leather   Small           2.0                Yes   \n",
       "3   3          Nike    Nylon   Small           8.0                Yes   \n",
       "4   4        Adidas   Canvas  Medium           1.0                Yes   \n",
       "\n",
       "  Waterproof      Style  Color  Weight Capacity (kg)      Price  \n",
       "0         No       Tote  Black             11.611723  112.15875  \n",
       "1        Yes  Messenger  Green             27.078537   68.88056  \n",
       "2         No  Messenger    Red             16.643760   39.17320  \n",
       "3         No  Messenger  Green             12.937220   80.60793  \n",
       "4        Yes  Messenger  Green             17.749338   86.02312  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Compartments</th>\n",
       "      <th>Weight Capacity (kg)</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.994318e+06</td>\n",
       "      <td>3.994318e+06</td>\n",
       "      <td>3.992510e+06</td>\n",
       "      <td>3.994318e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.182137e+06</td>\n",
       "      <td>5.434740e+00</td>\n",
       "      <td>1.801042e+01</td>\n",
       "      <td>8.136217e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.178058e+06</td>\n",
       "      <td>2.893043e+00</td>\n",
       "      <td>6.973969e+00</td>\n",
       "      <td>3.893868e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.500000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.198579e+06</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.206896e+01</td>\n",
       "      <td>4.747002e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.197158e+06</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.805436e+01</td>\n",
       "      <td>8.098495e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.195738e+06</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>2.398751e+01</td>\n",
       "      <td>1.148550e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.194317e+06</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>1.500000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  Compartments  Weight Capacity (kg)         Price\n",
       "count  3.994318e+06  3.994318e+06          3.992510e+06  3.994318e+06\n",
       "mean   2.182137e+06  5.434740e+00          1.801042e+01  8.136217e+01\n",
       "std    1.178058e+06  2.893043e+00          6.973969e+00  3.893868e+01\n",
       "min    0.000000e+00  1.000000e+00          5.000000e+00  1.500000e+01\n",
       "25%    1.198579e+06  3.000000e+00          1.206896e+01  4.747002e+01\n",
       "50%    2.197158e+06  5.000000e+00          1.805436e+01  8.098495e+01\n",
       "75%    3.195738e+06  8.000000e+00          2.398751e+01  1.148550e+02\n",
       "max    4.194317e+06  1.000000e+01          3.000000e+01  1.500000e+02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "Brand                   126758\n",
       "Material                110962\n",
       "Size                     87785\n",
       "Compartments                 0\n",
       "Laptop Compartment       98533\n",
       "Waterproof               94324\n",
       "Style                   104180\n",
       "Color                   133617\n",
       "Weight Capacity (kg)      1808\n",
       "Price                        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0       1       2 ... 4194315 4194316 4194317]\n",
      "int64\n",
      "['Jansport' 'Under Armour' 'Nike' 'Adidas' 'Puma' nan]\n",
      "object\n",
      "['Leather' 'Canvas' 'Nylon' nan 'Polyester']\n",
      "object\n",
      "['Medium' 'Small' 'Large' nan]\n",
      "object\n",
      "[ 7. 10.  2.  8.  1.  3.  5.  9.  6.  4.]\n",
      "float64\n",
      "['Yes' 'No' nan]\n",
      "object\n",
      "['No' 'Yes' nan]\n",
      "object\n",
      "['Tote' 'Messenger' nan 'Backpack']\n",
      "object\n",
      "['Black' 'Green' 'Red' 'Blue' 'Gray' 'Pink' nan]\n",
      "object\n",
      "[11.61172281 27.07853658 16.64375995 ... 12.79080004 22.95972519\n",
      " 16.64173875]\n",
      "float64\n",
      "[112.15875  68.88056  39.1732  ...  72.77859 100.96727 100.97298]\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "train1 = pd.read_csv(\"train.csv\")\n",
    "train2 = pd.read_csv(\"training_extra.csv\")\n",
    "train = pd.concat([train1, train2], ignore_index = True)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "display(train.head())\n",
    "display(train.describe())\n",
    "display(train.isna().sum())\n",
    "for col in train:\n",
    "    print(train[col].unique())\n",
    "    print(train[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "debug": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "class BackpackPriceDataset(Dataset):\n",
    "    def __init__(self, csv_files, test_mode=False) -> None:\n",
    "        self.device = torch.device('cpu')\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "        if isinstance(csv_files, str):\n",
    "            self.data = pd.read_csv(csv_files)\n",
    "        else:\n",
    "            dfs = [pd.read_csv(file) for file in csv_files]\n",
    "            self.data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "        cols_to_scale = ['Weight Capacity (kg)']\n",
    "        self.numerical_cols = ['Size', 'Compartments', 'Laptop Compartment', 'Waterproof', 'Weight Capacity (kg)']\n",
    "        self.categorical_cols = ['Brand', 'Material', 'Style', 'Color']\n",
    "\n",
    "        self._handle_missing_values()\n",
    "\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        for col in self.categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            self.data[col] = le.fit_transform(self.data[col])\n",
    "            self.label_encoders[col] = le\n",
    "\n",
    "        self.data[cols_to_scale] = self.scaler.fit_transform(self.data[cols_to_scale])\n",
    "\n",
    "        self.num_categories = {\n",
    "            col: len(self.data[col].unique()) for col in self.categorical_cols\n",
    "        }\n",
    "\n",
    "        drop_cols = ['id']\n",
    "        if not test_mode:\n",
    "            drop_cols.append('Price')\n",
    "            self.target = self.data['Price']\n",
    "\n",
    "        self.features = self.data.drop(drop_cols, axis=1)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return categorical and numerical seperately due to embedding of categorical\n",
    "        categorical = torch.tensor([\n",
    "            self.data[col].iloc[idx] for col in self.categorical_cols\n",
    "        ], dtype=torch.long)#.to(self.device)\n",
    "\n",
    "        numerical = torch.tensor([\n",
    "            self.data[col].iloc[idx] for col in self.numerical_cols\n",
    "        ], dtype=torch.float32)#.to(self.device)\n",
    "\n",
    "        if self.test_mode:\n",
    "            return categorical, numerical\n",
    "        else:\n",
    "            target = torch.tensor([self.target.iloc[idx]], dtype=torch.float32)#.to(self.device)\n",
    "            return categorical, numerical, target\n",
    "\n",
    "    def _handle_missing_values(self):\n",
    "        # https://medium.com/@felipecaballero/deciphering-the-cryptic-futurewarning-for-fillna-in-pandas-2-01deb4e411a1\n",
    "        with pd.option_context('future.no_silent_downcasting', True):\n",
    "            for col in self.categorical_cols:\n",
    "                self.data[col] = self.data[col].fillna(\"Missing\")\n",
    "        \n",
    "            self.data['Size'] = self.data['Size'].fillna(\"Missing\")\n",
    "            self.data['Size'] = self.data['Size'].replace({\n",
    "                'Small': -1,\n",
    "                'Medium': 0,\n",
    "                'Large': 1,\n",
    "                'Missing': 0  # Assume missing sizes are Medium\n",
    "            }).infer_objects()\n",
    "            \n",
    "            # Compartments (whole numbers)\n",
    "            median_compartments = round(self.data['Compartments'].median())\n",
    "            self.data['Compartments'] = self.data['Compartments'].fillna(median_compartments).infer_objects()\n",
    "            \n",
    "            # Weight Capacity (continuous)\n",
    "            self.data['Weight Capacity (kg)'] = self.data['Weight Capacity (kg)'].fillna(\n",
    "                self.data['Weight Capacity (kg)'].median()\n",
    "            )\n",
    "            \n",
    "            # Binary features (assume missing means \"No\")\n",
    "            for col in ['Laptop Compartment', 'Waterproof']:\n",
    "                self.data[col] = self.data[col].fillna(\"No\")\n",
    "                self.data[col] = self.data[col].replace({'No': 0, 'Yes': 1}).infer_objects()\n",
    "\n",
    "\n",
    "train_dataset = BackpackPriceDataset([\"train.csv\", \"training_extra.csv\"], test_mode=False)\n",
    "test_dataset = BackpackPriceDataset(\"test.csv\", test_mode=True)\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                feature  importance\n",
      "8  Weight Capacity (kg)    0.515199\n",
      "3          Compartments    0.111563\n",
      "7                 Color    0.089124\n",
      "0                 Brand    0.075244\n",
      "1              Material    0.058089\n",
      "6                 Style    0.052382\n",
      "2                  Size    0.046405\n",
      "5            Waterproof    0.026255\n",
      "4    Laptop Compartment    0.025739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np \n",
    "\n",
    "train2 = BackpackPriceDataset(\"train.csv\", test_mode=False)\n",
    "X, y = train2.features, train2.target\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=random_seed, max_depth=10)\n",
    "rf.fit(X,y)\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand: 6. Recommended embedding_dim: 3\n",
      "Material: 5. Recommended embedding_dim: 2\n",
      "Style: 4. Recommended embedding_dim: 2\n",
      "Color: 7. Recommended embedding_dim: 3\n"
     ]
    }
   ],
   "source": [
    "for col, count in train_dataset.num_categories.items():\n",
    "    rec_dim = min(50, count//2)\n",
    "    print(f\"{col}: {count}. Recommended embedding_dim: {rec_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BackpackPriceNet(nn.Module):\n",
    "    def __init__(self, num_categories_dict) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            'Brand': nn.Embedding(num_categories_dict['Brand'], 3),\n",
    "            'Material': nn.Embedding(num_categories_dict['Material'], 2),\n",
    "            'Style': nn.Embedding(num_categories_dict['Style'], 2),\n",
    "            'Color': nn.Embedding(num_categories_dict['Color'], 3)\n",
    "        })\n",
    "\n",
    "        embedding_dim = 3 + 2 + 2 + 3\n",
    "        numerical_dim = 5\n",
    "        input_dim = embedding_dim + numerical_dim\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(64, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, categorical_inputs, numerical_inputs):\n",
    "        embeddings = []\n",
    "        for i, (_, embedding_layer) in enumerate(self.embeddings.items()):\n",
    "            embedding = embedding_layer(categorical_inputs[:, i])\n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        x_cat = torch.cat(embeddings, dim=1)\n",
    "        x = torch.cat([x_cat, numerical_inputs], dim=1)\n",
    "\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def train_loop(dataloader: DataLoader, model: BackpackPriceNet, loss_fn, optimizer):\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch, (cat_features, num_features, target) in enumerate(dataloader):\n",
    "        pred = model(cat_features, num_features)\n",
    "        loss = loss_fn(pred, target) \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Training batch loss: {loss.item():>7f} [{batch:>5d}/{num_batches:>5d}]\")\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def val_loop(dataloader: DataLoader, model: BackpackPriceNet, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (cat_features, num_features, target) in enumerate(dataloader):\n",
    "            pred = model(cat_features, num_features)\n",
    "            val_loss += loss_fn(pred, target).item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    print(f\"\\nValdidation average loss: {val_loss:>8f}\\n\")\n",
    "    return val_loss\n",
    "\n",
    "def predict(dataloader: DataLoader, model: BackpackPriceNet):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cat_features, num_features in dataloader:\n",
    "            outputs = model(cat_features, num_features)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------------\n",
      "Training batch loss: 7435.022461 [    0/24965]\n",
      "Training batch loss: 8452.383789 [  100/24965]\n",
      "Training batch loss: 8118.629883 [  200/24965]\n",
      "Training batch loss: 8527.283203 [  300/24965]\n",
      "Training batch loss: 7577.905762 [  400/24965]\n",
      "Training batch loss: 9326.549805 [  500/24965]\n",
      "Training batch loss: 7728.471680 [  600/24965]\n",
      "Training batch loss: 7010.750488 [  700/24965]\n",
      "Training batch loss: 7773.871582 [  800/24965]\n",
      "Training batch loss: 8372.834961 [  900/24965]\n",
      "Training batch loss: 8770.481445 [ 1000/24965]\n",
      "Training batch loss: 8973.727539 [ 1100/24965]\n",
      "Training batch loss: 8109.026367 [ 1200/24965]\n",
      "Training batch loss: 7710.382812 [ 1300/24965]\n",
      "Training batch loss: 7395.491699 [ 1400/24965]\n",
      "Training batch loss: 7229.483887 [ 1500/24965]\n",
      "Training batch loss: 8430.384766 [ 1600/24965]\n",
      "Training batch loss: 8327.617188 [ 1700/24965]\n",
      "Training batch loss: 7625.402344 [ 1800/24965]\n",
      "Training batch loss: 7368.419922 [ 1900/24965]\n",
      "Training batch loss: 8170.317871 [ 2000/24965]\n",
      "Training batch loss: 7528.213379 [ 2100/24965]\n",
      "Training batch loss: 8024.295898 [ 2200/24965]\n",
      "Training batch loss: 7880.483887 [ 2300/24965]\n",
      "Training batch loss: 7729.218750 [ 2400/24965]\n",
      "Training batch loss: 8520.369141 [ 2500/24965]\n",
      "Training batch loss: 8308.625977 [ 2600/24965]\n",
      "Training batch loss: 6707.253906 [ 2700/24965]\n",
      "Training batch loss: 7599.891602 [ 2800/24965]\n",
      "Training batch loss: 8113.275879 [ 2900/24965]\n",
      "Training batch loss: 8787.597656 [ 3000/24965]\n",
      "Training batch loss: 8028.877441 [ 3100/24965]\n",
      "Training batch loss: 8053.670410 [ 3200/24965]\n",
      "Training batch loss: 6464.199219 [ 3300/24965]\n",
      "Training batch loss: 8302.884766 [ 3400/24965]\n",
      "Training batch loss: 7322.203613 [ 3500/24965]\n",
      "Training batch loss: 7712.575684 [ 3600/24965]\n",
      "Training batch loss: 6996.982422 [ 3700/24965]\n",
      "Training batch loss: 7406.141113 [ 3800/24965]\n",
      "Training batch loss: 8420.834961 [ 3900/24965]\n",
      "Training batch loss: 7495.597656 [ 4000/24965]\n",
      "Training batch loss: 7192.513672 [ 4100/24965]\n",
      "Training batch loss: 7231.454590 [ 4200/24965]\n",
      "Training batch loss: 7604.793457 [ 4300/24965]\n",
      "Training batch loss: 7323.498047 [ 4400/24965]\n",
      "Training batch loss: 6987.618164 [ 4500/24965]\n",
      "Training batch loss: 7308.202148 [ 4600/24965]\n",
      "Training batch loss: 6728.934570 [ 4700/24965]\n",
      "Training batch loss: 7579.995605 [ 4800/24965]\n",
      "Training batch loss: 6612.506836 [ 4900/24965]\n",
      "Training batch loss: 6828.201172 [ 5000/24965]\n",
      "Training batch loss: 6657.339844 [ 5100/24965]\n",
      "Training batch loss: 7205.987305 [ 5200/24965]\n",
      "Training batch loss: 7686.242676 [ 5300/24965]\n",
      "Training batch loss: 6867.727539 [ 5400/24965]\n",
      "Training batch loss: 6281.636719 [ 5500/24965]\n",
      "Training batch loss: 6881.946289 [ 5600/24965]\n",
      "Training batch loss: 6936.176270 [ 5700/24965]\n",
      "Training batch loss: 5927.309082 [ 5800/24965]\n",
      "Training batch loss: 5624.998047 [ 5900/24965]\n",
      "Training batch loss: 7000.063477 [ 6000/24965]\n",
      "Training batch loss: 6816.309570 [ 6100/24965]\n",
      "Training batch loss: 5783.600586 [ 6200/24965]\n",
      "Training batch loss: 6261.660156 [ 6300/24965]\n",
      "Training batch loss: 6702.200684 [ 6400/24965]\n",
      "Training batch loss: 6594.131348 [ 6500/24965]\n",
      "Training batch loss: 6821.526367 [ 6600/24965]\n",
      "Training batch loss: 6506.815918 [ 6700/24965]\n",
      "Training batch loss: 6293.863281 [ 6800/24965]\n",
      "Training batch loss: 6268.664551 [ 6900/24965]\n",
      "Training batch loss: 6094.226074 [ 7000/24965]\n",
      "Training batch loss: 5823.869141 [ 7100/24965]\n",
      "Training batch loss: 6587.831543 [ 7200/24965]\n",
      "Training batch loss: 6593.679199 [ 7300/24965]\n",
      "Training batch loss: 6129.944336 [ 7400/24965]\n",
      "Training batch loss: 6226.636230 [ 7500/24965]\n",
      "Training batch loss: 6208.337891 [ 7600/24965]\n",
      "Training batch loss: 7331.869629 [ 7700/24965]\n",
      "Training batch loss: 6292.024414 [ 7800/24965]\n",
      "Training batch loss: 5572.179199 [ 7900/24965]\n",
      "Training batch loss: 6536.908203 [ 8000/24965]\n",
      "Training batch loss: 5744.542969 [ 8100/24965]\n",
      "Training batch loss: 6229.397461 [ 8200/24965]\n",
      "Training batch loss: 5733.107422 [ 8300/24965]\n",
      "Training batch loss: 5605.360840 [ 8400/24965]\n",
      "Training batch loss: 6186.767578 [ 8500/24965]\n",
      "Training batch loss: 5174.506348 [ 8600/24965]\n",
      "Training batch loss: 5086.446777 [ 8700/24965]\n",
      "Training batch loss: 6065.295410 [ 8800/24965]\n",
      "Training batch loss: 5840.186523 [ 8900/24965]\n",
      "Training batch loss: 4871.253418 [ 9000/24965]\n",
      "Training batch loss: 5355.317383 [ 9100/24965]\n",
      "Training batch loss: 4894.167969 [ 9200/24965]\n",
      "Training batch loss: 4651.781738 [ 9300/24965]\n",
      "Training batch loss: 5083.891113 [ 9400/24965]\n",
      "Training batch loss: 6644.971191 [ 9500/24965]\n",
      "Training batch loss: 5138.472168 [ 9600/24965]\n",
      "Training batch loss: 4786.605469 [ 9700/24965]\n",
      "Training batch loss: 5601.994141 [ 9800/24965]\n",
      "Training batch loss: 4650.578613 [ 9900/24965]\n",
      "Training batch loss: 4956.455078 [10000/24965]\n",
      "Training batch loss: 5033.866699 [10100/24965]\n",
      "Training batch loss: 4787.102539 [10200/24965]\n",
      "Training batch loss: 4434.881348 [10300/24965]\n",
      "Training batch loss: 4981.665039 [10400/24965]\n",
      "Training batch loss: 5052.101562 [10500/24965]\n",
      "Training batch loss: 4688.866211 [10600/24965]\n",
      "Training batch loss: 4599.696289 [10700/24965]\n",
      "Training batch loss: 5034.124023 [10800/24965]\n",
      "Training batch loss: 4641.501953 [10900/24965]\n",
      "Training batch loss: 4859.000977 [11000/24965]\n",
      "Training batch loss: 4111.762207 [11100/24965]\n",
      "Training batch loss: 4266.723145 [11200/24965]\n",
      "Training batch loss: 4041.819336 [11300/24965]\n",
      "Training batch loss: 4847.109863 [11400/24965]\n",
      "Training batch loss: 4552.937500 [11500/24965]\n",
      "Training batch loss: 4302.255859 [11600/24965]\n",
      "Training batch loss: 4274.878418 [11700/24965]\n",
      "Training batch loss: 4063.153320 [11800/24965]\n",
      "Training batch loss: 3778.566895 [11900/24965]\n",
      "Training batch loss: 4469.604980 [12000/24965]\n",
      "Training batch loss: 4066.177490 [12100/24965]\n",
      "Training batch loss: 4237.058594 [12200/24965]\n",
      "Training batch loss: 4658.618652 [12300/24965]\n",
      "Training batch loss: 3970.314941 [12400/24965]\n",
      "Training batch loss: 3684.222656 [12500/24965]\n",
      "Training batch loss: 3248.855469 [12600/24965]\n",
      "Training batch loss: 3696.178711 [12700/24965]\n",
      "Training batch loss: 3691.850586 [12800/24965]\n",
      "Training batch loss: 3948.722168 [12900/24965]\n",
      "Training batch loss: 4122.716797 [13000/24965]\n",
      "Training batch loss: 3612.547363 [13100/24965]\n",
      "Training batch loss: 3517.464355 [13200/24965]\n",
      "Training batch loss: 3760.777344 [13300/24965]\n",
      "Training batch loss: 3910.476562 [13400/24965]\n",
      "Training batch loss: 3491.138672 [13500/24965]\n",
      "Training batch loss: 2660.828125 [13600/24965]\n",
      "Training batch loss: 3219.778076 [13700/24965]\n",
      "Training batch loss: 2713.647949 [13800/24965]\n",
      "Training batch loss: 3374.115234 [13900/24965]\n",
      "Training batch loss: 3082.063477 [14000/24965]\n",
      "Training batch loss: 2647.747314 [14100/24965]\n",
      "Training batch loss: 3085.165283 [14200/24965]\n",
      "Training batch loss: 3172.530273 [14300/24965]\n",
      "Training batch loss: 2657.226807 [14400/24965]\n",
      "Training batch loss: 3074.318604 [14500/24965]\n",
      "Training batch loss: 2875.039062 [14600/24965]\n",
      "Training batch loss: 2673.126953 [14700/24965]\n",
      "Training batch loss: 3137.571777 [14800/24965]\n",
      "Training batch loss: 2459.619141 [14900/24965]\n",
      "Training batch loss: 2724.320312 [15000/24965]\n",
      "Training batch loss: 2800.537109 [15100/24965]\n",
      "Training batch loss: 2746.443848 [15200/24965]\n",
      "Training batch loss: 3002.523926 [15300/24965]\n",
      "Training batch loss: 3307.472656 [15400/24965]\n",
      "Training batch loss: 2408.033447 [15500/24965]\n",
      "Training batch loss: 2376.043945 [15600/24965]\n",
      "Training batch loss: 2344.961670 [15700/24965]\n",
      "Training batch loss: 2840.270264 [15800/24965]\n",
      "Training batch loss: 2271.296875 [15900/24965]\n",
      "Training batch loss: 2881.432129 [16000/24965]\n",
      "Training batch loss: 1885.870239 [16100/24965]\n",
      "Training batch loss: 2593.366455 [16200/24965]\n",
      "Training batch loss: 2608.889404 [16300/24965]\n",
      "Training batch loss: 2371.062744 [16400/24965]\n",
      "Training batch loss: 2269.849365 [16500/24965]\n",
      "Training batch loss: 2190.389404 [16600/24965]\n",
      "Training batch loss: 2246.935791 [16700/24965]\n",
      "Training batch loss: 2107.090088 [16800/24965]\n",
      "Training batch loss: 2467.043213 [16900/24965]\n",
      "Training batch loss: 1997.855713 [17000/24965]\n",
      "Training batch loss: 2223.691895 [17100/24965]\n",
      "Training batch loss: 2010.448730 [17200/24965]\n",
      "Training batch loss: 2027.210083 [17300/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 2223.137207 [17400/24965]\n",
      "Training batch loss: 2066.749512 [17500/24965]\n",
      "Training batch loss: 1767.474121 [17600/24965]\n",
      "Training batch loss: 1879.516113 [17700/24965]\n",
      "Training batch loss: 2035.404297 [17800/24965]\n",
      "Training batch loss: 2177.940918 [17900/24965]\n",
      "Training batch loss: 1923.640259 [18000/24965]\n",
      "Training batch loss: 1957.852051 [18100/24965]\n",
      "Training batch loss: 1836.286621 [18200/24965]\n",
      "Training batch loss: 1663.052612 [18300/24965]\n",
      "Training batch loss: 1961.281860 [18400/24965]\n",
      "Training batch loss: 1613.505005 [18500/24965]\n",
      "Training batch loss: 2192.770020 [18600/24965]\n",
      "Training batch loss: 1799.018433 [18700/24965]\n",
      "Training batch loss: 1800.094849 [18800/24965]\n",
      "Training batch loss: 2074.854980 [18900/24965]\n",
      "Training batch loss: 1850.255249 [19000/24965]\n",
      "Training batch loss: 1687.624023 [19100/24965]\n",
      "Training batch loss: 1703.998291 [19200/24965]\n",
      "Training batch loss: 1706.073608 [19300/24965]\n",
      "Training batch loss: 1730.828857 [19400/24965]\n",
      "Training batch loss: 1614.959229 [19500/24965]\n",
      "Training batch loss: 1628.464966 [19600/24965]\n",
      "Training batch loss: 1550.143799 [19700/24965]\n",
      "Training batch loss: 1375.078125 [19800/24965]\n",
      "Training batch loss: 1925.305176 [19900/24965]\n",
      "Training batch loss: 1513.203125 [20000/24965]\n",
      "Training batch loss: 1543.433838 [20100/24965]\n",
      "Training batch loss: 1578.069702 [20200/24965]\n",
      "Training batch loss: 1670.769897 [20300/24965]\n",
      "Training batch loss: 1599.220825 [20400/24965]\n",
      "Training batch loss: 1537.671875 [20500/24965]\n",
      "Training batch loss: 1728.210205 [20600/24965]\n",
      "Training batch loss: 1541.508545 [20700/24965]\n",
      "Training batch loss: 1654.829590 [20800/24965]\n",
      "Training batch loss: 1562.361572 [20900/24965]\n",
      "Training batch loss: 1590.196533 [21000/24965]\n",
      "Training batch loss: 1928.038208 [21100/24965]\n",
      "Training batch loss: 1460.949707 [21200/24965]\n",
      "Training batch loss: 1386.693481 [21300/24965]\n",
      "Training batch loss: 1548.062256 [21400/24965]\n",
      "Training batch loss: 1474.112549 [21500/24965]\n",
      "Training batch loss: 1455.183716 [21600/24965]\n",
      "Training batch loss: 1542.281372 [21700/24965]\n",
      "Training batch loss: 1713.055908 [21800/24965]\n",
      "Training batch loss: 1512.687134 [21900/24965]\n",
      "Training batch loss: 1509.715454 [22000/24965]\n",
      "Training batch loss: 1184.116455 [22100/24965]\n",
      "Training batch loss: 1313.753662 [22200/24965]\n",
      "Training batch loss: 1526.917969 [22300/24965]\n",
      "Training batch loss: 1686.042725 [22400/24965]\n",
      "Training batch loss: 1634.064941 [22500/24965]\n",
      "Training batch loss: 1714.541016 [22600/24965]\n",
      "Training batch loss: 1615.517822 [22700/24965]\n",
      "Training batch loss: 1591.553345 [22800/24965]\n",
      "Training batch loss: 1515.705322 [22900/24965]\n",
      "Training batch loss: 1768.566040 [23000/24965]\n",
      "Training batch loss: 1827.345947 [23100/24965]\n",
      "Training batch loss: 1559.815674 [23200/24965]\n",
      "Training batch loss: 1681.955566 [23300/24965]\n",
      "Training batch loss: 1607.219482 [23400/24965]\n",
      "Training batch loss: 1385.120117 [23500/24965]\n",
      "Training batch loss: 1628.374756 [23600/24965]\n",
      "Training batch loss: 1295.578491 [23700/24965]\n",
      "Training batch loss: 1595.420654 [23800/24965]\n",
      "Training batch loss: 1407.832153 [23900/24965]\n",
      "Training batch loss: 1588.565186 [24000/24965]\n",
      "Training batch loss: 1725.672729 [24100/24965]\n",
      "Training batch loss: 1351.939453 [24200/24965]\n",
      "Training batch loss: 1640.877563 [24300/24965]\n",
      "Training batch loss: 1558.655640 [24400/24965]\n",
      "Training batch loss: 1458.311646 [24500/24965]\n",
      "Training batch loss: 1612.542725 [24600/24965]\n",
      "Training batch loss: 1559.010986 [24700/24965]\n",
      "Training batch loss: 1611.645630 [24800/24965]\n",
      "Training batch loss: 1406.977417 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1514.204642\n",
      "\n",
      "Saved best new model with val_loss: 1514.2046\n",
      "Epoch 2\n",
      "----------------------\n",
      "Training batch loss: 1557.215210 [    0/24965]\n",
      "Training batch loss: 1749.424438 [  100/24965]\n",
      "Training batch loss: 1785.719360 [  200/24965]\n",
      "Training batch loss: 1707.666016 [  300/24965]\n",
      "Training batch loss: 1563.741455 [  400/24965]\n",
      "Training batch loss: 1602.017456 [  500/24965]\n",
      "Training batch loss: 1529.143555 [  600/24965]\n",
      "Training batch loss: 1227.383667 [  700/24965]\n",
      "Training batch loss: 1548.670532 [  800/24965]\n",
      "Training batch loss: 1408.971191 [  900/24965]\n",
      "Training batch loss: 1616.918579 [ 1000/24965]\n",
      "Training batch loss: 1389.925537 [ 1100/24965]\n",
      "Training batch loss: 1518.412354 [ 1200/24965]\n",
      "Training batch loss: 1363.275391 [ 1300/24965]\n",
      "Training batch loss: 1517.446289 [ 1400/24965]\n",
      "Training batch loss: 1572.437744 [ 1500/24965]\n",
      "Training batch loss: 1630.973755 [ 1600/24965]\n",
      "Training batch loss: 1683.664062 [ 1700/24965]\n",
      "Training batch loss: 1608.022827 [ 1800/24965]\n",
      "Training batch loss: 1555.148926 [ 1900/24965]\n",
      "Training batch loss: 1488.436279 [ 2000/24965]\n",
      "Training batch loss: 1778.309570 [ 2100/24965]\n",
      "Training batch loss: 1553.687500 [ 2200/24965]\n",
      "Training batch loss: 1593.112305 [ 2300/24965]\n",
      "Training batch loss: 1580.172607 [ 2400/24965]\n",
      "Training batch loss: 1692.918701 [ 2500/24965]\n",
      "Training batch loss: 1524.082886 [ 2600/24965]\n",
      "Training batch loss: 1486.620972 [ 2700/24965]\n",
      "Training batch loss: 1491.626587 [ 2800/24965]\n",
      "Training batch loss: 1214.827148 [ 2900/24965]\n",
      "Training batch loss: 1504.901733 [ 3000/24965]\n",
      "Training batch loss: 1608.682129 [ 3100/24965]\n",
      "Training batch loss: 1717.864136 [ 3200/24965]\n",
      "Training batch loss: 1519.421143 [ 3300/24965]\n",
      "Training batch loss: 1727.245728 [ 3400/24965]\n",
      "Training batch loss: 1625.915894 [ 3500/24965]\n",
      "Training batch loss: 1461.338501 [ 3600/24965]\n",
      "Training batch loss: 1327.779663 [ 3700/24965]\n",
      "Training batch loss: 1465.228149 [ 3800/24965]\n",
      "Training batch loss: 1617.199585 [ 3900/24965]\n",
      "Training batch loss: 1717.484131 [ 4000/24965]\n",
      "Training batch loss: 1361.678711 [ 4100/24965]\n",
      "Training batch loss: 1392.748779 [ 4200/24965]\n",
      "Training batch loss: 1537.103394 [ 4300/24965]\n",
      "Training batch loss: 1445.687012 [ 4400/24965]\n",
      "Training batch loss: 1327.968018 [ 4500/24965]\n",
      "Training batch loss: 1937.234863 [ 4600/24965]\n",
      "Training batch loss: 1541.638428 [ 4700/24965]\n",
      "Training batch loss: 1440.167725 [ 4800/24965]\n",
      "Training batch loss: 1564.771362 [ 4900/24965]\n",
      "Training batch loss: 1396.573975 [ 5000/24965]\n",
      "Training batch loss: 1554.390869 [ 5100/24965]\n",
      "Training batch loss: 1601.277588 [ 5200/24965]\n",
      "Training batch loss: 1487.019775 [ 5300/24965]\n",
      "Training batch loss: 1584.110107 [ 5400/24965]\n",
      "Training batch loss: 1712.478882 [ 5500/24965]\n",
      "Training batch loss: 1392.635986 [ 5600/24965]\n",
      "Training batch loss: 1489.819336 [ 5700/24965]\n",
      "Training batch loss: 1515.480835 [ 5800/24965]\n",
      "Training batch loss: 1421.113770 [ 5900/24965]\n",
      "Training batch loss: 1451.496826 [ 6000/24965]\n",
      "Training batch loss: 1580.497314 [ 6100/24965]\n",
      "Training batch loss: 1638.877563 [ 6200/24965]\n",
      "Training batch loss: 1599.819702 [ 6300/24965]\n",
      "Training batch loss: 1572.076660 [ 6400/24965]\n",
      "Training batch loss: 1362.006592 [ 6500/24965]\n",
      "Training batch loss: 1284.431396 [ 6600/24965]\n",
      "Training batch loss: 1535.425903 [ 6700/24965]\n",
      "Training batch loss: 1692.475586 [ 6800/24965]\n",
      "Training batch loss: 1384.513672 [ 6900/24965]\n",
      "Training batch loss: 1490.929688 [ 7000/24965]\n",
      "Training batch loss: 1634.413940 [ 7100/24965]\n",
      "Training batch loss: 1579.603638 [ 7200/24965]\n",
      "Training batch loss: 1657.778076 [ 7300/24965]\n",
      "Training batch loss: 1529.129639 [ 7400/24965]\n",
      "Training batch loss: 1749.892212 [ 7500/24965]\n",
      "Training batch loss: 1375.112183 [ 7600/24965]\n",
      "Training batch loss: 1208.188843 [ 7700/24965]\n",
      "Training batch loss: 1522.452148 [ 7800/24965]\n",
      "Training batch loss: 1597.764893 [ 7900/24965]\n",
      "Training batch loss: 1755.524170 [ 8000/24965]\n",
      "Training batch loss: 1655.835205 [ 8100/24965]\n",
      "Training batch loss: 1566.443237 [ 8200/24965]\n",
      "Training batch loss: 1554.843140 [ 8300/24965]\n",
      "Training batch loss: 1547.074951 [ 8400/24965]\n",
      "Training batch loss: 1722.041870 [ 8500/24965]\n",
      "Training batch loss: 1445.581787 [ 8600/24965]\n",
      "Training batch loss: 1188.486572 [ 8700/24965]\n",
      "Training batch loss: 1680.768677 [ 8800/24965]\n",
      "Training batch loss: 1442.741089 [ 8900/24965]\n",
      "Training batch loss: 1653.533936 [ 9000/24965]\n",
      "Training batch loss: 1591.572876 [ 9100/24965]\n",
      "Training batch loss: 1500.895264 [ 9200/24965]\n",
      "Training batch loss: 1410.088379 [ 9300/24965]\n",
      "Training batch loss: 1672.681763 [ 9400/24965]\n",
      "Training batch loss: 1667.165283 [ 9500/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1654.461304 [ 9600/24965]\n",
      "Training batch loss: 1379.119019 [ 9700/24965]\n",
      "Training batch loss: 1451.369141 [ 9800/24965]\n",
      "Training batch loss: 1524.604004 [ 9900/24965]\n",
      "Training batch loss: 1510.420166 [10000/24965]\n",
      "Training batch loss: 1439.000244 [10100/24965]\n",
      "Training batch loss: 1597.928955 [10200/24965]\n",
      "Training batch loss: 1296.024292 [10300/24965]\n",
      "Training batch loss: 1433.372070 [10400/24965]\n",
      "Training batch loss: 1430.522461 [10500/24965]\n",
      "Training batch loss: 1419.919678 [10600/24965]\n",
      "Training batch loss: 1647.433350 [10700/24965]\n",
      "Training batch loss: 1424.681641 [10800/24965]\n",
      "Training batch loss: 1388.713501 [10900/24965]\n",
      "Training batch loss: 1591.324097 [11000/24965]\n",
      "Training batch loss: 1620.975098 [11100/24965]\n",
      "Training batch loss: 1554.707031 [11200/24965]\n",
      "Training batch loss: 1632.086670 [11300/24965]\n",
      "Training batch loss: 1386.722168 [11400/24965]\n",
      "Training batch loss: 1449.629883 [11500/24965]\n",
      "Training batch loss: 1555.185791 [11600/24965]\n",
      "Training batch loss: 1642.624512 [11700/24965]\n",
      "Training batch loss: 1610.734985 [11800/24965]\n",
      "Training batch loss: 1304.570312 [11900/24965]\n",
      "Training batch loss: 1467.343506 [12000/24965]\n",
      "Training batch loss: 1663.573242 [12100/24965]\n",
      "Training batch loss: 1620.804810 [12200/24965]\n",
      "Training batch loss: 1489.438965 [12300/24965]\n",
      "Training batch loss: 1620.264893 [12400/24965]\n",
      "Training batch loss: 1576.145996 [12500/24965]\n",
      "Training batch loss: 1499.803833 [12600/24965]\n",
      "Training batch loss: 1408.693481 [12700/24965]\n",
      "Training batch loss: 1431.352905 [12800/24965]\n",
      "Training batch loss: 1430.911133 [12900/24965]\n",
      "Training batch loss: 1662.013306 [13000/24965]\n",
      "Training batch loss: 1690.648438 [13100/24965]\n",
      "Training batch loss: 1595.905151 [13200/24965]\n",
      "Training batch loss: 1395.054199 [13300/24965]\n",
      "Training batch loss: 1396.296631 [13400/24965]\n",
      "Training batch loss: 1693.128906 [13500/24965]\n",
      "Training batch loss: 1440.268799 [13600/24965]\n",
      "Training batch loss: 1388.569824 [13700/24965]\n",
      "Training batch loss: 1522.701660 [13800/24965]\n",
      "Training batch loss: 1530.895996 [13900/24965]\n",
      "Training batch loss: 1484.531250 [14000/24965]\n",
      "Training batch loss: 1663.186279 [14100/24965]\n",
      "Training batch loss: 1722.844727 [14200/24965]\n",
      "Training batch loss: 1476.051147 [14300/24965]\n",
      "Training batch loss: 1474.153076 [14400/24965]\n",
      "Training batch loss: 1554.547852 [14500/24965]\n",
      "Training batch loss: 1654.155273 [14600/24965]\n",
      "Training batch loss: 1633.464355 [14700/24965]\n",
      "Training batch loss: 1631.639893 [14800/24965]\n",
      "Training batch loss: 1657.137939 [14900/24965]\n",
      "Training batch loss: 1712.484985 [15000/24965]\n",
      "Training batch loss: 1492.044312 [15100/24965]\n",
      "Training batch loss: 1661.788086 [15200/24965]\n",
      "Training batch loss: 1614.123779 [15300/24965]\n",
      "Training batch loss: 1551.441162 [15400/24965]\n",
      "Training batch loss: 1588.357056 [15500/24965]\n",
      "Training batch loss: 1368.380859 [15600/24965]\n",
      "Training batch loss: 1812.856934 [15700/24965]\n",
      "Training batch loss: 1661.174561 [15800/24965]\n",
      "Training batch loss: 1495.811035 [15900/24965]\n",
      "Training batch loss: 1377.861084 [16000/24965]\n",
      "Training batch loss: 1323.148438 [16100/24965]\n",
      "Training batch loss: 1567.649658 [16200/24965]\n",
      "Training batch loss: 1678.687744 [16300/24965]\n",
      "Training batch loss: 1411.614868 [16400/24965]\n",
      "Training batch loss: 1669.668945 [16500/24965]\n",
      "Training batch loss: 1582.767578 [16600/24965]\n",
      "Training batch loss: 1508.921631 [16700/24965]\n",
      "Training batch loss: 1730.096802 [16800/24965]\n",
      "Training batch loss: 1604.084229 [16900/24965]\n",
      "Training batch loss: 1624.569946 [17000/24965]\n",
      "Training batch loss: 1479.920166 [17100/24965]\n",
      "Training batch loss: 1491.701050 [17200/24965]\n",
      "Training batch loss: 1448.912354 [17300/24965]\n",
      "Training batch loss: 1648.007324 [17400/24965]\n",
      "Training batch loss: 1595.380859 [17500/24965]\n",
      "Training batch loss: 1511.437500 [17600/24965]\n",
      "Training batch loss: 1199.397461 [17700/24965]\n",
      "Training batch loss: 1550.164185 [17800/24965]\n",
      "Training batch loss: 1478.520630 [17900/24965]\n",
      "Training batch loss: 1805.684326 [18000/24965]\n",
      "Training batch loss: 1699.114502 [18100/24965]\n",
      "Training batch loss: 1583.270142 [18200/24965]\n",
      "Training batch loss: 1527.998657 [18300/24965]\n",
      "Training batch loss: 1629.289307 [18400/24965]\n",
      "Training batch loss: 1575.250732 [18500/24965]\n",
      "Training batch loss: 1617.916992 [18600/24965]\n",
      "Training batch loss: 1392.435425 [18700/24965]\n",
      "Training batch loss: 1529.169922 [18800/24965]\n",
      "Training batch loss: 1416.412720 [18900/24965]\n",
      "Training batch loss: 1449.983765 [19000/24965]\n",
      "Training batch loss: 1476.283081 [19100/24965]\n",
      "Training batch loss: 1485.382324 [19200/24965]\n",
      "Training batch loss: 1800.159790 [19300/24965]\n",
      "Training batch loss: 1584.422485 [19400/24965]\n",
      "Training batch loss: 1464.189087 [19500/24965]\n",
      "Training batch loss: 1401.235840 [19600/24965]\n",
      "Training batch loss: 1502.182495 [19700/24965]\n",
      "Training batch loss: 1304.854736 [19800/24965]\n",
      "Training batch loss: 1701.545410 [19900/24965]\n",
      "Training batch loss: 1641.588867 [20000/24965]\n",
      "Training batch loss: 1213.139282 [20100/24965]\n",
      "Training batch loss: 1734.101196 [20200/24965]\n",
      "Training batch loss: 1396.355225 [20300/24965]\n",
      "Training batch loss: 1249.165649 [20400/24965]\n",
      "Training batch loss: 1767.558105 [20500/24965]\n",
      "Training batch loss: 1637.222168 [20600/24965]\n",
      "Training batch loss: 1318.092529 [20700/24965]\n",
      "Training batch loss: 1434.843384 [20800/24965]\n",
      "Training batch loss: 1500.708252 [20900/24965]\n",
      "Training batch loss: 1620.319580 [21000/24965]\n",
      "Training batch loss: 1669.845581 [21100/24965]\n",
      "Training batch loss: 1663.460693 [21200/24965]\n",
      "Training batch loss: 1723.303223 [21300/24965]\n",
      "Training batch loss: 1831.015625 [21400/24965]\n",
      "Training batch loss: 1570.891602 [21500/24965]\n",
      "Training batch loss: 1514.319824 [21600/24965]\n",
      "Training batch loss: 1497.040527 [21700/24965]\n",
      "Training batch loss: 1486.902344 [21800/24965]\n",
      "Training batch loss: 1563.431152 [21900/24965]\n",
      "Training batch loss: 1576.489258 [22000/24965]\n",
      "Training batch loss: 1361.307129 [22100/24965]\n",
      "Training batch loss: 1703.190186 [22200/24965]\n",
      "Training batch loss: 1623.632202 [22300/24965]\n",
      "Training batch loss: 1453.366699 [22400/24965]\n",
      "Training batch loss: 1580.365967 [22500/24965]\n",
      "Training batch loss: 1779.505127 [22600/24965]\n",
      "Training batch loss: 1530.819702 [22700/24965]\n",
      "Training batch loss: 1710.981079 [22800/24965]\n",
      "Training batch loss: 1847.279663 [22900/24965]\n",
      "Training batch loss: 1453.331909 [23000/24965]\n",
      "Training batch loss: 1799.203003 [23100/24965]\n",
      "Training batch loss: 1762.574219 [23200/24965]\n",
      "Training batch loss: 1317.062500 [23300/24965]\n",
      "Training batch loss: 1609.988647 [23400/24965]\n",
      "Training batch loss: 1444.460205 [23500/24965]\n",
      "Training batch loss: 1508.951904 [23600/24965]\n",
      "Training batch loss: 1464.139771 [23700/24965]\n",
      "Training batch loss: 1588.272461 [23800/24965]\n",
      "Training batch loss: 1593.115234 [23900/24965]\n",
      "Training batch loss: 1736.871582 [24000/24965]\n",
      "Training batch loss: 1333.512939 [24100/24965]\n",
      "Training batch loss: 1421.330688 [24200/24965]\n",
      "Training batch loss: 1589.796021 [24300/24965]\n",
      "Training batch loss: 1622.972168 [24400/24965]\n",
      "Training batch loss: 1433.466675 [24500/24965]\n",
      "Training batch loss: 1506.196533 [24600/24965]\n",
      "Training batch loss: 1550.326172 [24700/24965]\n",
      "Training batch loss: 1472.557617 [24800/24965]\n",
      "Training batch loss: 1465.281006 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1513.210866\n",
      "\n",
      "Saved best new model with val_loss: 1513.2109\n",
      "Epoch 3\n",
      "----------------------\n",
      "Training batch loss: 1596.167969 [    0/24965]\n",
      "Training batch loss: 1640.262817 [  100/24965]\n",
      "Training batch loss: 1457.103149 [  200/24965]\n",
      "Training batch loss: 1396.630981 [  300/24965]\n",
      "Training batch loss: 1681.199829 [  400/24965]\n",
      "Training batch loss: 1453.301636 [  500/24965]\n",
      "Training batch loss: 1434.356934 [  600/24965]\n",
      "Training batch loss: 1578.695068 [  700/24965]\n",
      "Training batch loss: 1493.714966 [  800/24965]\n",
      "Training batch loss: 1769.045654 [  900/24965]\n",
      "Training batch loss: 1420.583252 [ 1000/24965]\n",
      "Training batch loss: 1580.651001 [ 1100/24965]\n",
      "Training batch loss: 1453.418335 [ 1200/24965]\n",
      "Training batch loss: 1538.215454 [ 1300/24965]\n",
      "Training batch loss: 1490.673340 [ 1400/24965]\n",
      "Training batch loss: 1433.940918 [ 1500/24965]\n",
      "Training batch loss: 1683.383789 [ 1600/24965]\n",
      "Training batch loss: 1527.657715 [ 1700/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1566.329346 [ 1800/24965]\n",
      "Training batch loss: 1279.798706 [ 1900/24965]\n",
      "Training batch loss: 1528.656006 [ 2000/24965]\n",
      "Training batch loss: 1409.481201 [ 2100/24965]\n",
      "Training batch loss: 1649.669312 [ 2200/24965]\n",
      "Training batch loss: 1593.489014 [ 2300/24965]\n",
      "Training batch loss: 1728.335449 [ 2400/24965]\n",
      "Training batch loss: 1825.300537 [ 2500/24965]\n",
      "Training batch loss: 1501.076538 [ 2600/24965]\n",
      "Training batch loss: 1527.099609 [ 2700/24965]\n",
      "Training batch loss: 1662.006470 [ 2800/24965]\n",
      "Training batch loss: 1443.320190 [ 2900/24965]\n",
      "Training batch loss: 1823.209473 [ 3000/24965]\n",
      "Training batch loss: 1428.242920 [ 3100/24965]\n",
      "Training batch loss: 1552.173096 [ 3200/24965]\n",
      "Training batch loss: 1529.279541 [ 3300/24965]\n",
      "Training batch loss: 1460.691895 [ 3400/24965]\n",
      "Training batch loss: 1634.933594 [ 3500/24965]\n",
      "Training batch loss: 1397.359253 [ 3600/24965]\n",
      "Training batch loss: 1617.921265 [ 3700/24965]\n",
      "Training batch loss: 1628.846313 [ 3800/24965]\n",
      "Training batch loss: 1625.563965 [ 3900/24965]\n",
      "Training batch loss: 1637.956543 [ 4000/24965]\n",
      "Training batch loss: 1397.512207 [ 4100/24965]\n",
      "Training batch loss: 1777.243896 [ 4200/24965]\n",
      "Training batch loss: 1385.146973 [ 4300/24965]\n",
      "Training batch loss: 1685.907715 [ 4400/24965]\n",
      "Training batch loss: 1567.248901 [ 4500/24965]\n",
      "Training batch loss: 1575.773438 [ 4600/24965]\n",
      "Training batch loss: 1704.816406 [ 4700/24965]\n",
      "Training batch loss: 1844.754883 [ 4800/24965]\n",
      "Training batch loss: 1469.369507 [ 4900/24965]\n",
      "Training batch loss: 1500.878662 [ 5000/24965]\n",
      "Training batch loss: 1400.136353 [ 5100/24965]\n",
      "Training batch loss: 1833.698608 [ 5200/24965]\n",
      "Training batch loss: 1639.773315 [ 5300/24965]\n",
      "Training batch loss: 1458.072754 [ 5400/24965]\n",
      "Training batch loss: 1496.562256 [ 5500/24965]\n",
      "Training batch loss: 1427.254028 [ 5600/24965]\n",
      "Training batch loss: 1465.143677 [ 5700/24965]\n",
      "Training batch loss: 1606.289551 [ 5800/24965]\n",
      "Training batch loss: 1578.979126 [ 5900/24965]\n",
      "Training batch loss: 1514.765503 [ 6000/24965]\n",
      "Training batch loss: 1360.370850 [ 6100/24965]\n",
      "Training batch loss: 1560.275757 [ 6200/24965]\n",
      "Training batch loss: 1613.411743 [ 6300/24965]\n",
      "Training batch loss: 1553.727783 [ 6400/24965]\n",
      "Training batch loss: 1703.544678 [ 6500/24965]\n",
      "Training batch loss: 1466.970825 [ 6600/24965]\n",
      "Training batch loss: 1572.880615 [ 6700/24965]\n",
      "Training batch loss: 1690.699463 [ 6800/24965]\n",
      "Training batch loss: 1358.249512 [ 6900/24965]\n",
      "Training batch loss: 1470.962402 [ 7000/24965]\n",
      "Training batch loss: 1400.289307 [ 7100/24965]\n",
      "Training batch loss: 1385.318115 [ 7200/24965]\n",
      "Training batch loss: 1499.704834 [ 7300/24965]\n",
      "Training batch loss: 1232.562744 [ 7400/24965]\n",
      "Training batch loss: 1527.030518 [ 7500/24965]\n",
      "Training batch loss: 1610.026367 [ 7600/24965]\n",
      "Training batch loss: 1695.289429 [ 7700/24965]\n",
      "Training batch loss: 1556.038330 [ 7800/24965]\n",
      "Training batch loss: 1416.817749 [ 7900/24965]\n",
      "Training batch loss: 1501.969238 [ 8000/24965]\n",
      "Training batch loss: 1613.060425 [ 8100/24965]\n",
      "Training batch loss: 1396.976929 [ 8200/24965]\n",
      "Training batch loss: 1582.264893 [ 8300/24965]\n",
      "Training batch loss: 1560.689697 [ 8400/24965]\n",
      "Training batch loss: 1475.872925 [ 8500/24965]\n",
      "Training batch loss: 1652.818237 [ 8600/24965]\n",
      "Training batch loss: 1550.933594 [ 8700/24965]\n",
      "Training batch loss: 1478.575684 [ 8800/24965]\n",
      "Training batch loss: 1466.715698 [ 8900/24965]\n",
      "Training batch loss: 1542.634521 [ 9000/24965]\n",
      "Training batch loss: 1439.321777 [ 9100/24965]\n",
      "Training batch loss: 1521.392700 [ 9200/24965]\n",
      "Training batch loss: 1498.052246 [ 9300/24965]\n",
      "Training batch loss: 1405.100708 [ 9400/24965]\n",
      "Training batch loss: 1631.383057 [ 9500/24965]\n",
      "Training batch loss: 1320.906616 [ 9600/24965]\n",
      "Training batch loss: 1720.140625 [ 9700/24965]\n",
      "Training batch loss: 1584.433716 [ 9800/24965]\n",
      "Training batch loss: 1585.005127 [ 9900/24965]\n",
      "Training batch loss: 1729.953857 [10000/24965]\n",
      "Training batch loss: 1566.005859 [10100/24965]\n",
      "Training batch loss: 1505.142578 [10200/24965]\n",
      "Training batch loss: 1401.169678 [10300/24965]\n",
      "Training batch loss: 1393.472046 [10400/24965]\n",
      "Training batch loss: 1524.764160 [10500/24965]\n",
      "Training batch loss: 1432.951538 [10600/24965]\n",
      "Training batch loss: 1570.804199 [10700/24965]\n",
      "Training batch loss: 1657.273560 [10800/24965]\n",
      "Training batch loss: 1773.288330 [10900/24965]\n",
      "Training batch loss: 1464.145386 [11000/24965]\n",
      "Training batch loss: 1429.240234 [11100/24965]\n",
      "Training batch loss: 1622.284424 [11200/24965]\n",
      "Training batch loss: 1535.854980 [11300/24965]\n",
      "Training batch loss: 1154.964233 [11400/24965]\n",
      "Training batch loss: 1718.852051 [11500/24965]\n",
      "Training batch loss: 1531.774658 [11600/24965]\n",
      "Training batch loss: 1552.293823 [11700/24965]\n",
      "Training batch loss: 1395.164551 [11800/24965]\n",
      "Training batch loss: 1529.167358 [11900/24965]\n",
      "Training batch loss: 1638.862915 [12000/24965]\n",
      "Training batch loss: 1479.217407 [12100/24965]\n",
      "Training batch loss: 1709.145630 [12200/24965]\n",
      "Training batch loss: 1545.331177 [12300/24965]\n",
      "Training batch loss: 1410.027344 [12400/24965]\n",
      "Training batch loss: 1741.213379 [12500/24965]\n",
      "Training batch loss: 1561.561035 [12600/24965]\n",
      "Training batch loss: 1433.360352 [12700/24965]\n",
      "Training batch loss: 1343.920532 [12800/24965]\n",
      "Training batch loss: 1709.779785 [12900/24965]\n",
      "Training batch loss: 1554.788086 [13000/24965]\n",
      "Training batch loss: 1677.724121 [13100/24965]\n",
      "Training batch loss: 1521.277832 [13200/24965]\n",
      "Training batch loss: 1499.937988 [13300/24965]\n",
      "Training batch loss: 1286.304932 [13400/24965]\n",
      "Training batch loss: 1369.559082 [13500/24965]\n",
      "Training batch loss: 1546.713623 [13600/24965]\n",
      "Training batch loss: 1566.049805 [13700/24965]\n",
      "Training batch loss: 1489.289062 [13800/24965]\n",
      "Training batch loss: 1720.218628 [13900/24965]\n",
      "Training batch loss: 1612.730957 [14000/24965]\n",
      "Training batch loss: 1430.294312 [14100/24965]\n",
      "Training batch loss: 1740.179810 [14200/24965]\n",
      "Training batch loss: 1715.050537 [14300/24965]\n",
      "Training batch loss: 1618.113647 [14400/24965]\n",
      "Training batch loss: 1674.452148 [14500/24965]\n",
      "Training batch loss: 1538.986328 [14600/24965]\n",
      "Training batch loss: 1685.632812 [14700/24965]\n",
      "Training batch loss: 1595.829590 [14800/24965]\n",
      "Training batch loss: 1652.986084 [14900/24965]\n",
      "Training batch loss: 1691.367065 [15000/24965]\n",
      "Training batch loss: 1615.574341 [15100/24965]\n",
      "Training batch loss: 1693.963013 [15200/24965]\n",
      "Training batch loss: 1494.056641 [15300/24965]\n",
      "Training batch loss: 1710.228882 [15400/24965]\n",
      "Training batch loss: 1472.946655 [15500/24965]\n",
      "Training batch loss: 1582.375732 [15600/24965]\n",
      "Training batch loss: 1575.181519 [15700/24965]\n",
      "Training batch loss: 1409.706055 [15800/24965]\n",
      "Training batch loss: 1343.766235 [15900/24965]\n",
      "Training batch loss: 1563.894287 [16000/24965]\n",
      "Training batch loss: 1361.836426 [16100/24965]\n",
      "Training batch loss: 1510.610596 [16200/24965]\n",
      "Training batch loss: 1364.568481 [16300/24965]\n",
      "Training batch loss: 1751.094482 [16400/24965]\n",
      "Training batch loss: 1625.089966 [16500/24965]\n",
      "Training batch loss: 1484.247192 [16600/24965]\n",
      "Training batch loss: 1539.327759 [16700/24965]\n",
      "Training batch loss: 1450.331543 [16800/24965]\n",
      "Training batch loss: 1618.207642 [16900/24965]\n",
      "Training batch loss: 1513.888428 [17000/24965]\n",
      "Training batch loss: 1206.561035 [17100/24965]\n",
      "Training batch loss: 1592.787720 [17200/24965]\n",
      "Training batch loss: 1519.720581 [17300/24965]\n",
      "Training batch loss: 1674.391113 [17400/24965]\n",
      "Training batch loss: 1638.791016 [17500/24965]\n",
      "Training batch loss: 1508.703613 [17600/24965]\n",
      "Training batch loss: 1519.317139 [17700/24965]\n",
      "Training batch loss: 1732.620728 [17800/24965]\n",
      "Training batch loss: 1638.471191 [17900/24965]\n",
      "Training batch loss: 1436.444092 [18000/24965]\n",
      "Training batch loss: 1482.947754 [18100/24965]\n",
      "Training batch loss: 1673.899048 [18200/24965]\n",
      "Training batch loss: 1409.663574 [18300/24965]\n",
      "Training batch loss: 1544.552734 [18400/24965]\n",
      "Training batch loss: 1668.815552 [18500/24965]\n",
      "Training batch loss: 1567.396973 [18600/24965]\n",
      "Training batch loss: 1659.117188 [18700/24965]\n",
      "Training batch loss: 1333.478638 [18800/24965]\n",
      "Training batch loss: 1721.995239 [18900/24965]\n",
      "Training batch loss: 1519.115967 [19000/24965]\n",
      "Training batch loss: 1407.115234 [19100/24965]\n",
      "Training batch loss: 1545.024170 [19200/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1447.532227 [19300/24965]\n",
      "Training batch loss: 1505.209473 [19400/24965]\n",
      "Training batch loss: 1453.030273 [19500/24965]\n",
      "Training batch loss: 1838.580811 [19600/24965]\n",
      "Training batch loss: 1398.358154 [19700/24965]\n",
      "Training batch loss: 1552.665405 [19800/24965]\n",
      "Training batch loss: 1881.666504 [19900/24965]\n",
      "Training batch loss: 1731.533936 [20000/24965]\n",
      "Training batch loss: 1577.401367 [20100/24965]\n",
      "Training batch loss: 1559.473389 [20200/24965]\n",
      "Training batch loss: 1336.409668 [20300/24965]\n",
      "Training batch loss: 1505.838867 [20400/24965]\n",
      "Training batch loss: 1312.485962 [20500/24965]\n",
      "Training batch loss: 1457.518433 [20600/24965]\n",
      "Training batch loss: 1556.842041 [20700/24965]\n",
      "Training batch loss: 1436.734375 [20800/24965]\n",
      "Training batch loss: 1537.797607 [20900/24965]\n",
      "Training batch loss: 1697.265625 [21000/24965]\n",
      "Training batch loss: 1640.639526 [21100/24965]\n",
      "Training batch loss: 1532.984619 [21200/24965]\n",
      "Training batch loss: 1534.218018 [21300/24965]\n",
      "Training batch loss: 1490.989502 [21400/24965]\n",
      "Training batch loss: 1569.416992 [21500/24965]\n",
      "Training batch loss: 1582.081055 [21600/24965]\n",
      "Training batch loss: 1450.978760 [21700/24965]\n",
      "Training batch loss: 1477.803833 [21800/24965]\n",
      "Training batch loss: 1808.910034 [21900/24965]\n",
      "Training batch loss: 1412.132568 [22000/24965]\n",
      "Training batch loss: 1223.645996 [22100/24965]\n",
      "Training batch loss: 1617.315430 [22200/24965]\n",
      "Training batch loss: 1702.708252 [22300/24965]\n",
      "Training batch loss: 1602.758789 [22400/24965]\n",
      "Training batch loss: 1491.253540 [22500/24965]\n",
      "Training batch loss: 1532.390991 [22600/24965]\n",
      "Training batch loss: 1807.930420 [22700/24965]\n",
      "Training batch loss: 1335.410034 [22800/24965]\n",
      "Training batch loss: 1474.355835 [22900/24965]\n",
      "Training batch loss: 1626.450684 [23000/24965]\n",
      "Training batch loss: 1714.838745 [23100/24965]\n",
      "Training batch loss: 1733.614868 [23200/24965]\n",
      "Training batch loss: 1799.378784 [23300/24965]\n",
      "Training batch loss: 1741.849976 [23400/24965]\n",
      "Training batch loss: 1595.802368 [23500/24965]\n",
      "Training batch loss: 1342.340576 [23600/24965]\n",
      "Training batch loss: 1488.298340 [23700/24965]\n",
      "Training batch loss: 1713.660645 [23800/24965]\n",
      "Training batch loss: 1617.292603 [23900/24965]\n",
      "Training batch loss: 1695.821289 [24000/24965]\n",
      "Training batch loss: 1729.439453 [24100/24965]\n",
      "Training batch loss: 1604.800659 [24200/24965]\n",
      "Training batch loss: 1320.143188 [24300/24965]\n",
      "Training batch loss: 1542.189819 [24400/24965]\n",
      "Training batch loss: 1425.545532 [24500/24965]\n",
      "Training batch loss: 1360.666992 [24600/24965]\n",
      "Training batch loss: 1415.092163 [24700/24965]\n",
      "Training batch loss: 1415.393311 [24800/24965]\n",
      "Training batch loss: 1570.842651 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1513.222321\n",
      "\n",
      "Epoch 4\n",
      "----------------------\n",
      "Training batch loss: 1433.923218 [    0/24965]\n",
      "Training batch loss: 1514.606812 [  100/24965]\n",
      "Training batch loss: 1451.260010 [  200/24965]\n",
      "Training batch loss: 1596.325439 [  300/24965]\n",
      "Training batch loss: 1515.386963 [  400/24965]\n",
      "Training batch loss: 1771.662964 [  500/24965]\n",
      "Training batch loss: 1438.153442 [  600/24965]\n",
      "Training batch loss: 1515.697754 [  700/24965]\n",
      "Training batch loss: 1306.288574 [  800/24965]\n",
      "Training batch loss: 1400.093628 [  900/24965]\n",
      "Training batch loss: 1760.290039 [ 1000/24965]\n",
      "Training batch loss: 1466.581177 [ 1100/24965]\n",
      "Training batch loss: 1377.406128 [ 1200/24965]\n",
      "Training batch loss: 1588.096436 [ 1300/24965]\n",
      "Training batch loss: 1666.630737 [ 1400/24965]\n",
      "Training batch loss: 1498.351318 [ 1500/24965]\n",
      "Training batch loss: 1538.559570 [ 1600/24965]\n",
      "Training batch loss: 1362.161621 [ 1700/24965]\n",
      "Training batch loss: 1457.928467 [ 1800/24965]\n",
      "Training batch loss: 1345.911499 [ 1900/24965]\n",
      "Training batch loss: 1597.922974 [ 2000/24965]\n",
      "Training batch loss: 1399.204224 [ 2100/24965]\n",
      "Training batch loss: 1507.646606 [ 2200/24965]\n",
      "Training batch loss: 1424.462891 [ 2300/24965]\n",
      "Training batch loss: 1443.192627 [ 2400/24965]\n",
      "Training batch loss: 1608.598145 [ 2500/24965]\n",
      "Training batch loss: 1431.213257 [ 2600/24965]\n",
      "Training batch loss: 1448.646118 [ 2700/24965]\n",
      "Training batch loss: 1620.066406 [ 2800/24965]\n",
      "Training batch loss: 1485.995605 [ 2900/24965]\n",
      "Training batch loss: 1285.674072 [ 3000/24965]\n",
      "Training batch loss: 1656.078125 [ 3100/24965]\n",
      "Training batch loss: 1487.703735 [ 3200/24965]\n",
      "Training batch loss: 1385.548096 [ 3300/24965]\n",
      "Training batch loss: 1538.775635 [ 3400/24965]\n",
      "Training batch loss: 1492.254150 [ 3500/24965]\n",
      "Training batch loss: 1787.181396 [ 3600/24965]\n",
      "Training batch loss: 1391.738892 [ 3700/24965]\n",
      "Training batch loss: 1591.595825 [ 3800/24965]\n",
      "Training batch loss: 1669.375977 [ 3900/24965]\n",
      "Training batch loss: 1545.626831 [ 4000/24965]\n",
      "Training batch loss: 1342.868652 [ 4100/24965]\n",
      "Training batch loss: 1626.661743 [ 4200/24965]\n",
      "Training batch loss: 1519.073486 [ 4300/24965]\n",
      "Training batch loss: 1540.042480 [ 4400/24965]\n",
      "Training batch loss: 1585.102661 [ 4500/24965]\n",
      "Training batch loss: 1652.710693 [ 4600/24965]\n",
      "Training batch loss: 1669.791138 [ 4700/24965]\n",
      "Training batch loss: 1531.221191 [ 4800/24965]\n",
      "Training batch loss: 1518.797363 [ 4900/24965]\n",
      "Training batch loss: 1455.219727 [ 5000/24965]\n",
      "Training batch loss: 1320.512451 [ 5100/24965]\n",
      "Training batch loss: 1633.281738 [ 5200/24965]\n",
      "Training batch loss: 1317.519531 [ 5300/24965]\n",
      "Training batch loss: 1427.535889 [ 5400/24965]\n",
      "Training batch loss: 1711.830322 [ 5500/24965]\n",
      "Training batch loss: 1656.765015 [ 5600/24965]\n",
      "Training batch loss: 1463.695801 [ 5700/24965]\n",
      "Training batch loss: 1394.494263 [ 5800/24965]\n",
      "Training batch loss: 1709.766113 [ 5900/24965]\n",
      "Training batch loss: 1578.635620 [ 6000/24965]\n",
      "Training batch loss: 1808.466187 [ 6100/24965]\n",
      "Training batch loss: 1461.523193 [ 6200/24965]\n",
      "Training batch loss: 1492.838745 [ 6300/24965]\n",
      "Training batch loss: 1405.344360 [ 6400/24965]\n",
      "Training batch loss: 1474.591309 [ 6500/24965]\n",
      "Training batch loss: 1378.248291 [ 6600/24965]\n",
      "Training batch loss: 1669.153320 [ 6700/24965]\n",
      "Training batch loss: 1591.741089 [ 6800/24965]\n",
      "Training batch loss: 1736.416992 [ 6900/24965]\n",
      "Training batch loss: 1635.395020 [ 7000/24965]\n",
      "Training batch loss: 1474.546875 [ 7100/24965]\n",
      "Training batch loss: 1371.181763 [ 7200/24965]\n",
      "Training batch loss: 1714.063110 [ 7300/24965]\n",
      "Training batch loss: 1321.270386 [ 7400/24965]\n",
      "Training batch loss: 1470.462891 [ 7500/24965]\n",
      "Training batch loss: 1439.302734 [ 7600/24965]\n",
      "Training batch loss: 1542.970703 [ 7700/24965]\n",
      "Training batch loss: 1655.509521 [ 7800/24965]\n",
      "Training batch loss: 1450.842285 [ 7900/24965]\n",
      "Training batch loss: 1514.827148 [ 8000/24965]\n",
      "Training batch loss: 1411.859863 [ 8100/24965]\n",
      "Training batch loss: 1578.811890 [ 8200/24965]\n",
      "Training batch loss: 1600.675537 [ 8300/24965]\n",
      "Training batch loss: 1496.618286 [ 8400/24965]\n",
      "Training batch loss: 1614.750732 [ 8500/24965]\n",
      "Training batch loss: 1383.053833 [ 8600/24965]\n",
      "Training batch loss: 1434.369385 [ 8700/24965]\n",
      "Training batch loss: 1561.686035 [ 8800/24965]\n",
      "Training batch loss: 1379.570679 [ 8900/24965]\n",
      "Training batch loss: 1475.919678 [ 9000/24965]\n",
      "Training batch loss: 1747.274902 [ 9100/24965]\n",
      "Training batch loss: 1627.959473 [ 9200/24965]\n",
      "Training batch loss: 1457.053345 [ 9300/24965]\n",
      "Training batch loss: 1365.409424 [ 9400/24965]\n",
      "Training batch loss: 1706.462036 [ 9500/24965]\n",
      "Training batch loss: 1732.723877 [ 9600/24965]\n",
      "Training batch loss: 1371.756592 [ 9700/24965]\n",
      "Training batch loss: 1685.346191 [ 9800/24965]\n",
      "Training batch loss: 1344.243408 [ 9900/24965]\n",
      "Training batch loss: 1643.812500 [10000/24965]\n",
      "Training batch loss: 1676.876709 [10100/24965]\n",
      "Training batch loss: 1567.780884 [10200/24965]\n",
      "Training batch loss: 1529.790161 [10300/24965]\n",
      "Training batch loss: 1254.919312 [10400/24965]\n",
      "Training batch loss: 1491.192139 [10500/24965]\n",
      "Training batch loss: 1540.864380 [10600/24965]\n",
      "Training batch loss: 1427.560181 [10700/24965]\n",
      "Training batch loss: 1334.925537 [10800/24965]\n",
      "Training batch loss: 1445.334229 [10900/24965]\n",
      "Training batch loss: 1482.500732 [11000/24965]\n",
      "Training batch loss: 1635.059692 [11100/24965]\n",
      "Training batch loss: 1363.717407 [11200/24965]\n",
      "Training batch loss: 1772.496460 [11300/24965]\n",
      "Training batch loss: 1670.499268 [11400/24965]\n",
      "Training batch loss: 1496.404785 [11500/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1569.105835 [11600/24965]\n",
      "Training batch loss: 1501.100464 [11700/24965]\n",
      "Training batch loss: 1665.773682 [11800/24965]\n",
      "Training batch loss: 1560.345459 [11900/24965]\n",
      "Training batch loss: 1612.896973 [12000/24965]\n",
      "Training batch loss: 1640.108521 [12100/24965]\n",
      "Training batch loss: 1575.722046 [12200/24965]\n",
      "Training batch loss: 1513.358032 [12300/24965]\n",
      "Training batch loss: 1765.749023 [12400/24965]\n",
      "Training batch loss: 1479.712769 [12500/24965]\n",
      "Training batch loss: 1487.331299 [12600/24965]\n",
      "Training batch loss: 1442.582764 [12700/24965]\n",
      "Training batch loss: 1558.166016 [12800/24965]\n",
      "Training batch loss: 1324.170532 [12900/24965]\n",
      "Training batch loss: 1609.466553 [13000/24965]\n",
      "Training batch loss: 1629.092407 [13100/24965]\n",
      "Training batch loss: 1373.449097 [13200/24965]\n",
      "Training batch loss: 1581.291748 [13300/24965]\n",
      "Training batch loss: 1475.670044 [13400/24965]\n",
      "Training batch loss: 1463.971924 [13500/24965]\n",
      "Training batch loss: 1788.394409 [13600/24965]\n",
      "Training batch loss: 1530.452271 [13700/24965]\n",
      "Training batch loss: 1528.092285 [13800/24965]\n",
      "Training batch loss: 1680.648438 [13900/24965]\n",
      "Training batch loss: 1451.335449 [14000/24965]\n",
      "Training batch loss: 1376.805298 [14100/24965]\n",
      "Training batch loss: 1804.840332 [14200/24965]\n",
      "Training batch loss: 1431.643066 [14300/24965]\n",
      "Training batch loss: 1428.628296 [14400/24965]\n",
      "Training batch loss: 1562.062256 [14500/24965]\n",
      "Training batch loss: 1392.874146 [14600/24965]\n",
      "Training batch loss: 1484.798706 [14700/24965]\n",
      "Training batch loss: 1456.817871 [14800/24965]\n",
      "Training batch loss: 1453.901855 [14900/24965]\n",
      "Training batch loss: 1574.014160 [15000/24965]\n",
      "Training batch loss: 1419.083740 [15100/24965]\n",
      "Training batch loss: 1787.367920 [15200/24965]\n",
      "Training batch loss: 1665.928589 [15300/24965]\n",
      "Training batch loss: 1607.764282 [15400/24965]\n",
      "Training batch loss: 1645.756958 [15500/24965]\n",
      "Training batch loss: 1629.444336 [15600/24965]\n",
      "Training batch loss: 1560.129028 [15700/24965]\n",
      "Training batch loss: 1533.029175 [15800/24965]\n",
      "Training batch loss: 1470.468140 [15900/24965]\n",
      "Training batch loss: 1561.378296 [16000/24965]\n",
      "Training batch loss: 1572.376709 [16100/24965]\n",
      "Training batch loss: 1421.653687 [16200/24965]\n",
      "Training batch loss: 1346.708862 [16300/24965]\n",
      "Training batch loss: 1384.214844 [16400/24965]\n",
      "Training batch loss: 1581.978760 [16500/24965]\n",
      "Training batch loss: 1612.742798 [16600/24965]\n",
      "Training batch loss: 1616.903076 [16700/24965]\n",
      "Training batch loss: 1521.723145 [16800/24965]\n",
      "Training batch loss: 1437.179321 [16900/24965]\n",
      "Training batch loss: 1640.689087 [17000/24965]\n",
      "Training batch loss: 1669.367554 [17100/24965]\n",
      "Training batch loss: 1473.135742 [17200/24965]\n",
      "Training batch loss: 1619.156982 [17300/24965]\n",
      "Training batch loss: 1376.756104 [17400/24965]\n",
      "Training batch loss: 1074.918945 [17500/24965]\n",
      "Training batch loss: 1441.099121 [17600/24965]\n",
      "Training batch loss: 1572.028198 [17700/24965]\n",
      "Training batch loss: 1595.974854 [17800/24965]\n",
      "Training batch loss: 1482.863647 [17900/24965]\n",
      "Training batch loss: 1449.134644 [18000/24965]\n",
      "Training batch loss: 1541.045532 [18100/24965]\n",
      "Training batch loss: 1566.364258 [18200/24965]\n",
      "Training batch loss: 1542.567627 [18300/24965]\n",
      "Training batch loss: 1494.340210 [18400/24965]\n",
      "Training batch loss: 1819.329346 [18500/24965]\n",
      "Training batch loss: 1485.476440 [18600/24965]\n",
      "Training batch loss: 1658.008179 [18700/24965]\n",
      "Training batch loss: 1662.127686 [18800/24965]\n",
      "Training batch loss: 1488.732422 [18900/24965]\n",
      "Training batch loss: 1567.844116 [19000/24965]\n",
      "Training batch loss: 1535.172119 [19100/24965]\n",
      "Training batch loss: 1489.798096 [19200/24965]\n",
      "Training batch loss: 1531.498779 [19300/24965]\n",
      "Training batch loss: 1343.807129 [19400/24965]\n",
      "Training batch loss: 1594.215332 [19500/24965]\n",
      "Training batch loss: 1612.099487 [19600/24965]\n",
      "Training batch loss: 1500.087891 [19700/24965]\n",
      "Training batch loss: 1547.495117 [19800/24965]\n",
      "Training batch loss: 1433.344238 [19900/24965]\n",
      "Training batch loss: 1450.938721 [20000/24965]\n",
      "Training batch loss: 1728.022339 [20100/24965]\n",
      "Training batch loss: 1628.179077 [20200/24965]\n",
      "Training batch loss: 1523.588013 [20300/24965]\n",
      "Training batch loss: 1357.322388 [20400/24965]\n",
      "Training batch loss: 1416.230103 [20500/24965]\n",
      "Training batch loss: 1546.278198 [20600/24965]\n",
      "Training batch loss: 1582.406738 [20700/24965]\n",
      "Training batch loss: 1446.734619 [20800/24965]\n",
      "Training batch loss: 1457.572510 [20900/24965]\n",
      "Training batch loss: 1419.607544 [21000/24965]\n",
      "Training batch loss: 1707.013916 [21100/24965]\n",
      "Training batch loss: 1639.424316 [21200/24965]\n",
      "Training batch loss: 1643.683716 [21300/24965]\n",
      "Training batch loss: 1467.439697 [21400/24965]\n",
      "Training batch loss: 1443.695068 [21500/24965]\n",
      "Training batch loss: 1516.381836 [21600/24965]\n",
      "Training batch loss: 1399.504761 [21700/24965]\n",
      "Training batch loss: 1687.322754 [21800/24965]\n",
      "Training batch loss: 1134.539062 [21900/24965]\n",
      "Training batch loss: 1713.932861 [22000/24965]\n",
      "Training batch loss: 1278.036987 [22100/24965]\n",
      "Training batch loss: 1542.846191 [22200/24965]\n",
      "Training batch loss: 1566.259277 [22300/24965]\n",
      "Training batch loss: 1491.108398 [22400/24965]\n",
      "Training batch loss: 1597.664062 [22500/24965]\n",
      "Training batch loss: 1622.770630 [22600/24965]\n",
      "Training batch loss: 1812.239746 [22700/24965]\n",
      "Training batch loss: 1715.167358 [22800/24965]\n",
      "Training batch loss: 1541.851562 [22900/24965]\n",
      "Training batch loss: 1691.497070 [23000/24965]\n",
      "Training batch loss: 1632.607300 [23100/24965]\n",
      "Training batch loss: 1754.637695 [23200/24965]\n",
      "Training batch loss: 1764.393555 [23300/24965]\n",
      "Training batch loss: 1460.043335 [23400/24965]\n",
      "Training batch loss: 1588.778564 [23500/24965]\n",
      "Training batch loss: 1568.881104 [23600/24965]\n",
      "Training batch loss: 1358.179443 [23700/24965]\n",
      "Training batch loss: 1582.489868 [23800/24965]\n",
      "Training batch loss: 1412.285278 [23900/24965]\n",
      "Training batch loss: 1678.609497 [24000/24965]\n",
      "Training batch loss: 1460.058228 [24100/24965]\n",
      "Training batch loss: 1799.577393 [24200/24965]\n",
      "Training batch loss: 1307.153809 [24300/24965]\n",
      "Training batch loss: 1398.576294 [24400/24965]\n",
      "Training batch loss: 1346.086792 [24500/24965]\n",
      "Training batch loss: 1427.511475 [24600/24965]\n",
      "Training batch loss: 1521.632568 [24700/24965]\n",
      "Training batch loss: 1643.200073 [24800/24965]\n",
      "Training batch loss: 1610.907471 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.999932\n",
      "\n",
      "Saved best new model with val_loss: 1512.9999\n",
      "Epoch 5\n",
      "----------------------\n",
      "Training batch loss: 1618.035767 [    0/24965]\n",
      "Training batch loss: 1546.630371 [  100/24965]\n",
      "Training batch loss: 1371.511597 [  200/24965]\n",
      "Training batch loss: 1589.905762 [  300/24965]\n",
      "Training batch loss: 1446.195679 [  400/24965]\n",
      "Training batch loss: 1501.812256 [  500/24965]\n",
      "Training batch loss: 1565.010254 [  600/24965]\n",
      "Training batch loss: 1622.056763 [  700/24965]\n",
      "Training batch loss: 1607.564087 [  800/24965]\n",
      "Training batch loss: 1462.860474 [  900/24965]\n",
      "Training batch loss: 1621.581543 [ 1000/24965]\n",
      "Training batch loss: 1725.288452 [ 1100/24965]\n",
      "Training batch loss: 1504.246338 [ 1200/24965]\n",
      "Training batch loss: 1497.207275 [ 1300/24965]\n",
      "Training batch loss: 1411.395142 [ 1400/24965]\n",
      "Training batch loss: 1517.195923 [ 1500/24965]\n",
      "Training batch loss: 1787.088745 [ 1600/24965]\n",
      "Training batch loss: 1609.743042 [ 1700/24965]\n",
      "Training batch loss: 1512.996948 [ 1800/24965]\n",
      "Training batch loss: 1530.782593 [ 1900/24965]\n",
      "Training batch loss: 1457.817627 [ 2000/24965]\n",
      "Training batch loss: 1359.863525 [ 2100/24965]\n",
      "Training batch loss: 1628.743164 [ 2200/24965]\n",
      "Training batch loss: 1318.640869 [ 2300/24965]\n",
      "Training batch loss: 1578.026855 [ 2400/24965]\n",
      "Training batch loss: 1452.472900 [ 2500/24965]\n",
      "Training batch loss: 1425.463867 [ 2600/24965]\n",
      "Training batch loss: 1838.344971 [ 2700/24965]\n",
      "Training batch loss: 1346.849854 [ 2800/24965]\n",
      "Training batch loss: 1457.151001 [ 2900/24965]\n",
      "Training batch loss: 1362.351562 [ 3000/24965]\n",
      "Training batch loss: 1441.108032 [ 3100/24965]\n",
      "Training batch loss: 1675.669922 [ 3200/24965]\n",
      "Training batch loss: 1699.268921 [ 3300/24965]\n",
      "Training batch loss: 1516.919556 [ 3400/24965]\n",
      "Training batch loss: 1406.531006 [ 3500/24965]\n",
      "Training batch loss: 1460.552002 [ 3600/24965]\n",
      "Training batch loss: 1555.570801 [ 3700/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1493.183105 [ 3800/24965]\n",
      "Training batch loss: 1547.776245 [ 3900/24965]\n",
      "Training batch loss: 1458.045288 [ 4000/24965]\n",
      "Training batch loss: 1911.064941 [ 4100/24965]\n",
      "Training batch loss: 1596.598389 [ 4200/24965]\n",
      "Training batch loss: 1537.109253 [ 4300/24965]\n",
      "Training batch loss: 1583.044312 [ 4400/24965]\n",
      "Training batch loss: 1629.024048 [ 4500/24965]\n",
      "Training batch loss: 1372.716431 [ 4600/24965]\n",
      "Training batch loss: 1473.670044 [ 4700/24965]\n",
      "Training batch loss: 1470.822510 [ 4800/24965]\n",
      "Training batch loss: 1319.551758 [ 4900/24965]\n",
      "Training batch loss: 1549.680664 [ 5000/24965]\n",
      "Training batch loss: 1344.026367 [ 5100/24965]\n",
      "Training batch loss: 1553.631226 [ 5200/24965]\n",
      "Training batch loss: 1568.559204 [ 5300/24965]\n",
      "Training batch loss: 1339.137573 [ 5400/24965]\n",
      "Training batch loss: 1520.302856 [ 5500/24965]\n",
      "Training batch loss: 1707.706909 [ 5600/24965]\n",
      "Training batch loss: 1459.256592 [ 5700/24965]\n",
      "Training batch loss: 1571.508057 [ 5800/24965]\n",
      "Training batch loss: 1536.859985 [ 5900/24965]\n",
      "Training batch loss: 1457.431885 [ 6000/24965]\n",
      "Training batch loss: 1667.203979 [ 6100/24965]\n",
      "Training batch loss: 1534.714478 [ 6200/24965]\n",
      "Training batch loss: 1479.398682 [ 6300/24965]\n",
      "Training batch loss: 1699.859253 [ 6400/24965]\n",
      "Training batch loss: 1422.004883 [ 6500/24965]\n",
      "Training batch loss: 1545.555054 [ 6600/24965]\n",
      "Training batch loss: 1677.715454 [ 6700/24965]\n",
      "Training batch loss: 1496.262329 [ 6800/24965]\n",
      "Training batch loss: 1563.602539 [ 6900/24965]\n",
      "Training batch loss: 1242.982300 [ 7000/24965]\n",
      "Training batch loss: 1545.530396 [ 7100/24965]\n",
      "Training batch loss: 1864.425537 [ 7200/24965]\n",
      "Training batch loss: 1503.779175 [ 7300/24965]\n",
      "Training batch loss: 1472.603882 [ 7400/24965]\n",
      "Training batch loss: 1571.270508 [ 7500/24965]\n",
      "Training batch loss: 1655.949341 [ 7600/24965]\n",
      "Training batch loss: 1634.139893 [ 7700/24965]\n",
      "Training batch loss: 1607.816772 [ 7800/24965]\n",
      "Training batch loss: 1474.629883 [ 7900/24965]\n",
      "Training batch loss: 1527.792725 [ 8000/24965]\n",
      "Training batch loss: 1378.267944 [ 8100/24965]\n",
      "Training batch loss: 1327.272339 [ 8200/24965]\n",
      "Training batch loss: 1463.582764 [ 8300/24965]\n",
      "Training batch loss: 1536.036255 [ 8400/24965]\n",
      "Training batch loss: 1395.073730 [ 8500/24965]\n",
      "Training batch loss: 1738.819580 [ 8600/24965]\n",
      "Training batch loss: 1602.319458 [ 8700/24965]\n",
      "Training batch loss: 1432.675293 [ 8800/24965]\n",
      "Training batch loss: 1616.746582 [ 8900/24965]\n",
      "Training batch loss: 1710.219116 [ 9000/24965]\n",
      "Training batch loss: 1696.525391 [ 9100/24965]\n",
      "Training batch loss: 1419.369629 [ 9200/24965]\n",
      "Training batch loss: 1475.874878 [ 9300/24965]\n",
      "Training batch loss: 1469.329102 [ 9400/24965]\n",
      "Training batch loss: 1325.072998 [ 9500/24965]\n",
      "Training batch loss: 1488.955566 [ 9600/24965]\n",
      "Training batch loss: 1628.940430 [ 9700/24965]\n",
      "Training batch loss: 1600.557007 [ 9800/24965]\n",
      "Training batch loss: 1265.694092 [ 9900/24965]\n",
      "Training batch loss: 1478.484253 [10000/24965]\n",
      "Training batch loss: 1577.503906 [10100/24965]\n",
      "Training batch loss: 1674.760498 [10200/24965]\n",
      "Training batch loss: 1405.523193 [10300/24965]\n",
      "Training batch loss: 1703.938721 [10400/24965]\n",
      "Training batch loss: 1302.305054 [10500/24965]\n",
      "Training batch loss: 1534.258545 [10600/24965]\n",
      "Training batch loss: 1541.754761 [10700/24965]\n",
      "Training batch loss: 1650.452393 [10800/24965]\n",
      "Training batch loss: 1712.702637 [10900/24965]\n",
      "Training batch loss: 1733.587769 [11000/24965]\n",
      "Training batch loss: 1564.868042 [11100/24965]\n",
      "Training batch loss: 1486.190674 [11200/24965]\n",
      "Training batch loss: 1728.649170 [11300/24965]\n",
      "Training batch loss: 1480.021606 [11400/24965]\n",
      "Training batch loss: 1495.192383 [11500/24965]\n",
      "Training batch loss: 1353.665161 [11600/24965]\n",
      "Training batch loss: 1552.893188 [11700/24965]\n",
      "Training batch loss: 1310.626831 [11800/24965]\n",
      "Training batch loss: 1503.783325 [11900/24965]\n",
      "Training batch loss: 1688.475342 [12000/24965]\n",
      "Training batch loss: 1480.411865 [12100/24965]\n",
      "Training batch loss: 1632.878662 [12200/24965]\n",
      "Training batch loss: 1754.045044 [12300/24965]\n",
      "Training batch loss: 1491.890137 [12400/24965]\n",
      "Training batch loss: 1611.865234 [12500/24965]\n",
      "Training batch loss: 1686.922729 [12600/24965]\n",
      "Training batch loss: 1683.175781 [12700/24965]\n",
      "Training batch loss: 1663.296021 [12800/24965]\n",
      "Training batch loss: 1449.223877 [12900/24965]\n",
      "Training batch loss: 1765.724365 [13000/24965]\n",
      "Training batch loss: 1610.875244 [13100/24965]\n",
      "Training batch loss: 1401.485107 [13200/24965]\n",
      "Training batch loss: 1558.111084 [13300/24965]\n",
      "Training batch loss: 1603.520508 [13400/24965]\n",
      "Training batch loss: 1705.097412 [13500/24965]\n",
      "Training batch loss: 1572.400391 [13600/24965]\n",
      "Training batch loss: 1628.406738 [13700/24965]\n",
      "Training batch loss: 1561.095825 [13800/24965]\n",
      "Training batch loss: 1561.827393 [13900/24965]\n",
      "Training batch loss: 1542.441772 [14000/24965]\n",
      "Training batch loss: 1624.258057 [14100/24965]\n",
      "Training batch loss: 1480.494385 [14200/24965]\n",
      "Training batch loss: 1726.683350 [14300/24965]\n",
      "Training batch loss: 1422.811279 [14400/24965]\n",
      "Training batch loss: 1622.674072 [14500/24965]\n",
      "Training batch loss: 1669.126465 [14600/24965]\n",
      "Training batch loss: 1586.612183 [14700/24965]\n",
      "Training batch loss: 1448.264404 [14800/24965]\n",
      "Training batch loss: 1285.852905 [14900/24965]\n",
      "Training batch loss: 1527.010254 [15000/24965]\n",
      "Training batch loss: 1683.219604 [15100/24965]\n",
      "Training batch loss: 1466.999634 [15200/24965]\n",
      "Training batch loss: 1578.942261 [15300/24965]\n",
      "Training batch loss: 1359.253540 [15400/24965]\n",
      "Training batch loss: 1232.779785 [15500/24965]\n",
      "Training batch loss: 1475.310425 [15600/24965]\n",
      "Training batch loss: 1633.473755 [15700/24965]\n",
      "Training batch loss: 1490.831787 [15800/24965]\n",
      "Training batch loss: 1514.085449 [15900/24965]\n",
      "Training batch loss: 1428.542358 [16000/24965]\n",
      "Training batch loss: 1804.810059 [16100/24965]\n",
      "Training batch loss: 1369.443970 [16200/24965]\n",
      "Training batch loss: 1558.610962 [16300/24965]\n",
      "Training batch loss: 1683.911865 [16400/24965]\n",
      "Training batch loss: 1523.912109 [16500/24965]\n",
      "Training batch loss: 1388.698120 [16600/24965]\n",
      "Training batch loss: 1581.639771 [16700/24965]\n",
      "Training batch loss: 1386.164429 [16800/24965]\n",
      "Training batch loss: 1712.410278 [16900/24965]\n",
      "Training batch loss: 1574.044678 [17000/24965]\n",
      "Training batch loss: 1537.447510 [17100/24965]\n",
      "Training batch loss: 1476.256470 [17200/24965]\n",
      "Training batch loss: 1569.906982 [17300/24965]\n",
      "Training batch loss: 1361.648926 [17400/24965]\n",
      "Training batch loss: 1444.085938 [17500/24965]\n",
      "Training batch loss: 1453.076294 [17600/24965]\n",
      "Training batch loss: 1733.371582 [17700/24965]\n",
      "Training batch loss: 1651.996216 [17800/24965]\n",
      "Training batch loss: 1118.098145 [17900/24965]\n",
      "Training batch loss: 1572.586304 [18000/24965]\n",
      "Training batch loss: 1348.050537 [18100/24965]\n",
      "Training batch loss: 1503.708740 [18200/24965]\n",
      "Training batch loss: 1593.317383 [18300/24965]\n",
      "Training batch loss: 1454.128906 [18400/24965]\n",
      "Training batch loss: 1507.200684 [18500/24965]\n",
      "Training batch loss: 1672.557861 [18600/24965]\n",
      "Training batch loss: 1611.989258 [18700/24965]\n",
      "Training batch loss: 1389.174683 [18800/24965]\n",
      "Training batch loss: 1753.972778 [18900/24965]\n",
      "Training batch loss: 1549.744019 [19000/24965]\n",
      "Training batch loss: 1575.163086 [19100/24965]\n",
      "Training batch loss: 1846.039917 [19200/24965]\n",
      "Training batch loss: 1657.470825 [19300/24965]\n",
      "Training batch loss: 1513.966553 [19400/24965]\n",
      "Training batch loss: 1665.931396 [19500/24965]\n",
      "Training batch loss: 1540.636963 [19600/24965]\n",
      "Training batch loss: 1674.076904 [19700/24965]\n",
      "Training batch loss: 1750.573853 [19800/24965]\n",
      "Training batch loss: 1648.278687 [19900/24965]\n",
      "Training batch loss: 1567.384766 [20000/24965]\n",
      "Training batch loss: 1463.177490 [20100/24965]\n",
      "Training batch loss: 1489.517944 [20200/24965]\n",
      "Training batch loss: 1385.790405 [20300/24965]\n",
      "Training batch loss: 1448.503296 [20400/24965]\n",
      "Training batch loss: 1585.810547 [20500/24965]\n",
      "Training batch loss: 1722.770142 [20600/24965]\n",
      "Training batch loss: 1670.699829 [20700/24965]\n",
      "Training batch loss: 1498.133301 [20800/24965]\n",
      "Training batch loss: 1336.413940 [20900/24965]\n",
      "Training batch loss: 1512.543945 [21000/24965]\n",
      "Training batch loss: 1590.784180 [21100/24965]\n",
      "Training batch loss: 1698.358643 [21200/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1482.662720 [21300/24965]\n",
      "Training batch loss: 1579.262817 [21400/24965]\n",
      "Training batch loss: 1606.850098 [21500/24965]\n",
      "Training batch loss: 1576.645874 [21600/24965]\n",
      "Training batch loss: 1495.675415 [21700/24965]\n",
      "Training batch loss: 1730.849609 [21800/24965]\n",
      "Training batch loss: 1373.790161 [21900/24965]\n",
      "Training batch loss: 1553.114258 [22000/24965]\n",
      "Training batch loss: 1510.237915 [22100/24965]\n",
      "Training batch loss: 1476.708740 [22200/24965]\n",
      "Training batch loss: 1660.680176 [22300/24965]\n",
      "Training batch loss: 1433.407471 [22400/24965]\n",
      "Training batch loss: 1738.038330 [22500/24965]\n",
      "Training batch loss: 1412.877930 [22600/24965]\n",
      "Training batch loss: 1537.830200 [22700/24965]\n",
      "Training batch loss: 1706.430542 [22800/24965]\n",
      "Training batch loss: 1604.781982 [22900/24965]\n",
      "Training batch loss: 1573.181274 [23000/24965]\n",
      "Training batch loss: 1391.753418 [23100/24965]\n",
      "Training batch loss: 1532.963623 [23200/24965]\n",
      "Training batch loss: 1337.601562 [23300/24965]\n",
      "Training batch loss: 1644.772461 [23400/24965]\n",
      "Training batch loss: 1472.875488 [23500/24965]\n",
      "Training batch loss: 1506.255127 [23600/24965]\n",
      "Training batch loss: 1608.057983 [23700/24965]\n",
      "Training batch loss: 1437.204102 [23800/24965]\n",
      "Training batch loss: 1618.926514 [23900/24965]\n",
      "Training batch loss: 1312.083130 [24000/24965]\n",
      "Training batch loss: 1501.020020 [24100/24965]\n",
      "Training batch loss: 1586.450439 [24200/24965]\n",
      "Training batch loss: 1388.821167 [24300/24965]\n",
      "Training batch loss: 1577.517944 [24400/24965]\n",
      "Training batch loss: 1561.484131 [24500/24965]\n",
      "Training batch loss: 1618.370850 [24600/24965]\n",
      "Training batch loss: 1632.442627 [24700/24965]\n",
      "Training batch loss: 1582.195923 [24800/24965]\n",
      "Training batch loss: 1406.136230 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.881934\n",
      "\n",
      "Saved best new model with val_loss: 1512.8819\n",
      "Epoch 6\n",
      "----------------------\n",
      "Training batch loss: 1575.320312 [    0/24965]\n",
      "Training batch loss: 1816.988770 [  100/24965]\n",
      "Training batch loss: 1582.735962 [  200/24965]\n",
      "Training batch loss: 1638.961670 [  300/24965]\n",
      "Training batch loss: 1351.738281 [  400/24965]\n",
      "Training batch loss: 1492.978760 [  500/24965]\n",
      "Training batch loss: 1484.267090 [  600/24965]\n",
      "Training batch loss: 1514.838745 [  700/24965]\n",
      "Training batch loss: 1555.943481 [  800/24965]\n",
      "Training batch loss: 1601.728149 [  900/24965]\n",
      "Training batch loss: 1574.030518 [ 1000/24965]\n",
      "Training batch loss: 1599.847900 [ 1100/24965]\n",
      "Training batch loss: 1623.410767 [ 1200/24965]\n",
      "Training batch loss: 1614.586914 [ 1300/24965]\n",
      "Training batch loss: 1702.978760 [ 1400/24965]\n",
      "Training batch loss: 1362.315918 [ 1500/24965]\n",
      "Training batch loss: 1519.802979 [ 1600/24965]\n",
      "Training batch loss: 1585.530396 [ 1700/24965]\n",
      "Training batch loss: 1474.679199 [ 1800/24965]\n",
      "Training batch loss: 1273.354126 [ 1900/24965]\n",
      "Training batch loss: 1356.395142 [ 2000/24965]\n",
      "Training batch loss: 1673.737915 [ 2100/24965]\n",
      "Training batch loss: 1536.088257 [ 2200/24965]\n",
      "Training batch loss: 1496.950928 [ 2300/24965]\n",
      "Training batch loss: 1377.736328 [ 2400/24965]\n",
      "Training batch loss: 1621.439453 [ 2500/24965]\n",
      "Training batch loss: 1594.557251 [ 2600/24965]\n",
      "Training batch loss: 1529.356445 [ 2700/24965]\n",
      "Training batch loss: 1750.316040 [ 2800/24965]\n",
      "Training batch loss: 1335.931396 [ 2900/24965]\n",
      "Training batch loss: 1611.572510 [ 3000/24965]\n",
      "Training batch loss: 1561.660156 [ 3100/24965]\n",
      "Training batch loss: 1550.647705 [ 3200/24965]\n",
      "Training batch loss: 1602.183594 [ 3300/24965]\n",
      "Training batch loss: 1480.079224 [ 3400/24965]\n",
      "Training batch loss: 1603.806519 [ 3500/24965]\n",
      "Training batch loss: 1640.461548 [ 3600/24965]\n",
      "Training batch loss: 1492.705322 [ 3700/24965]\n",
      "Training batch loss: 1478.085327 [ 3800/24965]\n",
      "Training batch loss: 1550.812866 [ 3900/24965]\n",
      "Training batch loss: 1690.587280 [ 4000/24965]\n",
      "Training batch loss: 1484.538818 [ 4100/24965]\n",
      "Training batch loss: 1553.752197 [ 4200/24965]\n",
      "Training batch loss: 1466.998413 [ 4300/24965]\n",
      "Training batch loss: 1651.073608 [ 4400/24965]\n",
      "Training batch loss: 1553.108398 [ 4500/24965]\n",
      "Training batch loss: 1502.487305 [ 4600/24965]\n",
      "Training batch loss: 1513.070435 [ 4700/24965]\n",
      "Training batch loss: 1476.215942 [ 4800/24965]\n",
      "Training batch loss: 1450.425171 [ 4900/24965]\n",
      "Training batch loss: 1579.491211 [ 5000/24965]\n",
      "Training batch loss: 1436.206055 [ 5100/24965]\n",
      "Training batch loss: 1531.381836 [ 5200/24965]\n",
      "Training batch loss: 1690.217285 [ 5300/24965]\n",
      "Training batch loss: 1760.957642 [ 5400/24965]\n",
      "Training batch loss: 1274.400024 [ 5500/24965]\n",
      "Training batch loss: 1680.228882 [ 5600/24965]\n",
      "Training batch loss: 1369.728394 [ 5700/24965]\n",
      "Training batch loss: 1472.693359 [ 5800/24965]\n",
      "Training batch loss: 1830.522339 [ 5900/24965]\n",
      "Training batch loss: 1582.956543 [ 6000/24965]\n",
      "Training batch loss: 1524.163818 [ 6100/24965]\n",
      "Training batch loss: 1707.198364 [ 6200/24965]\n",
      "Training batch loss: 1425.740356 [ 6300/24965]\n",
      "Training batch loss: 1798.467896 [ 6400/24965]\n",
      "Training batch loss: 1401.620117 [ 6500/24965]\n",
      "Training batch loss: 1430.880127 [ 6600/24965]\n",
      "Training batch loss: 1515.966431 [ 6700/24965]\n",
      "Training batch loss: 1585.202148 [ 6800/24965]\n",
      "Training batch loss: 1469.415771 [ 6900/24965]\n",
      "Training batch loss: 1602.322144 [ 7000/24965]\n",
      "Training batch loss: 1440.758911 [ 7100/24965]\n",
      "Training batch loss: 1379.357056 [ 7200/24965]\n",
      "Training batch loss: 1439.322876 [ 7300/24965]\n",
      "Training batch loss: 1450.775024 [ 7400/24965]\n",
      "Training batch loss: 1570.696045 [ 7500/24965]\n",
      "Training batch loss: 1586.031494 [ 7600/24965]\n",
      "Training batch loss: 1529.741821 [ 7700/24965]\n",
      "Training batch loss: 1553.281494 [ 7800/24965]\n",
      "Training batch loss: 1636.796265 [ 7900/24965]\n",
      "Training batch loss: 1635.395020 [ 8000/24965]\n",
      "Training batch loss: 1561.637329 [ 8100/24965]\n",
      "Training batch loss: 1540.997070 [ 8200/24965]\n",
      "Training batch loss: 1527.306641 [ 8300/24965]\n",
      "Training batch loss: 1372.412109 [ 8400/24965]\n",
      "Training batch loss: 1636.804077 [ 8500/24965]\n",
      "Training batch loss: 1534.847656 [ 8600/24965]\n",
      "Training batch loss: 1602.179443 [ 8700/24965]\n",
      "Training batch loss: 1834.578247 [ 8800/24965]\n",
      "Training batch loss: 1443.357910 [ 8900/24965]\n",
      "Training batch loss: 1517.106812 [ 9000/24965]\n",
      "Training batch loss: 1524.201660 [ 9100/24965]\n",
      "Training batch loss: 1439.973267 [ 9200/24965]\n",
      "Training batch loss: 1575.280884 [ 9300/24965]\n",
      "Training batch loss: 1615.788208 [ 9400/24965]\n",
      "Training batch loss: 1409.271240 [ 9500/24965]\n",
      "Training batch loss: 1622.571167 [ 9600/24965]\n",
      "Training batch loss: 1560.192627 [ 9700/24965]\n",
      "Training batch loss: 1279.661865 [ 9800/24965]\n",
      "Training batch loss: 1514.828003 [ 9900/24965]\n",
      "Training batch loss: 1304.576172 [10000/24965]\n",
      "Training batch loss: 1401.256836 [10100/24965]\n",
      "Training batch loss: 1578.097168 [10200/24965]\n",
      "Training batch loss: 1439.503174 [10300/24965]\n",
      "Training batch loss: 1460.924683 [10400/24965]\n",
      "Training batch loss: 1749.053955 [10500/24965]\n",
      "Training batch loss: 1557.904785 [10600/24965]\n",
      "Training batch loss: 1546.472412 [10700/24965]\n",
      "Training batch loss: 1694.662354 [10800/24965]\n",
      "Training batch loss: 1534.157715 [10900/24965]\n",
      "Training batch loss: 1360.146851 [11000/24965]\n",
      "Training batch loss: 1655.869751 [11100/24965]\n",
      "Training batch loss: 1331.558594 [11200/24965]\n",
      "Training batch loss: 1486.999390 [11300/24965]\n",
      "Training batch loss: 1412.038818 [11400/24965]\n",
      "Training batch loss: 1559.402588 [11500/24965]\n",
      "Training batch loss: 1378.557861 [11600/24965]\n",
      "Training batch loss: 1435.401611 [11700/24965]\n",
      "Training batch loss: 1539.099487 [11800/24965]\n",
      "Training batch loss: 1488.413452 [11900/24965]\n",
      "Training batch loss: 1515.522949 [12000/24965]\n",
      "Training batch loss: 1692.734375 [12100/24965]\n",
      "Training batch loss: 1456.770752 [12200/24965]\n",
      "Training batch loss: 1277.307007 [12300/24965]\n",
      "Training batch loss: 1668.117432 [12400/24965]\n",
      "Training batch loss: 1415.384521 [12500/24965]\n",
      "Training batch loss: 1322.300537 [12600/24965]\n",
      "Training batch loss: 1541.829956 [12700/24965]\n",
      "Training batch loss: 1407.785767 [12800/24965]\n",
      "Training batch loss: 1728.136841 [12900/24965]\n",
      "Training batch loss: 1480.593628 [13000/24965]\n",
      "Training batch loss: 1623.939087 [13100/24965]\n",
      "Training batch loss: 1493.980713 [13200/24965]\n",
      "Training batch loss: 1527.241943 [13300/24965]\n",
      "Training batch loss: 1496.627441 [13400/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1739.799561 [13500/24965]\n",
      "Training batch loss: 1582.417725 [13600/24965]\n",
      "Training batch loss: 1712.798706 [13700/24965]\n",
      "Training batch loss: 1588.728882 [13800/24965]\n",
      "Training batch loss: 1711.854980 [13900/24965]\n",
      "Training batch loss: 1616.841431 [14000/24965]\n",
      "Training batch loss: 1535.154297 [14100/24965]\n",
      "Training batch loss: 1387.232178 [14200/24965]\n",
      "Training batch loss: 1642.568848 [14300/24965]\n",
      "Training batch loss: 1422.808105 [14400/24965]\n",
      "Training batch loss: 1655.029541 [14500/24965]\n",
      "Training batch loss: 1208.858032 [14600/24965]\n",
      "Training batch loss: 1649.759766 [14700/24965]\n",
      "Training batch loss: 1619.447510 [14800/24965]\n",
      "Training batch loss: 1672.494629 [14900/24965]\n",
      "Training batch loss: 1652.381836 [15000/24965]\n",
      "Training batch loss: 1518.801270 [15100/24965]\n",
      "Training batch loss: 1636.125732 [15200/24965]\n",
      "Training batch loss: 1597.594482 [15300/24965]\n",
      "Training batch loss: 1553.082031 [15400/24965]\n",
      "Training batch loss: 1450.116577 [15500/24965]\n",
      "Training batch loss: 1533.791138 [15600/24965]\n",
      "Training batch loss: 1567.565430 [15700/24965]\n",
      "Training batch loss: 1619.253418 [15800/24965]\n",
      "Training batch loss: 1517.673462 [15900/24965]\n",
      "Training batch loss: 1306.900635 [16000/24965]\n",
      "Training batch loss: 1667.135742 [16100/24965]\n",
      "Training batch loss: 1592.355835 [16200/24965]\n",
      "Training batch loss: 1350.141846 [16300/24965]\n",
      "Training batch loss: 1612.301514 [16400/24965]\n",
      "Training batch loss: 1579.926514 [16500/24965]\n",
      "Training batch loss: 1512.889404 [16600/24965]\n",
      "Training batch loss: 1394.606934 [16700/24965]\n",
      "Training batch loss: 1541.670776 [16800/24965]\n",
      "Training batch loss: 1579.153564 [16900/24965]\n",
      "Training batch loss: 1444.711060 [17000/24965]\n",
      "Training batch loss: 1676.358521 [17100/24965]\n",
      "Training batch loss: 1496.136719 [17200/24965]\n",
      "Training batch loss: 1315.779053 [17300/24965]\n",
      "Training batch loss: 1604.408447 [17400/24965]\n",
      "Training batch loss: 1514.586426 [17500/24965]\n",
      "Training batch loss: 1524.275757 [17600/24965]\n",
      "Training batch loss: 1516.842896 [17700/24965]\n",
      "Training batch loss: 1548.017578 [17800/24965]\n",
      "Training batch loss: 1700.228149 [17900/24965]\n",
      "Training batch loss: 1780.228882 [18000/24965]\n",
      "Training batch loss: 1493.709473 [18100/24965]\n",
      "Training batch loss: 1447.145386 [18200/24965]\n",
      "Training batch loss: 1491.329590 [18300/24965]\n",
      "Training batch loss: 1451.573608 [18400/24965]\n",
      "Training batch loss: 1689.025024 [18500/24965]\n",
      "Training batch loss: 1412.300293 [18600/24965]\n",
      "Training batch loss: 1490.197021 [18700/24965]\n",
      "Training batch loss: 1536.669312 [18800/24965]\n",
      "Training batch loss: 1630.434204 [18900/24965]\n",
      "Training batch loss: 1625.843506 [19000/24965]\n",
      "Training batch loss: 1583.471069 [19100/24965]\n",
      "Training batch loss: 1707.298340 [19200/24965]\n",
      "Training batch loss: 1424.465210 [19300/24965]\n",
      "Training batch loss: 1460.512451 [19400/24965]\n",
      "Training batch loss: 1509.739868 [19500/24965]\n",
      "Training batch loss: 1548.120850 [19600/24965]\n",
      "Training batch loss: 1580.499390 [19700/24965]\n",
      "Training batch loss: 1470.853638 [19800/24965]\n",
      "Training batch loss: 1644.812378 [19900/24965]\n",
      "Training batch loss: 1669.726440 [20000/24965]\n",
      "Training batch loss: 1640.039795 [20100/24965]\n",
      "Training batch loss: 1375.238770 [20200/24965]\n",
      "Training batch loss: 1752.333252 [20300/24965]\n",
      "Training batch loss: 1318.700439 [20400/24965]\n",
      "Training batch loss: 1757.274048 [20500/24965]\n",
      "Training batch loss: 1547.134644 [20600/24965]\n",
      "Training batch loss: 1605.097900 [20700/24965]\n",
      "Training batch loss: 1238.467773 [20800/24965]\n",
      "Training batch loss: 1666.993408 [20900/24965]\n",
      "Training batch loss: 1767.575806 [21000/24965]\n",
      "Training batch loss: 1682.814575 [21100/24965]\n",
      "Training batch loss: 1658.277466 [21200/24965]\n",
      "Training batch loss: 1290.223755 [21300/24965]\n",
      "Training batch loss: 1494.984375 [21400/24965]\n",
      "Training batch loss: 1298.677002 [21500/24965]\n",
      "Training batch loss: 1604.907837 [21600/24965]\n",
      "Training batch loss: 1519.085815 [21700/24965]\n",
      "Training batch loss: 1442.884033 [21800/24965]\n",
      "Training batch loss: 1578.894653 [21900/24965]\n",
      "Training batch loss: 1589.049438 [22000/24965]\n",
      "Training batch loss: 1621.278320 [22100/24965]\n",
      "Training batch loss: 1491.569702 [22200/24965]\n",
      "Training batch loss: 1623.374756 [22300/24965]\n",
      "Training batch loss: 1379.927002 [22400/24965]\n",
      "Training batch loss: 1639.255981 [22500/24965]\n",
      "Training batch loss: 1670.613770 [22600/24965]\n",
      "Training batch loss: 1485.729126 [22700/24965]\n",
      "Training batch loss: 1681.325195 [22800/24965]\n",
      "Training batch loss: 1728.550903 [22900/24965]\n",
      "Training batch loss: 1575.296265 [23000/24965]\n",
      "Training batch loss: 1462.857178 [23100/24965]\n",
      "Training batch loss: 1495.882446 [23200/24965]\n",
      "Training batch loss: 1419.812866 [23300/24965]\n",
      "Training batch loss: 1659.707520 [23400/24965]\n",
      "Training batch loss: 1558.592407 [23500/24965]\n",
      "Training batch loss: 1388.364502 [23600/24965]\n",
      "Training batch loss: 1811.565063 [23700/24965]\n",
      "Training batch loss: 1253.017456 [23800/24965]\n",
      "Training batch loss: 1540.294312 [23900/24965]\n",
      "Training batch loss: 1717.965820 [24000/24965]\n",
      "Training batch loss: 1752.358276 [24100/24965]\n",
      "Training batch loss: 1424.976807 [24200/24965]\n",
      "Training batch loss: 1371.241333 [24300/24965]\n",
      "Training batch loss: 1383.655151 [24400/24965]\n",
      "Training batch loss: 1686.133545 [24500/24965]\n",
      "Training batch loss: 1514.228760 [24600/24965]\n",
      "Training batch loss: 1619.713867 [24700/24965]\n",
      "Training batch loss: 1592.350342 [24800/24965]\n",
      "Training batch loss: 1388.671753 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.961337\n",
      "\n",
      "Epoch 7\n",
      "----------------------\n",
      "Training batch loss: 1736.909790 [    0/24965]\n",
      "Training batch loss: 1528.127319 [  100/24965]\n",
      "Training batch loss: 1419.450073 [  200/24965]\n",
      "Training batch loss: 1729.215576 [  300/24965]\n",
      "Training batch loss: 1562.180420 [  400/24965]\n",
      "Training batch loss: 1519.229858 [  500/24965]\n",
      "Training batch loss: 1489.645996 [  600/24965]\n",
      "Training batch loss: 1748.796021 [  700/24965]\n",
      "Training batch loss: 1736.621826 [  800/24965]\n",
      "Training batch loss: 1355.746948 [  900/24965]\n",
      "Training batch loss: 1471.416626 [ 1000/24965]\n",
      "Training batch loss: 1638.534790 [ 1100/24965]\n",
      "Training batch loss: 1620.357544 [ 1200/24965]\n",
      "Training batch loss: 1327.773560 [ 1300/24965]\n",
      "Training batch loss: 1233.453369 [ 1400/24965]\n",
      "Training batch loss: 1568.781982 [ 1500/24965]\n",
      "Training batch loss: 1600.153564 [ 1600/24965]\n",
      "Training batch loss: 1718.653442 [ 1700/24965]\n",
      "Training batch loss: 1563.269165 [ 1800/24965]\n",
      "Training batch loss: 1348.783569 [ 1900/24965]\n",
      "Training batch loss: 1603.312256 [ 2000/24965]\n",
      "Training batch loss: 1491.706421 [ 2100/24965]\n",
      "Training batch loss: 1670.279785 [ 2200/24965]\n",
      "Training batch loss: 1657.126831 [ 2300/24965]\n",
      "Training batch loss: 1663.531616 [ 2400/24965]\n",
      "Training batch loss: 1562.655273 [ 2500/24965]\n",
      "Training batch loss: 1580.785889 [ 2600/24965]\n",
      "Training batch loss: 1454.634766 [ 2700/24965]\n",
      "Training batch loss: 1447.532104 [ 2800/24965]\n",
      "Training batch loss: 1407.409424 [ 2900/24965]\n",
      "Training batch loss: 1483.927856 [ 3000/24965]\n",
      "Training batch loss: 1733.521240 [ 3100/24965]\n",
      "Training batch loss: 1507.932251 [ 3200/24965]\n",
      "Training batch loss: 1602.477539 [ 3300/24965]\n",
      "Training batch loss: 1553.636597 [ 3400/24965]\n",
      "Training batch loss: 1614.317383 [ 3500/24965]\n",
      "Training batch loss: 1591.093628 [ 3600/24965]\n",
      "Training batch loss: 1457.049072 [ 3700/24965]\n",
      "Training batch loss: 1362.495972 [ 3800/24965]\n",
      "Training batch loss: 1625.168701 [ 3900/24965]\n",
      "Training batch loss: 1346.081299 [ 4000/24965]\n",
      "Training batch loss: 1647.262451 [ 4100/24965]\n",
      "Training batch loss: 1556.505371 [ 4200/24965]\n",
      "Training batch loss: 1708.432617 [ 4300/24965]\n",
      "Training batch loss: 1481.467529 [ 4400/24965]\n",
      "Training batch loss: 1459.123657 [ 4500/24965]\n",
      "Training batch loss: 1624.135620 [ 4600/24965]\n",
      "Training batch loss: 1706.446045 [ 4700/24965]\n",
      "Training batch loss: 1531.091675 [ 4800/24965]\n",
      "Training batch loss: 1462.689941 [ 4900/24965]\n",
      "Training batch loss: 1387.352173 [ 5000/24965]\n",
      "Training batch loss: 1576.050903 [ 5100/24965]\n",
      "Training batch loss: 1310.329468 [ 5200/24965]\n",
      "Training batch loss: 1301.679321 [ 5300/24965]\n",
      "Training batch loss: 1519.337036 [ 5400/24965]\n",
      "Training batch loss: 1343.606323 [ 5500/24965]\n",
      "Training batch loss: 1612.404297 [ 5600/24965]\n",
      "Training batch loss: 1434.050781 [ 5700/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1364.312988 [ 5800/24965]\n",
      "Training batch loss: 1661.155762 [ 5900/24965]\n",
      "Training batch loss: 1442.313721 [ 6000/24965]\n",
      "Training batch loss: 1532.512939 [ 6100/24965]\n",
      "Training batch loss: 1516.230713 [ 6200/24965]\n",
      "Training batch loss: 1422.595947 [ 6300/24965]\n",
      "Training batch loss: 1561.768066 [ 6400/24965]\n",
      "Training batch loss: 1425.065430 [ 6500/24965]\n",
      "Training batch loss: 1619.039429 [ 6600/24965]\n",
      "Training batch loss: 1354.949951 [ 6700/24965]\n",
      "Training batch loss: 1501.912231 [ 6800/24965]\n",
      "Training batch loss: 1551.894287 [ 6900/24965]\n",
      "Training batch loss: 1562.266113 [ 7000/24965]\n",
      "Training batch loss: 1361.649902 [ 7100/24965]\n",
      "Training batch loss: 1597.582764 [ 7200/24965]\n",
      "Training batch loss: 1469.941650 [ 7300/24965]\n",
      "Training batch loss: 1655.879639 [ 7400/24965]\n",
      "Training batch loss: 1453.826172 [ 7500/24965]\n",
      "Training batch loss: 1515.516602 [ 7600/24965]\n",
      "Training batch loss: 1676.529785 [ 7700/24965]\n",
      "Training batch loss: 1509.465576 [ 7800/24965]\n",
      "Training batch loss: 1332.396484 [ 7900/24965]\n",
      "Training batch loss: 1511.685303 [ 8000/24965]\n",
      "Training batch loss: 1243.512817 [ 8100/24965]\n",
      "Training batch loss: 1445.256836 [ 8200/24965]\n",
      "Training batch loss: 1755.567871 [ 8300/24965]\n",
      "Training batch loss: 1580.413696 [ 8400/24965]\n",
      "Training batch loss: 1478.222290 [ 8500/24965]\n",
      "Training batch loss: 1511.577271 [ 8600/24965]\n",
      "Training batch loss: 1674.252075 [ 8700/24965]\n",
      "Training batch loss: 1588.676514 [ 8800/24965]\n",
      "Training batch loss: 1558.296509 [ 8900/24965]\n",
      "Training batch loss: 1343.802612 [ 9000/24965]\n",
      "Training batch loss: 1424.796875 [ 9100/24965]\n",
      "Training batch loss: 1404.192993 [ 9200/24965]\n",
      "Training batch loss: 1539.922241 [ 9300/24965]\n",
      "Training batch loss: 1723.472900 [ 9400/24965]\n",
      "Training batch loss: 1580.488770 [ 9500/24965]\n",
      "Training batch loss: 1654.361572 [ 9600/24965]\n",
      "Training batch loss: 1461.972778 [ 9700/24965]\n",
      "Training batch loss: 1577.267334 [ 9800/24965]\n",
      "Training batch loss: 1492.845581 [ 9900/24965]\n",
      "Training batch loss: 1452.853516 [10000/24965]\n",
      "Training batch loss: 1508.994019 [10100/24965]\n",
      "Training batch loss: 1650.692505 [10200/24965]\n",
      "Training batch loss: 1654.972900 [10300/24965]\n",
      "Training batch loss: 1566.272461 [10400/24965]\n",
      "Training batch loss: 1642.111694 [10500/24965]\n",
      "Training batch loss: 1525.083862 [10600/24965]\n",
      "Training batch loss: 1352.199463 [10700/24965]\n",
      "Training batch loss: 1623.785156 [10800/24965]\n",
      "Training batch loss: 1796.875977 [10900/24965]\n",
      "Training batch loss: 1446.782471 [11000/24965]\n",
      "Training batch loss: 1502.266602 [11100/24965]\n",
      "Training batch loss: 1559.950928 [11200/24965]\n",
      "Training batch loss: 1688.622803 [11300/24965]\n",
      "Training batch loss: 1426.174072 [11400/24965]\n",
      "Training batch loss: 1319.808594 [11500/24965]\n",
      "Training batch loss: 1601.624634 [11600/24965]\n",
      "Training batch loss: 1445.085938 [11700/24965]\n",
      "Training batch loss: 1593.846313 [11800/24965]\n",
      "Training batch loss: 1491.617554 [11900/24965]\n",
      "Training batch loss: 1688.862793 [12000/24965]\n",
      "Training batch loss: 1510.977783 [12100/24965]\n",
      "Training batch loss: 1416.748047 [12200/24965]\n",
      "Training batch loss: 1452.815186 [12300/24965]\n",
      "Training batch loss: 1549.953735 [12400/24965]\n",
      "Training batch loss: 1563.494141 [12500/24965]\n",
      "Training batch loss: 1241.967285 [12600/24965]\n",
      "Training batch loss: 1417.035522 [12700/24965]\n",
      "Training batch loss: 1459.547729 [12800/24965]\n",
      "Training batch loss: 1465.757568 [12900/24965]\n",
      "Training batch loss: 1615.742798 [13000/24965]\n",
      "Training batch loss: 1436.712158 [13100/24965]\n",
      "Training batch loss: 1642.753052 [13200/24965]\n",
      "Training batch loss: 1553.817505 [13300/24965]\n",
      "Training batch loss: 1459.148071 [13400/24965]\n",
      "Training batch loss: 1386.317383 [13500/24965]\n",
      "Training batch loss: 1699.675293 [13600/24965]\n",
      "Training batch loss: 1636.464966 [13700/24965]\n",
      "Training batch loss: 1496.518555 [13800/24965]\n",
      "Training batch loss: 1712.113403 [13900/24965]\n",
      "Training batch loss: 1545.500732 [14000/24965]\n",
      "Training batch loss: 1521.071655 [14100/24965]\n",
      "Training batch loss: 1460.910156 [14200/24965]\n",
      "Training batch loss: 1674.427246 [14300/24965]\n",
      "Training batch loss: 1606.778076 [14400/24965]\n",
      "Training batch loss: 1289.387085 [14500/24965]\n",
      "Training batch loss: 1579.313477 [14600/24965]\n",
      "Training batch loss: 1644.396118 [14700/24965]\n",
      "Training batch loss: 1488.512207 [14800/24965]\n",
      "Training batch loss: 1650.122314 [14900/24965]\n",
      "Training batch loss: 1422.474365 [15000/24965]\n",
      "Training batch loss: 1446.827881 [15100/24965]\n",
      "Training batch loss: 1545.451172 [15200/24965]\n",
      "Training batch loss: 1433.421387 [15300/24965]\n",
      "Training batch loss: 1461.466431 [15400/24965]\n",
      "Training batch loss: 1230.635986 [15500/24965]\n",
      "Training batch loss: 1415.430298 [15600/24965]\n",
      "Training batch loss: 1533.158447 [15700/24965]\n",
      "Training batch loss: 1608.944580 [15800/24965]\n",
      "Training batch loss: 1603.064209 [15900/24965]\n",
      "Training batch loss: 1766.273804 [16000/24965]\n",
      "Training batch loss: 1533.299927 [16100/24965]\n",
      "Training batch loss: 1724.118652 [16200/24965]\n",
      "Training batch loss: 1773.808472 [16300/24965]\n",
      "Training batch loss: 1767.986206 [16400/24965]\n",
      "Training batch loss: 1486.831543 [16500/24965]\n",
      "Training batch loss: 1670.685669 [16600/24965]\n",
      "Training batch loss: 1537.909912 [16700/24965]\n",
      "Training batch loss: 1658.281250 [16800/24965]\n",
      "Training batch loss: 1676.717529 [16900/24965]\n",
      "Training batch loss: 1716.670532 [17000/24965]\n",
      "Training batch loss: 1605.171509 [17100/24965]\n",
      "Training batch loss: 1399.009399 [17200/24965]\n",
      "Training batch loss: 1498.515259 [17300/24965]\n",
      "Training batch loss: 1534.235962 [17400/24965]\n",
      "Training batch loss: 1583.934082 [17500/24965]\n",
      "Training batch loss: 1488.761353 [17600/24965]\n",
      "Training batch loss: 1434.323608 [17700/24965]\n",
      "Training batch loss: 1610.874512 [17800/24965]\n",
      "Training batch loss: 1567.367554 [17900/24965]\n",
      "Training batch loss: 1596.373779 [18000/24965]\n",
      "Training batch loss: 1564.268311 [18100/24965]\n",
      "Training batch loss: 1560.997192 [18200/24965]\n",
      "Training batch loss: 1410.961548 [18300/24965]\n",
      "Training batch loss: 1387.175293 [18400/24965]\n",
      "Training batch loss: 1623.988647 [18500/24965]\n",
      "Training batch loss: 1573.654785 [18600/24965]\n",
      "Training batch loss: 1472.435303 [18700/24965]\n",
      "Training batch loss: 1529.606445 [18800/24965]\n",
      "Training batch loss: 1463.358521 [18900/24965]\n",
      "Training batch loss: 1554.486084 [19000/24965]\n",
      "Training batch loss: 1444.443848 [19100/24965]\n",
      "Training batch loss: 1542.260376 [19200/24965]\n",
      "Training batch loss: 1411.186279 [19300/24965]\n",
      "Training batch loss: 1445.062012 [19400/24965]\n",
      "Training batch loss: 1686.596069 [19500/24965]\n",
      "Training batch loss: 1631.827271 [19600/24965]\n",
      "Training batch loss: 1385.684204 [19700/24965]\n",
      "Training batch loss: 1419.239380 [19800/24965]\n",
      "Training batch loss: 1770.899780 [19900/24965]\n",
      "Training batch loss: 1616.683105 [20000/24965]\n",
      "Training batch loss: 1451.840820 [20100/24965]\n",
      "Training batch loss: 1463.439819 [20200/24965]\n",
      "Training batch loss: 1373.122925 [20300/24965]\n",
      "Training batch loss: 1366.812256 [20400/24965]\n",
      "Training batch loss: 1603.952393 [20500/24965]\n",
      "Training batch loss: 1454.326050 [20600/24965]\n",
      "Training batch loss: 1447.189331 [20700/24965]\n",
      "Training batch loss: 1603.964478 [20800/24965]\n",
      "Training batch loss: 1499.969727 [20900/24965]\n",
      "Training batch loss: 1538.920654 [21000/24965]\n",
      "Training batch loss: 1425.591309 [21100/24965]\n",
      "Training batch loss: 1396.544922 [21200/24965]\n",
      "Training batch loss: 1496.871338 [21300/24965]\n",
      "Training batch loss: 1399.279785 [21400/24965]\n",
      "Training batch loss: 1482.728760 [21500/24965]\n",
      "Training batch loss: 1581.615234 [21600/24965]\n",
      "Training batch loss: 1565.623901 [21700/24965]\n",
      "Training batch loss: 1634.764771 [21800/24965]\n",
      "Training batch loss: 1553.225586 [21900/24965]\n",
      "Training batch loss: 1540.156494 [22000/24965]\n",
      "Training batch loss: 1618.113770 [22100/24965]\n",
      "Training batch loss: 1535.984375 [22200/24965]\n",
      "Training batch loss: 1441.744141 [22300/24965]\n",
      "Training batch loss: 1615.231201 [22400/24965]\n",
      "Training batch loss: 1483.952759 [22500/24965]\n",
      "Training batch loss: 1486.249390 [22600/24965]\n",
      "Training batch loss: 1613.740967 [22700/24965]\n",
      "Training batch loss: 1615.758057 [22800/24965]\n",
      "Training batch loss: 1465.896118 [22900/24965]\n",
      "Training batch loss: 1564.350342 [23000/24965]\n",
      "Training batch loss: 1457.123291 [23100/24965]\n",
      "Training batch loss: 1682.137695 [23200/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1433.591064 [23300/24965]\n",
      "Training batch loss: 1499.760620 [23400/24965]\n",
      "Training batch loss: 1652.404419 [23500/24965]\n",
      "Training batch loss: 1495.310547 [23600/24965]\n",
      "Training batch loss: 1584.729248 [23700/24965]\n",
      "Training batch loss: 1466.127686 [23800/24965]\n",
      "Training batch loss: 1695.906372 [23900/24965]\n",
      "Training batch loss: 1630.242065 [24000/24965]\n",
      "Training batch loss: 1562.821533 [24100/24965]\n",
      "Training batch loss: 1493.789551 [24200/24965]\n",
      "Training batch loss: 1439.615845 [24300/24965]\n",
      "Training batch loss: 1399.052002 [24400/24965]\n",
      "Training batch loss: 1690.518677 [24500/24965]\n",
      "Training batch loss: 1585.938477 [24600/24965]\n",
      "Training batch loss: 1555.505371 [24700/24965]\n",
      "Training batch loss: 1489.509277 [24800/24965]\n",
      "Training batch loss: 1457.419067 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.852643\n",
      "\n",
      "Saved best new model with val_loss: 1512.8526\n",
      "Epoch 8\n",
      "----------------------\n",
      "Training batch loss: 1474.605347 [    0/24965]\n",
      "Training batch loss: 1564.360229 [  100/24965]\n",
      "Training batch loss: 1535.066650 [  200/24965]\n",
      "Training batch loss: 1453.737915 [  300/24965]\n",
      "Training batch loss: 1403.806519 [  400/24965]\n",
      "Training batch loss: 1463.170898 [  500/24965]\n",
      "Training batch loss: 1501.913208 [  600/24965]\n",
      "Training batch loss: 1277.867676 [  700/24965]\n",
      "Training batch loss: 1610.669067 [  800/24965]\n",
      "Training batch loss: 1576.484619 [  900/24965]\n",
      "Training batch loss: 1407.909302 [ 1000/24965]\n",
      "Training batch loss: 1430.483521 [ 1100/24965]\n",
      "Training batch loss: 1752.630127 [ 1200/24965]\n",
      "Training batch loss: 1585.276123 [ 1300/24965]\n",
      "Training batch loss: 1421.534546 [ 1400/24965]\n",
      "Training batch loss: 1510.107910 [ 1500/24965]\n",
      "Training batch loss: 1379.546753 [ 1600/24965]\n",
      "Training batch loss: 1380.815308 [ 1700/24965]\n",
      "Training batch loss: 1337.624023 [ 1800/24965]\n",
      "Training batch loss: 1387.701538 [ 1900/24965]\n",
      "Training batch loss: 1552.485596 [ 2000/24965]\n",
      "Training batch loss: 1601.189819 [ 2100/24965]\n",
      "Training batch loss: 1499.069336 [ 2200/24965]\n",
      "Training batch loss: 1567.785889 [ 2300/24965]\n",
      "Training batch loss: 1461.476074 [ 2400/24965]\n",
      "Training batch loss: 1306.897827 [ 2500/24965]\n",
      "Training batch loss: 1712.411621 [ 2600/24965]\n",
      "Training batch loss: 1750.230469 [ 2700/24965]\n",
      "Training batch loss: 1565.690063 [ 2800/24965]\n",
      "Training batch loss: 1504.338989 [ 2900/24965]\n",
      "Training batch loss: 1230.332275 [ 3000/24965]\n",
      "Training batch loss: 1391.114258 [ 3100/24965]\n",
      "Training batch loss: 1732.531006 [ 3200/24965]\n",
      "Training batch loss: 1583.872437 [ 3300/24965]\n",
      "Training batch loss: 1527.365723 [ 3400/24965]\n",
      "Training batch loss: 1530.277466 [ 3500/24965]\n",
      "Training batch loss: 1305.077637 [ 3600/24965]\n",
      "Training batch loss: 1800.416748 [ 3700/24965]\n",
      "Training batch loss: 1813.141724 [ 3800/24965]\n",
      "Training batch loss: 1541.421387 [ 3900/24965]\n",
      "Training batch loss: 1648.813599 [ 4000/24965]\n",
      "Training batch loss: 1597.371460 [ 4100/24965]\n",
      "Training batch loss: 1536.553223 [ 4200/24965]\n",
      "Training batch loss: 1539.691895 [ 4300/24965]\n",
      "Training batch loss: 1340.027222 [ 4400/24965]\n",
      "Training batch loss: 1610.379028 [ 4500/24965]\n",
      "Training batch loss: 1599.373169 [ 4600/24965]\n",
      "Training batch loss: 1394.013428 [ 4700/24965]\n",
      "Training batch loss: 1470.696289 [ 4800/24965]\n",
      "Training batch loss: 1580.775757 [ 4900/24965]\n",
      "Training batch loss: 1641.859009 [ 5000/24965]\n",
      "Training batch loss: 1651.839600 [ 5100/24965]\n",
      "Training batch loss: 1705.189697 [ 5200/24965]\n",
      "Training batch loss: 1626.922607 [ 5300/24965]\n",
      "Training batch loss: 1681.578857 [ 5400/24965]\n",
      "Training batch loss: 1507.151611 [ 5500/24965]\n",
      "Training batch loss: 1462.150513 [ 5600/24965]\n",
      "Training batch loss: 1662.752441 [ 5700/24965]\n",
      "Training batch loss: 1494.056274 [ 5800/24965]\n",
      "Training batch loss: 1506.749878 [ 5900/24965]\n",
      "Training batch loss: 1625.435669 [ 6000/24965]\n",
      "Training batch loss: 1400.425903 [ 6100/24965]\n",
      "Training batch loss: 1495.723755 [ 6200/24965]\n",
      "Training batch loss: 1498.309814 [ 6300/24965]\n",
      "Training batch loss: 1457.688843 [ 6400/24965]\n",
      "Training batch loss: 1455.651855 [ 6500/24965]\n",
      "Training batch loss: 1210.406372 [ 6600/24965]\n",
      "Training batch loss: 1557.811401 [ 6700/24965]\n",
      "Training batch loss: 1364.597168 [ 6800/24965]\n",
      "Training batch loss: 1626.145264 [ 6900/24965]\n",
      "Training batch loss: 1696.929199 [ 7000/24965]\n",
      "Training batch loss: 1437.262817 [ 7100/24965]\n",
      "Training batch loss: 1554.406860 [ 7200/24965]\n",
      "Training batch loss: 1451.456177 [ 7300/24965]\n",
      "Training batch loss: 1258.755371 [ 7400/24965]\n",
      "Training batch loss: 1666.598755 [ 7500/24965]\n",
      "Training batch loss: 1435.659302 [ 7600/24965]\n",
      "Training batch loss: 1373.492676 [ 7700/24965]\n",
      "Training batch loss: 1604.267700 [ 7800/24965]\n",
      "Training batch loss: 1599.441162 [ 7900/24965]\n",
      "Training batch loss: 1568.031006 [ 8000/24965]\n",
      "Training batch loss: 1640.992554 [ 8100/24965]\n",
      "Training batch loss: 1434.609863 [ 8200/24965]\n",
      "Training batch loss: 1649.855957 [ 8300/24965]\n",
      "Training batch loss: 1412.786133 [ 8400/24965]\n",
      "Training batch loss: 1807.183594 [ 8500/24965]\n",
      "Training batch loss: 1615.165527 [ 8600/24965]\n",
      "Training batch loss: 1723.867432 [ 8700/24965]\n",
      "Training batch loss: 1429.736572 [ 8800/24965]\n",
      "Training batch loss: 1542.440430 [ 8900/24965]\n",
      "Training batch loss: 1606.940552 [ 9000/24965]\n",
      "Training batch loss: 1694.213257 [ 9100/24965]\n",
      "Training batch loss: 1358.703125 [ 9200/24965]\n",
      "Training batch loss: 1716.514404 [ 9300/24965]\n",
      "Training batch loss: 1651.355713 [ 9400/24965]\n",
      "Training batch loss: 1717.159058 [ 9500/24965]\n",
      "Training batch loss: 1531.684204 [ 9600/24965]\n",
      "Training batch loss: 1477.520508 [ 9700/24965]\n",
      "Training batch loss: 1627.590698 [ 9800/24965]\n",
      "Training batch loss: 1511.909546 [ 9900/24965]\n",
      "Training batch loss: 1571.283691 [10000/24965]\n",
      "Training batch loss: 1297.182251 [10100/24965]\n",
      "Training batch loss: 1654.696777 [10200/24965]\n",
      "Training batch loss: 1565.042480 [10300/24965]\n",
      "Training batch loss: 1326.252441 [10400/24965]\n",
      "Training batch loss: 1427.334229 [10500/24965]\n",
      "Training batch loss: 1363.227173 [10600/24965]\n",
      "Training batch loss: 1398.317505 [10700/24965]\n",
      "Training batch loss: 1576.543091 [10800/24965]\n",
      "Training batch loss: 1610.612427 [10900/24965]\n",
      "Training batch loss: 1263.793823 [11000/24965]\n",
      "Training batch loss: 1293.342041 [11100/24965]\n",
      "Training batch loss: 1384.334473 [11200/24965]\n",
      "Training batch loss: 1399.398193 [11300/24965]\n",
      "Training batch loss: 1393.150635 [11400/24965]\n",
      "Training batch loss: 1677.997437 [11500/24965]\n",
      "Training batch loss: 1390.760986 [11600/24965]\n",
      "Training batch loss: 1192.629761 [11700/24965]\n",
      "Training batch loss: 1536.214355 [11800/24965]\n",
      "Training batch loss: 1582.712646 [11900/24965]\n",
      "Training batch loss: 1688.229370 [12000/24965]\n",
      "Training batch loss: 1590.402710 [12100/24965]\n",
      "Training batch loss: 1485.106201 [12200/24965]\n",
      "Training batch loss: 1491.950684 [12300/24965]\n",
      "Training batch loss: 1554.661133 [12400/24965]\n",
      "Training batch loss: 1767.151367 [12500/24965]\n",
      "Training batch loss: 1394.814209 [12600/24965]\n",
      "Training batch loss: 1514.589844 [12700/24965]\n",
      "Training batch loss: 1545.871216 [12800/24965]\n",
      "Training batch loss: 1464.949951 [12900/24965]\n",
      "Training batch loss: 1202.254272 [13000/24965]\n",
      "Training batch loss: 1641.344971 [13100/24965]\n",
      "Training batch loss: 1743.666016 [13200/24965]\n",
      "Training batch loss: 1585.880859 [13300/24965]\n",
      "Training batch loss: 1322.628296 [13400/24965]\n",
      "Training batch loss: 1493.445312 [13500/24965]\n",
      "Training batch loss: 1257.012329 [13600/24965]\n",
      "Training batch loss: 1531.722168 [13700/24965]\n",
      "Training batch loss: 1432.697754 [13800/24965]\n",
      "Training batch loss: 1367.714600 [13900/24965]\n",
      "Training batch loss: 1555.120728 [14000/24965]\n",
      "Training batch loss: 1472.663574 [14100/24965]\n",
      "Training batch loss: 1582.430298 [14200/24965]\n",
      "Training batch loss: 1417.555664 [14300/24965]\n",
      "Training batch loss: 1551.119019 [14400/24965]\n",
      "Training batch loss: 1781.315430 [14500/24965]\n",
      "Training batch loss: 1449.710205 [14600/24965]\n",
      "Training batch loss: 1522.675781 [14700/24965]\n",
      "Training batch loss: 1291.843384 [14800/24965]\n",
      "Training batch loss: 1533.755493 [14900/24965]\n",
      "Training batch loss: 1471.887085 [15000/24965]\n",
      "Training batch loss: 1433.705566 [15100/24965]\n",
      "Training batch loss: 1412.960205 [15200/24965]\n",
      "Training batch loss: 1409.088501 [15300/24965]\n",
      "Training batch loss: 1397.142700 [15400/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1492.590576 [15500/24965]\n",
      "Training batch loss: 1442.728638 [15600/24965]\n",
      "Training batch loss: 1589.690308 [15700/24965]\n",
      "Training batch loss: 1468.378174 [15800/24965]\n",
      "Training batch loss: 1456.079468 [15900/24965]\n",
      "Training batch loss: 1583.744507 [16000/24965]\n",
      "Training batch loss: 1486.659668 [16100/24965]\n",
      "Training batch loss: 1656.908081 [16200/24965]\n",
      "Training batch loss: 1753.070312 [16300/24965]\n",
      "Training batch loss: 1524.675781 [16400/24965]\n",
      "Training batch loss: 1567.188843 [16500/24965]\n",
      "Training batch loss: 1557.633545 [16600/24965]\n",
      "Training batch loss: 1460.766235 [16700/24965]\n",
      "Training batch loss: 1559.666016 [16800/24965]\n",
      "Training batch loss: 1514.164551 [16900/24965]\n",
      "Training batch loss: 1320.965088 [17000/24965]\n",
      "Training batch loss: 1235.179199 [17100/24965]\n",
      "Training batch loss: 1552.304199 [17200/24965]\n",
      "Training batch loss: 1288.289551 [17300/24965]\n",
      "Training batch loss: 1456.307617 [17400/24965]\n",
      "Training batch loss: 1499.489990 [17500/24965]\n",
      "Training batch loss: 1335.427734 [17600/24965]\n",
      "Training batch loss: 1471.026611 [17700/24965]\n",
      "Training batch loss: 1220.782227 [17800/24965]\n",
      "Training batch loss: 1719.994263 [17900/24965]\n",
      "Training batch loss: 1776.682861 [18000/24965]\n",
      "Training batch loss: 1405.297363 [18100/24965]\n",
      "Training batch loss: 1636.404785 [18200/24965]\n",
      "Training batch loss: 1525.055908 [18300/24965]\n",
      "Training batch loss: 1454.010132 [18400/24965]\n",
      "Training batch loss: 1418.991577 [18500/24965]\n",
      "Training batch loss: 1663.162109 [18600/24965]\n",
      "Training batch loss: 1645.654785 [18700/24965]\n",
      "Training batch loss: 1717.348267 [18800/24965]\n",
      "Training batch loss: 1541.797119 [18900/24965]\n",
      "Training batch loss: 1542.570190 [19000/24965]\n",
      "Training batch loss: 1644.297729 [19100/24965]\n",
      "Training batch loss: 1493.744263 [19200/24965]\n",
      "Training batch loss: 1464.706299 [19300/24965]\n",
      "Training batch loss: 1458.096680 [19400/24965]\n",
      "Training batch loss: 1449.254150 [19500/24965]\n",
      "Training batch loss: 1484.425659 [19600/24965]\n",
      "Training batch loss: 1575.719604 [19700/24965]\n",
      "Training batch loss: 1462.226562 [19800/24965]\n",
      "Training batch loss: 1489.445068 [19900/24965]\n",
      "Training batch loss: 1558.391968 [20000/24965]\n",
      "Training batch loss: 1733.283936 [20100/24965]\n",
      "Training batch loss: 1396.684326 [20200/24965]\n",
      "Training batch loss: 1611.517334 [20300/24965]\n",
      "Training batch loss: 1462.283081 [20400/24965]\n",
      "Training batch loss: 1553.388550 [20500/24965]\n",
      "Training batch loss: 1552.809448 [20600/24965]\n",
      "Training batch loss: 1488.880493 [20700/24965]\n",
      "Training batch loss: 1729.274780 [20800/24965]\n",
      "Training batch loss: 1444.217407 [20900/24965]\n",
      "Training batch loss: 1434.780762 [21000/24965]\n",
      "Training batch loss: 1592.428711 [21100/24965]\n",
      "Training batch loss: 1606.965942 [21200/24965]\n",
      "Training batch loss: 1363.065674 [21300/24965]\n",
      "Training batch loss: 1616.366577 [21400/24965]\n",
      "Training batch loss: 1626.042114 [21500/24965]\n",
      "Training batch loss: 1618.501099 [21600/24965]\n",
      "Training batch loss: 1774.000732 [21700/24965]\n",
      "Training batch loss: 1513.678467 [21800/24965]\n",
      "Training batch loss: 1535.482422 [21900/24965]\n",
      "Training batch loss: 1337.125244 [22000/24965]\n",
      "Training batch loss: 1694.258057 [22100/24965]\n",
      "Training batch loss: 1669.978027 [22200/24965]\n",
      "Training batch loss: 1699.273438 [22300/24965]\n",
      "Training batch loss: 1568.912842 [22400/24965]\n",
      "Training batch loss: 1828.971191 [22500/24965]\n",
      "Training batch loss: 1340.345581 [22600/24965]\n",
      "Training batch loss: 1652.682861 [22700/24965]\n",
      "Training batch loss: 1533.665527 [22800/24965]\n",
      "Training batch loss: 1250.877197 [22900/24965]\n",
      "Training batch loss: 1420.923706 [23000/24965]\n",
      "Training batch loss: 1710.107910 [23100/24965]\n",
      "Training batch loss: 1546.828735 [23200/24965]\n",
      "Training batch loss: 1778.968018 [23300/24965]\n",
      "Training batch loss: 1587.350220 [23400/24965]\n",
      "Training batch loss: 1417.522827 [23500/24965]\n",
      "Training batch loss: 1533.181885 [23600/24965]\n",
      "Training batch loss: 1461.010376 [23700/24965]\n",
      "Training batch loss: 1488.299561 [23800/24965]\n",
      "Training batch loss: 1603.121582 [23900/24965]\n",
      "Training batch loss: 1644.890869 [24000/24965]\n",
      "Training batch loss: 1378.182617 [24100/24965]\n",
      "Training batch loss: 1530.944214 [24200/24965]\n",
      "Training batch loss: 1637.778564 [24300/24965]\n",
      "Training batch loss: 1940.452515 [24400/24965]\n",
      "Training batch loss: 1466.417358 [24500/24965]\n",
      "Training batch loss: 1306.444702 [24600/24965]\n",
      "Training batch loss: 1483.686646 [24700/24965]\n",
      "Training batch loss: 1285.179810 [24800/24965]\n",
      "Training batch loss: 1761.221069 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.813232\n",
      "\n",
      "Saved best new model with val_loss: 1512.8132\n",
      "Epoch 9\n",
      "----------------------\n",
      "Training batch loss: 1629.292969 [    0/24965]\n",
      "Training batch loss: 1598.035400 [  100/24965]\n",
      "Training batch loss: 1542.350342 [  200/24965]\n",
      "Training batch loss: 1499.435791 [  300/24965]\n",
      "Training batch loss: 1521.175293 [  400/24965]\n",
      "Training batch loss: 1574.781982 [  500/24965]\n",
      "Training batch loss: 1619.679321 [  600/24965]\n",
      "Training batch loss: 1559.267212 [  700/24965]\n",
      "Training batch loss: 1631.454346 [  800/24965]\n",
      "Training batch loss: 1356.002686 [  900/24965]\n",
      "Training batch loss: 1496.606934 [ 1000/24965]\n",
      "Training batch loss: 1433.886108 [ 1100/24965]\n",
      "Training batch loss: 1526.892334 [ 1200/24965]\n",
      "Training batch loss: 1630.811035 [ 1300/24965]\n",
      "Training batch loss: 1704.224609 [ 1400/24965]\n",
      "Training batch loss: 1497.243774 [ 1500/24965]\n",
      "Training batch loss: 1481.921387 [ 1600/24965]\n",
      "Training batch loss: 1537.273682 [ 1700/24965]\n",
      "Training batch loss: 1252.789429 [ 1800/24965]\n",
      "Training batch loss: 1479.753418 [ 1900/24965]\n",
      "Training batch loss: 1635.518799 [ 2000/24965]\n",
      "Training batch loss: 1442.610840 [ 2100/24965]\n",
      "Training batch loss: 1531.594971 [ 2200/24965]\n",
      "Training batch loss: 1427.969727 [ 2300/24965]\n",
      "Training batch loss: 1491.832275 [ 2400/24965]\n",
      "Training batch loss: 1548.100586 [ 2500/24965]\n",
      "Training batch loss: 1430.213745 [ 2600/24965]\n",
      "Training batch loss: 1629.850464 [ 2700/24965]\n",
      "Training batch loss: 1551.763306 [ 2800/24965]\n",
      "Training batch loss: 1613.891113 [ 2900/24965]\n",
      "Training batch loss: 1470.746948 [ 3000/24965]\n",
      "Training batch loss: 1743.660156 [ 3100/24965]\n",
      "Training batch loss: 1322.931885 [ 3200/24965]\n",
      "Training batch loss: 1351.955688 [ 3300/24965]\n",
      "Training batch loss: 1768.742920 [ 3400/24965]\n",
      "Training batch loss: 1508.691895 [ 3500/24965]\n",
      "Training batch loss: 1661.540649 [ 3600/24965]\n",
      "Training batch loss: 1753.375977 [ 3700/24965]\n",
      "Training batch loss: 1593.299805 [ 3800/24965]\n",
      "Training batch loss: 1591.737549 [ 3900/24965]\n",
      "Training batch loss: 1638.554443 [ 4000/24965]\n",
      "Training batch loss: 1689.473389 [ 4100/24965]\n",
      "Training batch loss: 1578.345215 [ 4200/24965]\n",
      "Training batch loss: 1644.839355 [ 4300/24965]\n",
      "Training batch loss: 1572.299561 [ 4400/24965]\n",
      "Training batch loss: 1585.166504 [ 4500/24965]\n",
      "Training batch loss: 1717.435059 [ 4600/24965]\n",
      "Training batch loss: 1462.566650 [ 4700/24965]\n",
      "Training batch loss: 1501.598633 [ 4800/24965]\n",
      "Training batch loss: 1515.502197 [ 4900/24965]\n",
      "Training batch loss: 1468.036133 [ 5000/24965]\n",
      "Training batch loss: 1529.420044 [ 5100/24965]\n",
      "Training batch loss: 1520.633179 [ 5200/24965]\n",
      "Training batch loss: 1461.486450 [ 5300/24965]\n",
      "Training batch loss: 1602.733398 [ 5400/24965]\n",
      "Training batch loss: 1617.745728 [ 5500/24965]\n",
      "Training batch loss: 1666.619141 [ 5600/24965]\n",
      "Training batch loss: 1617.492676 [ 5700/24965]\n",
      "Training batch loss: 1559.496094 [ 5800/24965]\n",
      "Training batch loss: 1428.765259 [ 5900/24965]\n",
      "Training batch loss: 1505.624512 [ 6000/24965]\n",
      "Training batch loss: 1488.759033 [ 6100/24965]\n",
      "Training batch loss: 1507.575806 [ 6200/24965]\n",
      "Training batch loss: 1444.819458 [ 6300/24965]\n",
      "Training batch loss: 1568.456299 [ 6400/24965]\n",
      "Training batch loss: 1515.737671 [ 6500/24965]\n",
      "Training batch loss: 1604.307861 [ 6600/24965]\n",
      "Training batch loss: 1434.205933 [ 6700/24965]\n",
      "Training batch loss: 1712.856689 [ 6800/24965]\n",
      "Training batch loss: 1637.329590 [ 6900/24965]\n",
      "Training batch loss: 1545.358276 [ 7000/24965]\n",
      "Training batch loss: 1487.401733 [ 7100/24965]\n",
      "Training batch loss: 1542.264282 [ 7200/24965]\n",
      "Training batch loss: 1320.630859 [ 7300/24965]\n",
      "Training batch loss: 1369.196899 [ 7400/24965]\n",
      "Training batch loss: 1535.551392 [ 7500/24965]\n",
      "Training batch loss: 1357.610718 [ 7600/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1525.964233 [ 7700/24965]\n",
      "Training batch loss: 1642.516479 [ 7800/24965]\n",
      "Training batch loss: 1547.167114 [ 7900/24965]\n",
      "Training batch loss: 1408.508423 [ 8000/24965]\n",
      "Training batch loss: 1514.239136 [ 8100/24965]\n",
      "Training batch loss: 1503.700928 [ 8200/24965]\n",
      "Training batch loss: 1670.320312 [ 8300/24965]\n",
      "Training batch loss: 1594.161133 [ 8400/24965]\n",
      "Training batch loss: 1382.493286 [ 8500/24965]\n",
      "Training batch loss: 1659.177002 [ 8600/24965]\n",
      "Training batch loss: 1480.062988 [ 8700/24965]\n",
      "Training batch loss: 1783.317139 [ 8800/24965]\n",
      "Training batch loss: 1694.484863 [ 8900/24965]\n",
      "Training batch loss: 1737.347900 [ 9000/24965]\n",
      "Training batch loss: 1687.156616 [ 9100/24965]\n",
      "Training batch loss: 1557.188477 [ 9200/24965]\n",
      "Training batch loss: 1709.616577 [ 9300/24965]\n",
      "Training batch loss: 1703.900757 [ 9400/24965]\n",
      "Training batch loss: 1508.000732 [ 9500/24965]\n",
      "Training batch loss: 1401.359131 [ 9600/24965]\n",
      "Training batch loss: 1418.184814 [ 9700/24965]\n",
      "Training batch loss: 1521.742920 [ 9800/24965]\n",
      "Training batch loss: 1611.694336 [ 9900/24965]\n",
      "Training batch loss: 1497.947388 [10000/24965]\n",
      "Training batch loss: 1551.202759 [10100/24965]\n",
      "Training batch loss: 1587.604004 [10200/24965]\n",
      "Training batch loss: 1579.006226 [10300/24965]\n",
      "Training batch loss: 1564.911865 [10400/24965]\n",
      "Training batch loss: 1635.320068 [10500/24965]\n",
      "Training batch loss: 1643.369141 [10600/24965]\n",
      "Training batch loss: 1650.421631 [10700/24965]\n",
      "Training batch loss: 1405.420166 [10800/24965]\n",
      "Training batch loss: 1323.551514 [10900/24965]\n",
      "Training batch loss: 1530.928833 [11000/24965]\n",
      "Training batch loss: 1489.907227 [11100/24965]\n",
      "Training batch loss: 1553.255981 [11200/24965]\n",
      "Training batch loss: 1736.455322 [11300/24965]\n",
      "Training batch loss: 1557.741455 [11400/24965]\n",
      "Training batch loss: 1573.020630 [11500/24965]\n",
      "Training batch loss: 1479.667969 [11600/24965]\n",
      "Training batch loss: 1541.597900 [11700/24965]\n",
      "Training batch loss: 1635.588501 [11800/24965]\n",
      "Training batch loss: 1645.644653 [11900/24965]\n",
      "Training batch loss: 1523.378662 [12000/24965]\n",
      "Training batch loss: 1540.861572 [12100/24965]\n",
      "Training batch loss: 1392.692627 [12200/24965]\n",
      "Training batch loss: 1261.956055 [12300/24965]\n",
      "Training batch loss: 1569.467041 [12400/24965]\n",
      "Training batch loss: 1672.049927 [12500/24965]\n",
      "Training batch loss: 1416.950195 [12600/24965]\n",
      "Training batch loss: 1354.392334 [12700/24965]\n",
      "Training batch loss: 1728.922363 [12800/24965]\n",
      "Training batch loss: 1649.170288 [12900/24965]\n",
      "Training batch loss: 1391.072388 [13000/24965]\n",
      "Training batch loss: 1520.340820 [13100/24965]\n",
      "Training batch loss: 1606.033203 [13200/24965]\n",
      "Training batch loss: 1330.570435 [13300/24965]\n",
      "Training batch loss: 1558.702148 [13400/24965]\n",
      "Training batch loss: 1469.965332 [13500/24965]\n",
      "Training batch loss: 1165.059570 [13600/24965]\n",
      "Training batch loss: 1455.679810 [13700/24965]\n",
      "Training batch loss: 1580.295776 [13800/24965]\n",
      "Training batch loss: 1537.753418 [13900/24965]\n",
      "Training batch loss: 1670.960327 [14000/24965]\n",
      "Training batch loss: 1561.547363 [14100/24965]\n",
      "Training batch loss: 1537.736328 [14200/24965]\n",
      "Training batch loss: 1424.850342 [14300/24965]\n",
      "Training batch loss: 1318.410889 [14400/24965]\n",
      "Training batch loss: 1592.833496 [14500/24965]\n",
      "Training batch loss: 1573.870850 [14600/24965]\n",
      "Training batch loss: 1444.487793 [14700/24965]\n",
      "Training batch loss: 1569.073486 [14800/24965]\n",
      "Training batch loss: 1364.588989 [14900/24965]\n",
      "Training batch loss: 1408.977295 [15000/24965]\n",
      "Training batch loss: 1469.307617 [15100/24965]\n",
      "Training batch loss: 1597.473999 [15200/24965]\n",
      "Training batch loss: 1667.919556 [15300/24965]\n",
      "Training batch loss: 1395.554688 [15400/24965]\n",
      "Training batch loss: 1400.909546 [15500/24965]\n",
      "Training batch loss: 1678.974609 [15600/24965]\n",
      "Training batch loss: 1508.968262 [15700/24965]\n",
      "Training batch loss: 1414.820435 [15800/24965]\n",
      "Training batch loss: 1563.559692 [15900/24965]\n",
      "Training batch loss: 1642.099976 [16000/24965]\n",
      "Training batch loss: 1660.431519 [16100/24965]\n",
      "Training batch loss: 1404.520752 [16200/24965]\n",
      "Training batch loss: 1395.170410 [16300/24965]\n",
      "Training batch loss: 1559.485474 [16400/24965]\n",
      "Training batch loss: 1562.938232 [16500/24965]\n",
      "Training batch loss: 1501.436157 [16600/24965]\n",
      "Training batch loss: 1522.863770 [16700/24965]\n",
      "Training batch loss: 1413.171997 [16800/24965]\n",
      "Training batch loss: 1572.734497 [16900/24965]\n",
      "Training batch loss: 1455.145752 [17000/24965]\n",
      "Training batch loss: 1618.350342 [17100/24965]\n",
      "Training batch loss: 1291.580811 [17200/24965]\n",
      "Training batch loss: 1597.682129 [17300/24965]\n",
      "Training batch loss: 1588.166748 [17400/24965]\n",
      "Training batch loss: 1342.011719 [17500/24965]\n",
      "Training batch loss: 1618.578247 [17600/24965]\n",
      "Training batch loss: 1511.784546 [17700/24965]\n",
      "Training batch loss: 1601.629883 [17800/24965]\n",
      "Training batch loss: 1413.615356 [17900/24965]\n",
      "Training batch loss: 1331.468018 [18000/24965]\n",
      "Training batch loss: 1378.680542 [18100/24965]\n",
      "Training batch loss: 1521.823608 [18200/24965]\n",
      "Training batch loss: 1459.003540 [18300/24965]\n",
      "Training batch loss: 1507.650024 [18400/24965]\n",
      "Training batch loss: 1163.055664 [18500/24965]\n",
      "Training batch loss: 1632.773926 [18600/24965]\n",
      "Training batch loss: 1653.411621 [18700/24965]\n",
      "Training batch loss: 1673.335938 [18800/24965]\n",
      "Training batch loss: 1500.787842 [18900/24965]\n",
      "Training batch loss: 1491.201660 [19000/24965]\n",
      "Training batch loss: 1591.940796 [19100/24965]\n",
      "Training batch loss: 1588.766479 [19200/24965]\n",
      "Training batch loss: 1529.764771 [19300/24965]\n",
      "Training batch loss: 1642.246582 [19400/24965]\n",
      "Training batch loss: 1551.969971 [19500/24965]\n",
      "Training batch loss: 1535.323975 [19600/24965]\n",
      "Training batch loss: 1216.406738 [19700/24965]\n",
      "Training batch loss: 1513.561890 [19800/24965]\n",
      "Training batch loss: 1542.792236 [19900/24965]\n",
      "Training batch loss: 1550.587402 [20000/24965]\n",
      "Training batch loss: 1470.989746 [20100/24965]\n",
      "Training batch loss: 1493.513672 [20200/24965]\n",
      "Training batch loss: 1629.415894 [20300/24965]\n",
      "Training batch loss: 1482.230957 [20400/24965]\n",
      "Training batch loss: 1643.384521 [20500/24965]\n",
      "Training batch loss: 1543.709839 [20600/24965]\n",
      "Training batch loss: 1567.623047 [20700/24965]\n",
      "Training batch loss: 1452.052979 [20800/24965]\n",
      "Training batch loss: 1370.144775 [20900/24965]\n",
      "Training batch loss: 1320.594482 [21000/24965]\n",
      "Training batch loss: 1637.265137 [21100/24965]\n",
      "Training batch loss: 1326.479126 [21200/24965]\n",
      "Training batch loss: 1609.564819 [21300/24965]\n",
      "Training batch loss: 1730.461792 [21400/24965]\n",
      "Training batch loss: 1778.859741 [21500/24965]\n",
      "Training batch loss: 1602.398071 [21600/24965]\n",
      "Training batch loss: 1574.698975 [21700/24965]\n",
      "Training batch loss: 1875.892212 [21800/24965]\n",
      "Training batch loss: 1594.616333 [21900/24965]\n",
      "Training batch loss: 1470.771851 [22000/24965]\n",
      "Training batch loss: 1429.828979 [22100/24965]\n",
      "Training batch loss: 1382.592041 [22200/24965]\n",
      "Training batch loss: 1780.621704 [22300/24965]\n",
      "Training batch loss: 1696.994751 [22400/24965]\n",
      "Training batch loss: 1605.454712 [22500/24965]\n",
      "Training batch loss: 1362.336670 [22600/24965]\n",
      "Training batch loss: 1536.404419 [22700/24965]\n",
      "Training batch loss: 1859.234985 [22800/24965]\n",
      "Training batch loss: 1705.974854 [22900/24965]\n",
      "Training batch loss: 1586.387817 [23000/24965]\n",
      "Training batch loss: 1487.761841 [23100/24965]\n",
      "Training batch loss: 1428.216675 [23200/24965]\n",
      "Training batch loss: 1442.154419 [23300/24965]\n",
      "Training batch loss: 1442.020874 [23400/24965]\n",
      "Training batch loss: 1363.450684 [23500/24965]\n",
      "Training batch loss: 1423.495605 [23600/24965]\n",
      "Training batch loss: 1338.650513 [23700/24965]\n",
      "Training batch loss: 1459.648682 [23800/24965]\n",
      "Training batch loss: 1608.240723 [23900/24965]\n",
      "Training batch loss: 1404.669189 [24000/24965]\n",
      "Training batch loss: 1455.873657 [24100/24965]\n",
      "Training batch loss: 1624.612061 [24200/24965]\n",
      "Training batch loss: 1415.149292 [24300/24965]\n",
      "Training batch loss: 1395.963135 [24400/24965]\n",
      "Training batch loss: 1798.034180 [24500/24965]\n",
      "Training batch loss: 1434.451294 [24600/24965]\n",
      "Training batch loss: 1648.148560 [24700/24965]\n",
      "Training batch loss: 1351.894043 [24800/24965]\n",
      "Training batch loss: 1523.586670 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.675759\n",
      "\n",
      "Saved best new model with val_loss: 1512.6758\n",
      "Epoch 10\n",
      "----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1747.395752 [    0/24965]\n",
      "Training batch loss: 1566.282104 [  100/24965]\n",
      "Training batch loss: 1419.647705 [  200/24965]\n",
      "Training batch loss: 1408.679321 [  300/24965]\n",
      "Training batch loss: 1505.484375 [  400/24965]\n",
      "Training batch loss: 1471.615845 [  500/24965]\n",
      "Training batch loss: 1387.279297 [  600/24965]\n",
      "Training batch loss: 1355.944336 [  700/24965]\n",
      "Training batch loss: 1566.263672 [  800/24965]\n",
      "Training batch loss: 1600.377075 [  900/24965]\n",
      "Training batch loss: 1428.605469 [ 1000/24965]\n",
      "Training batch loss: 1570.876831 [ 1100/24965]\n",
      "Training batch loss: 1539.791992 [ 1200/24965]\n",
      "Training batch loss: 1730.848877 [ 1300/24965]\n",
      "Training batch loss: 1688.593628 [ 1400/24965]\n",
      "Training batch loss: 1512.961426 [ 1500/24965]\n",
      "Training batch loss: 1881.628662 [ 1600/24965]\n",
      "Training batch loss: 1528.375610 [ 1700/24965]\n",
      "Training batch loss: 1443.827148 [ 1800/24965]\n",
      "Training batch loss: 1638.624756 [ 1900/24965]\n",
      "Training batch loss: 1881.926758 [ 2000/24965]\n",
      "Training batch loss: 1676.282715 [ 2100/24965]\n",
      "Training batch loss: 1552.271484 [ 2200/24965]\n",
      "Training batch loss: 1724.035522 [ 2300/24965]\n",
      "Training batch loss: 1442.324097 [ 2400/24965]\n",
      "Training batch loss: 1504.258057 [ 2500/24965]\n",
      "Training batch loss: 1432.602539 [ 2600/24965]\n",
      "Training batch loss: 1449.811523 [ 2700/24965]\n",
      "Training batch loss: 1770.091553 [ 2800/24965]\n",
      "Training batch loss: 1692.154175 [ 2900/24965]\n",
      "Training batch loss: 1413.560791 [ 3000/24965]\n",
      "Training batch loss: 1613.172485 [ 3100/24965]\n",
      "Training batch loss: 1591.643555 [ 3200/24965]\n",
      "Training batch loss: 1599.554932 [ 3300/24965]\n",
      "Training batch loss: 1591.730469 [ 3400/24965]\n",
      "Training batch loss: 1394.126709 [ 3500/24965]\n",
      "Training batch loss: 1577.108398 [ 3600/24965]\n",
      "Training batch loss: 1742.482788 [ 3700/24965]\n",
      "Training batch loss: 1656.604004 [ 3800/24965]\n",
      "Training batch loss: 1577.556152 [ 3900/24965]\n",
      "Training batch loss: 1439.083740 [ 4000/24965]\n",
      "Training batch loss: 1494.341431 [ 4100/24965]\n",
      "Training batch loss: 1489.655396 [ 4200/24965]\n",
      "Training batch loss: 1516.072754 [ 4300/24965]\n",
      "Training batch loss: 1498.653076 [ 4400/24965]\n",
      "Training batch loss: 1207.574585 [ 4500/24965]\n",
      "Training batch loss: 1604.915894 [ 4600/24965]\n",
      "Training batch loss: 1616.635742 [ 4700/24965]\n",
      "Training batch loss: 1618.035034 [ 4800/24965]\n",
      "Training batch loss: 1566.403564 [ 4900/24965]\n",
      "Training batch loss: 1491.579590 [ 5000/24965]\n",
      "Training batch loss: 1494.622192 [ 5100/24965]\n",
      "Training batch loss: 1623.662109 [ 5200/24965]\n",
      "Training batch loss: 1537.618896 [ 5300/24965]\n",
      "Training batch loss: 1326.682007 [ 5400/24965]\n",
      "Training batch loss: 1644.090942 [ 5500/24965]\n",
      "Training batch loss: 1718.646240 [ 5600/24965]\n",
      "Training batch loss: 1650.373535 [ 5700/24965]\n",
      "Training batch loss: 1726.913574 [ 5800/24965]\n",
      "Training batch loss: 1539.557983 [ 5900/24965]\n",
      "Training batch loss: 1624.553833 [ 6000/24965]\n",
      "Training batch loss: 1702.277588 [ 6100/24965]\n",
      "Training batch loss: 1413.493530 [ 6200/24965]\n",
      "Training batch loss: 1711.740967 [ 6300/24965]\n",
      "Training batch loss: 1398.067383 [ 6400/24965]\n",
      "Training batch loss: 1522.617432 [ 6500/24965]\n",
      "Training batch loss: 1652.713867 [ 6600/24965]\n",
      "Training batch loss: 1712.822144 [ 6700/24965]\n",
      "Training batch loss: 1563.695557 [ 6800/24965]\n",
      "Training batch loss: 1491.338135 [ 6900/24965]\n",
      "Training batch loss: 1493.156006 [ 7000/24965]\n",
      "Training batch loss: 1712.367920 [ 7100/24965]\n",
      "Training batch loss: 1573.427002 [ 7200/24965]\n",
      "Training batch loss: 1436.009644 [ 7300/24965]\n",
      "Training batch loss: 1805.541992 [ 7400/24965]\n",
      "Training batch loss: 1595.776733 [ 7500/24965]\n",
      "Training batch loss: 1480.247925 [ 7600/24965]\n",
      "Training batch loss: 1396.540894 [ 7700/24965]\n",
      "Training batch loss: 1590.920410 [ 7800/24965]\n",
      "Training batch loss: 1631.498291 [ 7900/24965]\n",
      "Training batch loss: 1576.640503 [ 8000/24965]\n",
      "Training batch loss: 1678.989258 [ 8100/24965]\n",
      "Training batch loss: 1531.435303 [ 8200/24965]\n",
      "Training batch loss: 1587.367920 [ 8300/24965]\n",
      "Training batch loss: 1552.934692 [ 8400/24965]\n",
      "Training batch loss: 1560.596436 [ 8500/24965]\n",
      "Training batch loss: 1658.359253 [ 8600/24965]\n",
      "Training batch loss: 1394.491699 [ 8700/24965]\n",
      "Training batch loss: 1632.111694 [ 8800/24965]\n",
      "Training batch loss: 1354.881592 [ 8900/24965]\n",
      "Training batch loss: 1678.979004 [ 9000/24965]\n",
      "Training batch loss: 1741.246948 [ 9100/24965]\n",
      "Training batch loss: 1776.698608 [ 9200/24965]\n",
      "Training batch loss: 1248.940918 [ 9300/24965]\n",
      "Training batch loss: 1415.234253 [ 9400/24965]\n",
      "Training batch loss: 1325.108276 [ 9500/24965]\n",
      "Training batch loss: 1672.300293 [ 9600/24965]\n",
      "Training batch loss: 1436.684937 [ 9700/24965]\n",
      "Training batch loss: 1587.998169 [ 9800/24965]\n",
      "Training batch loss: 1901.016846 [ 9900/24965]\n",
      "Training batch loss: 1394.179810 [10000/24965]\n",
      "Training batch loss: 1554.792969 [10100/24965]\n",
      "Training batch loss: 1485.042603 [10200/24965]\n",
      "Training batch loss: 1535.673340 [10300/24965]\n",
      "Training batch loss: 1504.654785 [10400/24965]\n",
      "Training batch loss: 1454.173462 [10500/24965]\n",
      "Training batch loss: 1462.414673 [10600/24965]\n",
      "Training batch loss: 1583.964966 [10700/24965]\n",
      "Training batch loss: 1248.424683 [10800/24965]\n",
      "Training batch loss: 1661.927002 [10900/24965]\n",
      "Training batch loss: 1424.946045 [11000/24965]\n",
      "Training batch loss: 1435.494629 [11100/24965]\n",
      "Training batch loss: 1509.561646 [11200/24965]\n",
      "Training batch loss: 1506.103882 [11300/24965]\n",
      "Training batch loss: 1613.472656 [11400/24965]\n",
      "Training batch loss: 1696.921875 [11500/24965]\n",
      "Training batch loss: 1734.886108 [11600/24965]\n",
      "Training batch loss: 1513.067871 [11700/24965]\n",
      "Training batch loss: 1595.008423 [11800/24965]\n",
      "Training batch loss: 1634.169189 [11900/24965]\n",
      "Training batch loss: 1565.778687 [12000/24965]\n",
      "Training batch loss: 1670.075684 [12100/24965]\n",
      "Training batch loss: 1434.816406 [12200/24965]\n",
      "Training batch loss: 1453.908447 [12300/24965]\n",
      "Training batch loss: 1650.317383 [12400/24965]\n",
      "Training batch loss: 1261.975464 [12500/24965]\n",
      "Training batch loss: 1069.444580 [12600/24965]\n",
      "Training batch loss: 1474.741333 [12700/24965]\n",
      "Training batch loss: 1396.526123 [12800/24965]\n",
      "Training batch loss: 1409.762695 [12900/24965]\n",
      "Training batch loss: 1575.647583 [13000/24965]\n",
      "Training batch loss: 1434.490234 [13100/24965]\n",
      "Training batch loss: 1469.343262 [13200/24965]\n",
      "Training batch loss: 1632.455933 [13300/24965]\n",
      "Training batch loss: 1386.858887 [13400/24965]\n",
      "Training batch loss: 1495.344971 [13500/24965]\n",
      "Training batch loss: 1616.233643 [13600/24965]\n",
      "Training batch loss: 1693.057983 [13700/24965]\n",
      "Training batch loss: 1352.471924 [13800/24965]\n",
      "Training batch loss: 1505.036499 [13900/24965]\n",
      "Training batch loss: 1619.248291 [14000/24965]\n",
      "Training batch loss: 1575.942627 [14100/24965]\n",
      "Training batch loss: 1538.170532 [14200/24965]\n",
      "Training batch loss: 1569.635742 [14300/24965]\n",
      "Training batch loss: 1576.522705 [14400/24965]\n",
      "Training batch loss: 1658.981812 [14500/24965]\n",
      "Training batch loss: 1545.960449 [14600/24965]\n",
      "Training batch loss: 1471.137451 [14700/24965]\n",
      "Training batch loss: 1594.521362 [14800/24965]\n",
      "Training batch loss: 1483.747803 [14900/24965]\n",
      "Training batch loss: 1632.097046 [15000/24965]\n",
      "Training batch loss: 1516.566162 [15100/24965]\n",
      "Training batch loss: 1530.638672 [15200/24965]\n",
      "Training batch loss: 1487.063599 [15300/24965]\n",
      "Training batch loss: 1768.697876 [15400/24965]\n",
      "Training batch loss: 1679.049927 [15500/24965]\n",
      "Training batch loss: 1580.106567 [15600/24965]\n",
      "Training batch loss: 1384.890869 [15700/24965]\n",
      "Training batch loss: 1465.238525 [15800/24965]\n",
      "Training batch loss: 1408.859985 [15900/24965]\n",
      "Training batch loss: 1386.372314 [16000/24965]\n",
      "Training batch loss: 1620.477661 [16100/24965]\n",
      "Training batch loss: 1582.511353 [16200/24965]\n",
      "Training batch loss: 1438.840088 [16300/24965]\n",
      "Training batch loss: 1414.272217 [16400/24965]\n",
      "Training batch loss: 1742.993408 [16500/24965]\n",
      "Training batch loss: 1472.836182 [16600/24965]\n",
      "Training batch loss: 1547.653076 [16700/24965]\n",
      "Training batch loss: 1580.740723 [16800/24965]\n",
      "Training batch loss: 1511.392212 [16900/24965]\n",
      "Training batch loss: 1656.393677 [17000/24965]\n",
      "Training batch loss: 1660.195923 [17100/24965]\n",
      "Training batch loss: 1435.679565 [17200/24965]\n",
      "Training batch loss: 1507.019165 [17300/24965]\n",
      "Training batch loss: 1426.577759 [17400/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1495.550537 [17500/24965]\n",
      "Training batch loss: 1581.665039 [17600/24965]\n",
      "Training batch loss: 1658.697632 [17700/24965]\n",
      "Training batch loss: 1277.811401 [17800/24965]\n",
      "Training batch loss: 1372.690186 [17900/24965]\n",
      "Training batch loss: 1686.733765 [18000/24965]\n",
      "Training batch loss: 1662.855835 [18100/24965]\n",
      "Training batch loss: 1768.999756 [18200/24965]\n",
      "Training batch loss: 1593.817993 [18300/24965]\n",
      "Training batch loss: 1736.391357 [18400/24965]\n",
      "Training batch loss: 1469.734497 [18500/24965]\n",
      "Training batch loss: 1392.452026 [18600/24965]\n",
      "Training batch loss: 1498.351685 [18700/24965]\n",
      "Training batch loss: 1563.713501 [18800/24965]\n",
      "Training batch loss: 1497.589355 [18900/24965]\n",
      "Training batch loss: 1451.369751 [19000/24965]\n",
      "Training batch loss: 1405.109863 [19100/24965]\n",
      "Training batch loss: 1575.748413 [19200/24965]\n",
      "Training batch loss: 1724.825439 [19300/24965]\n",
      "Training batch loss: 1802.944702 [19400/24965]\n",
      "Training batch loss: 1290.926025 [19500/24965]\n",
      "Training batch loss: 1643.398560 [19600/24965]\n",
      "Training batch loss: 1906.903076 [19700/24965]\n",
      "Training batch loss: 1556.117798 [19800/24965]\n",
      "Training batch loss: 1621.644897 [19900/24965]\n",
      "Training batch loss: 1568.649048 [20000/24965]\n",
      "Training batch loss: 1605.528564 [20100/24965]\n",
      "Training batch loss: 1435.989990 [20200/24965]\n",
      "Training batch loss: 1500.270020 [20300/24965]\n",
      "Training batch loss: 1388.119507 [20400/24965]\n",
      "Training batch loss: 1560.399414 [20500/24965]\n",
      "Training batch loss: 1662.439453 [20600/24965]\n",
      "Training batch loss: 1644.684326 [20700/24965]\n",
      "Training batch loss: 1458.062012 [20800/24965]\n",
      "Training batch loss: 1397.866943 [20900/24965]\n",
      "Training batch loss: 1568.805786 [21000/24965]\n",
      "Training batch loss: 1449.139404 [21100/24965]\n",
      "Training batch loss: 1900.394287 [21200/24965]\n",
      "Training batch loss: 1567.868164 [21300/24965]\n",
      "Training batch loss: 1631.030762 [21400/24965]\n",
      "Training batch loss: 1728.055542 [21500/24965]\n",
      "Training batch loss: 1515.182007 [21600/24965]\n",
      "Training batch loss: 1595.634521 [21700/24965]\n",
      "Training batch loss: 1741.490845 [21800/24965]\n",
      "Training batch loss: 1463.071411 [21900/24965]\n",
      "Training batch loss: 1440.801025 [22000/24965]\n",
      "Training batch loss: 1419.185547 [22100/24965]\n",
      "Training batch loss: 1457.243286 [22200/24965]\n",
      "Training batch loss: 1650.937988 [22300/24965]\n",
      "Training batch loss: 1364.024902 [22400/24965]\n",
      "Training batch loss: 1406.473267 [22500/24965]\n",
      "Training batch loss: 1429.871582 [22600/24965]\n",
      "Training batch loss: 1480.209961 [22700/24965]\n",
      "Training batch loss: 1459.544556 [22800/24965]\n",
      "Training batch loss: 1623.753540 [22900/24965]\n",
      "Training batch loss: 1566.448242 [23000/24965]\n",
      "Training batch loss: 1439.495605 [23100/24965]\n",
      "Training batch loss: 1610.692871 [23200/24965]\n",
      "Training batch loss: 1343.853760 [23300/24965]\n",
      "Training batch loss: 1350.102783 [23400/24965]\n",
      "Training batch loss: 1589.496094 [23500/24965]\n",
      "Training batch loss: 1591.560791 [23600/24965]\n",
      "Training batch loss: 1587.605713 [23700/24965]\n",
      "Training batch loss: 1570.922363 [23800/24965]\n",
      "Training batch loss: 1588.581299 [23900/24965]\n",
      "Training batch loss: 1682.753662 [24000/24965]\n",
      "Training batch loss: 1523.569580 [24100/24965]\n",
      "Training batch loss: 1490.526733 [24200/24965]\n",
      "Training batch loss: 1569.482422 [24300/24965]\n",
      "Training batch loss: 1591.723267 [24400/24965]\n",
      "Training batch loss: 1480.800659 [24500/24965]\n",
      "Training batch loss: 1487.099121 [24600/24965]\n",
      "Training batch loss: 1652.580444 [24700/24965]\n",
      "Training batch loss: 1441.132690 [24800/24965]\n",
      "Training batch loss: 1681.806396 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.709688\n",
      "\n",
      "Epoch 11\n",
      "----------------------\n",
      "Training batch loss: 1605.804443 [    0/24965]\n",
      "Training batch loss: 1627.088257 [  100/24965]\n",
      "Training batch loss: 1472.871948 [  200/24965]\n",
      "Training batch loss: 1731.288574 [  300/24965]\n",
      "Training batch loss: 1526.732422 [  400/24965]\n",
      "Training batch loss: 1294.923096 [  500/24965]\n",
      "Training batch loss: 1669.122559 [  600/24965]\n",
      "Training batch loss: 1431.533936 [  700/24965]\n",
      "Training batch loss: 1769.949219 [  800/24965]\n",
      "Training batch loss: 1721.182617 [  900/24965]\n",
      "Training batch loss: 1512.674927 [ 1000/24965]\n",
      "Training batch loss: 1206.536621 [ 1100/24965]\n",
      "Training batch loss: 1636.563354 [ 1200/24965]\n",
      "Training batch loss: 1551.802979 [ 1300/24965]\n",
      "Training batch loss: 1782.347168 [ 1400/24965]\n",
      "Training batch loss: 1409.557129 [ 1500/24965]\n",
      "Training batch loss: 1453.230957 [ 1600/24965]\n",
      "Training batch loss: 1787.910278 [ 1700/24965]\n",
      "Training batch loss: 1428.037354 [ 1800/24965]\n",
      "Training batch loss: 1420.817993 [ 1900/24965]\n",
      "Training batch loss: 1545.152466 [ 2000/24965]\n",
      "Training batch loss: 1651.928345 [ 2100/24965]\n",
      "Training batch loss: 1612.097290 [ 2200/24965]\n",
      "Training batch loss: 1597.413574 [ 2300/24965]\n",
      "Training batch loss: 1432.792603 [ 2400/24965]\n",
      "Training batch loss: 1523.776489 [ 2500/24965]\n",
      "Training batch loss: 1592.551758 [ 2600/24965]\n",
      "Training batch loss: 1443.521606 [ 2700/24965]\n",
      "Training batch loss: 1538.315552 [ 2800/24965]\n",
      "Training batch loss: 1431.471924 [ 2900/24965]\n",
      "Training batch loss: 1609.308838 [ 3000/24965]\n",
      "Training batch loss: 1369.041992 [ 3100/24965]\n",
      "Training batch loss: 1311.027344 [ 3200/24965]\n",
      "Training batch loss: 1404.514526 [ 3300/24965]\n",
      "Training batch loss: 1564.621826 [ 3400/24965]\n",
      "Training batch loss: 1740.761230 [ 3500/24965]\n",
      "Training batch loss: 1745.408813 [ 3600/24965]\n",
      "Training batch loss: 1448.063477 [ 3700/24965]\n",
      "Training batch loss: 1356.986450 [ 3800/24965]\n",
      "Training batch loss: 1587.134766 [ 3900/24965]\n",
      "Training batch loss: 1434.390259 [ 4000/24965]\n",
      "Training batch loss: 1217.457764 [ 4100/24965]\n",
      "Training batch loss: 1640.373535 [ 4200/24965]\n",
      "Training batch loss: 1573.292725 [ 4300/24965]\n",
      "Training batch loss: 1520.273315 [ 4400/24965]\n",
      "Training batch loss: 1453.918823 [ 4500/24965]\n",
      "Training batch loss: 1572.180664 [ 4600/24965]\n",
      "Training batch loss: 1515.463379 [ 4700/24965]\n",
      "Training batch loss: 1538.236084 [ 4800/24965]\n",
      "Training batch loss: 1462.216187 [ 4900/24965]\n",
      "Training batch loss: 1385.126465 [ 5000/24965]\n",
      "Training batch loss: 1368.678711 [ 5100/24965]\n",
      "Training batch loss: 1566.808350 [ 5200/24965]\n",
      "Training batch loss: 1404.576172 [ 5300/24965]\n",
      "Training batch loss: 1512.345093 [ 5400/24965]\n",
      "Training batch loss: 1409.981689 [ 5500/24965]\n",
      "Training batch loss: 1440.113159 [ 5600/24965]\n",
      "Training batch loss: 1470.574707 [ 5700/24965]\n",
      "Training batch loss: 1505.556885 [ 5800/24965]\n",
      "Training batch loss: 1626.595215 [ 5900/24965]\n",
      "Training batch loss: 1479.939819 [ 6000/24965]\n",
      "Training batch loss: 1687.293335 [ 6100/24965]\n",
      "Training batch loss: 1548.477173 [ 6200/24965]\n",
      "Training batch loss: 1389.970459 [ 6300/24965]\n",
      "Training batch loss: 1514.578979 [ 6400/24965]\n",
      "Training batch loss: 1519.331909 [ 6500/24965]\n",
      "Training batch loss: 1538.420776 [ 6600/24965]\n",
      "Training batch loss: 1616.568115 [ 6700/24965]\n",
      "Training batch loss: 1588.394897 [ 6800/24965]\n",
      "Training batch loss: 1745.587402 [ 6900/24965]\n",
      "Training batch loss: 1364.499023 [ 7000/24965]\n",
      "Training batch loss: 1670.893555 [ 7100/24965]\n",
      "Training batch loss: 1520.807373 [ 7200/24965]\n",
      "Training batch loss: 1680.100586 [ 7300/24965]\n",
      "Training batch loss: 1439.747681 [ 7400/24965]\n",
      "Training batch loss: 1533.399536 [ 7500/24965]\n",
      "Training batch loss: 1629.045898 [ 7600/24965]\n",
      "Training batch loss: 1408.723999 [ 7700/24965]\n",
      "Training batch loss: 1609.598999 [ 7800/24965]\n",
      "Training batch loss: 1450.197876 [ 7900/24965]\n",
      "Training batch loss: 1705.233643 [ 8000/24965]\n",
      "Training batch loss: 1520.769531 [ 8100/24965]\n",
      "Training batch loss: 1549.728516 [ 8200/24965]\n",
      "Training batch loss: 1382.335571 [ 8300/24965]\n",
      "Training batch loss: 1679.743286 [ 8400/24965]\n",
      "Training batch loss: 1497.466431 [ 8500/24965]\n",
      "Training batch loss: 1479.586426 [ 8600/24965]\n",
      "Training batch loss: 1427.352661 [ 8700/24965]\n",
      "Training batch loss: 1566.198486 [ 8800/24965]\n",
      "Training batch loss: 1569.633789 [ 8900/24965]\n",
      "Training batch loss: 1574.573486 [ 9000/24965]\n",
      "Training batch loss: 1358.616943 [ 9100/24965]\n",
      "Training batch loss: 1605.811401 [ 9200/24965]\n",
      "Training batch loss: 1644.810547 [ 9300/24965]\n",
      "Training batch loss: 1392.648438 [ 9400/24965]\n",
      "Training batch loss: 1377.366699 [ 9500/24965]\n",
      "Training batch loss: 1544.069946 [ 9600/24965]\n",
      "Training batch loss: 1460.084595 [ 9700/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1540.551025 [ 9800/24965]\n",
      "Training batch loss: 1481.988403 [ 9900/24965]\n",
      "Training batch loss: 1582.145020 [10000/24965]\n",
      "Training batch loss: 1634.385986 [10100/24965]\n",
      "Training batch loss: 1527.229492 [10200/24965]\n",
      "Training batch loss: 1254.158203 [10300/24965]\n",
      "Training batch loss: 1641.947021 [10400/24965]\n",
      "Training batch loss: 1415.331299 [10500/24965]\n",
      "Training batch loss: 1479.791992 [10600/24965]\n",
      "Training batch loss: 1622.416504 [10700/24965]\n",
      "Training batch loss: 1534.786011 [10800/24965]\n",
      "Training batch loss: 1556.313965 [10900/24965]\n",
      "Training batch loss: 1326.552979 [11000/24965]\n",
      "Training batch loss: 1639.604004 [11100/24965]\n",
      "Training batch loss: 1501.131470 [11200/24965]\n",
      "Training batch loss: 1394.296265 [11300/24965]\n",
      "Training batch loss: 1587.944458 [11400/24965]\n",
      "Training batch loss: 1512.000732 [11500/24965]\n",
      "Training batch loss: 1548.135010 [11600/24965]\n",
      "Training batch loss: 1463.538818 [11700/24965]\n",
      "Training batch loss: 1447.692627 [11800/24965]\n",
      "Training batch loss: 1365.453125 [11900/24965]\n",
      "Training batch loss: 1319.555542 [12000/24965]\n",
      "Training batch loss: 1506.175659 [12100/24965]\n",
      "Training batch loss: 1504.616577 [12200/24965]\n",
      "Training batch loss: 1342.101318 [12300/24965]\n",
      "Training batch loss: 1682.545532 [12400/24965]\n",
      "Training batch loss: 1561.531494 [12500/24965]\n",
      "Training batch loss: 1574.771240 [12600/24965]\n",
      "Training batch loss: 1246.921875 [12700/24965]\n",
      "Training batch loss: 1583.005981 [12800/24965]\n",
      "Training batch loss: 1505.035767 [12900/24965]\n",
      "Training batch loss: 1900.064697 [13000/24965]\n",
      "Training batch loss: 1561.964600 [13100/24965]\n",
      "Training batch loss: 1672.389160 [13200/24965]\n",
      "Training batch loss: 1555.362671 [13300/24965]\n",
      "Training batch loss: 1512.993164 [13400/24965]\n",
      "Training batch loss: 1397.320435 [13500/24965]\n",
      "Training batch loss: 1664.022095 [13600/24965]\n",
      "Training batch loss: 1450.661255 [13700/24965]\n",
      "Training batch loss: 1457.497070 [13800/24965]\n",
      "Training batch loss: 1409.943359 [13900/24965]\n",
      "Training batch loss: 1575.212646 [14000/24965]\n",
      "Training batch loss: 1396.454224 [14100/24965]\n",
      "Training batch loss: 1527.564453 [14200/24965]\n",
      "Training batch loss: 1552.478882 [14300/24965]\n",
      "Training batch loss: 1460.537354 [14400/24965]\n",
      "Training batch loss: 1461.145508 [14500/24965]\n",
      "Training batch loss: 1546.387939 [14600/24965]\n",
      "Training batch loss: 1434.868164 [14700/24965]\n",
      "Training batch loss: 1738.837036 [14800/24965]\n",
      "Training batch loss: 1450.113892 [14900/24965]\n",
      "Training batch loss: 1488.075806 [15000/24965]\n",
      "Training batch loss: 1741.978516 [15100/24965]\n",
      "Training batch loss: 1440.078369 [15200/24965]\n",
      "Training batch loss: 1587.367554 [15300/24965]\n",
      "Training batch loss: 1513.711060 [15400/24965]\n",
      "Training batch loss: 1461.805542 [15500/24965]\n",
      "Training batch loss: 1400.695923 [15600/24965]\n",
      "Training batch loss: 1602.490601 [15700/24965]\n",
      "Training batch loss: 1573.413452 [15800/24965]\n",
      "Training batch loss: 1234.745972 [15900/24965]\n",
      "Training batch loss: 1505.967529 [16000/24965]\n",
      "Training batch loss: 1562.882202 [16100/24965]\n",
      "Training batch loss: 1478.925171 [16200/24965]\n",
      "Training batch loss: 1456.008911 [16300/24965]\n",
      "Training batch loss: 1493.714844 [16400/24965]\n",
      "Training batch loss: 1609.613647 [16500/24965]\n",
      "Training batch loss: 1691.385742 [16600/24965]\n",
      "Training batch loss: 1522.160522 [16700/24965]\n",
      "Training batch loss: 1626.333130 [16800/24965]\n",
      "Training batch loss: 1502.504639 [16900/24965]\n",
      "Training batch loss: 1513.163818 [17000/24965]\n",
      "Training batch loss: 1627.659546 [17100/24965]\n",
      "Training batch loss: 1256.896484 [17200/24965]\n",
      "Training batch loss: 1654.003052 [17300/24965]\n",
      "Training batch loss: 1541.162598 [17400/24965]\n",
      "Training batch loss: 1676.552368 [17500/24965]\n",
      "Training batch loss: 1519.652100 [17600/24965]\n",
      "Training batch loss: 1533.130127 [17700/24965]\n",
      "Training batch loss: 1500.270752 [17800/24965]\n",
      "Training batch loss: 1514.331421 [17900/24965]\n",
      "Training batch loss: 1616.314209 [18000/24965]\n",
      "Training batch loss: 1596.478271 [18100/24965]\n",
      "Training batch loss: 1463.613770 [18200/24965]\n",
      "Training batch loss: 1406.964600 [18300/24965]\n",
      "Training batch loss: 1345.814575 [18400/24965]\n",
      "Training batch loss: 1450.109375 [18500/24965]\n",
      "Training batch loss: 1550.569214 [18600/24965]\n",
      "Training batch loss: 1660.110474 [18700/24965]\n",
      "Training batch loss: 1678.220337 [18800/24965]\n",
      "Training batch loss: 1661.880859 [18900/24965]\n",
      "Training batch loss: 1509.118408 [19000/24965]\n",
      "Training batch loss: 1499.086670 [19100/24965]\n",
      "Training batch loss: 1416.652100 [19200/24965]\n",
      "Training batch loss: 1489.607422 [19300/24965]\n",
      "Training batch loss: 1597.780762 [19400/24965]\n",
      "Training batch loss: 1582.085571 [19500/24965]\n",
      "Training batch loss: 1588.972412 [19600/24965]\n",
      "Training batch loss: 1416.892090 [19700/24965]\n",
      "Training batch loss: 1460.708252 [19800/24965]\n",
      "Training batch loss: 1311.016357 [19900/24965]\n",
      "Training batch loss: 1524.517456 [20000/24965]\n",
      "Training batch loss: 1658.710693 [20100/24965]\n",
      "Training batch loss: 1599.719849 [20200/24965]\n",
      "Training batch loss: 1359.137695 [20300/24965]\n",
      "Training batch loss: 1845.767334 [20400/24965]\n",
      "Training batch loss: 1578.523438 [20500/24965]\n",
      "Training batch loss: 1682.901978 [20600/24965]\n",
      "Training batch loss: 1544.829224 [20700/24965]\n",
      "Training batch loss: 1715.544556 [20800/24965]\n",
      "Training batch loss: 1456.949463 [20900/24965]\n",
      "Training batch loss: 1508.234741 [21000/24965]\n",
      "Training batch loss: 1462.651489 [21100/24965]\n",
      "Training batch loss: 1520.013306 [21200/24965]\n",
      "Training batch loss: 1548.192993 [21300/24965]\n",
      "Training batch loss: 1637.316406 [21400/24965]\n",
      "Training batch loss: 1516.500732 [21500/24965]\n",
      "Training batch loss: 1572.595947 [21600/24965]\n",
      "Training batch loss: 1835.259521 [21700/24965]\n",
      "Training batch loss: 1616.216064 [21800/24965]\n",
      "Training batch loss: 1719.795898 [21900/24965]\n",
      "Training batch loss: 1462.901367 [22000/24965]\n",
      "Training batch loss: 1469.005859 [22100/24965]\n",
      "Training batch loss: 1271.975342 [22200/24965]\n",
      "Training batch loss: 1357.383789 [22300/24965]\n",
      "Training batch loss: 1571.087769 [22400/24965]\n",
      "Training batch loss: 1680.087646 [22500/24965]\n",
      "Training batch loss: 1526.943970 [22600/24965]\n",
      "Training batch loss: 1530.829224 [22700/24965]\n",
      "Training batch loss: 1559.847168 [22800/24965]\n",
      "Training batch loss: 1687.041992 [22900/24965]\n",
      "Training batch loss: 1708.306763 [23000/24965]\n",
      "Training batch loss: 1412.167969 [23100/24965]\n",
      "Training batch loss: 1632.193481 [23200/24965]\n",
      "Training batch loss: 1338.027100 [23300/24965]\n",
      "Training batch loss: 1487.154419 [23400/24965]\n",
      "Training batch loss: 1591.326904 [23500/24965]\n",
      "Training batch loss: 1374.901245 [23600/24965]\n",
      "Training batch loss: 1636.051880 [23700/24965]\n",
      "Training batch loss: 1522.621826 [23800/24965]\n",
      "Training batch loss: 1485.205688 [23900/24965]\n",
      "Training batch loss: 1631.181519 [24000/24965]\n",
      "Training batch loss: 1421.952271 [24100/24965]\n",
      "Training batch loss: 1664.304321 [24200/24965]\n",
      "Training batch loss: 1554.551514 [24300/24965]\n",
      "Training batch loss: 1484.491943 [24400/24965]\n",
      "Training batch loss: 1434.541016 [24500/24965]\n",
      "Training batch loss: 1603.037842 [24600/24965]\n",
      "Training batch loss: 1377.150146 [24700/24965]\n",
      "Training batch loss: 1708.089355 [24800/24965]\n",
      "Training batch loss: 1581.723145 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.593353\n",
      "\n",
      "Saved best new model with val_loss: 1512.5934\n",
      "Epoch 12\n",
      "----------------------\n",
      "Training batch loss: 1561.404663 [    0/24965]\n",
      "Training batch loss: 1516.425781 [  100/24965]\n",
      "Training batch loss: 1550.018555 [  200/24965]\n",
      "Training batch loss: 1551.547974 [  300/24965]\n",
      "Training batch loss: 1584.896118 [  400/24965]\n",
      "Training batch loss: 1607.976196 [  500/24965]\n",
      "Training batch loss: 1653.961182 [  600/24965]\n",
      "Training batch loss: 1551.980469 [  700/24965]\n",
      "Training batch loss: 1562.292480 [  800/24965]\n",
      "Training batch loss: 1303.904907 [  900/24965]\n",
      "Training batch loss: 1583.758667 [ 1000/24965]\n",
      "Training batch loss: 1481.121826 [ 1100/24965]\n",
      "Training batch loss: 1631.469727 [ 1200/24965]\n",
      "Training batch loss: 1460.817383 [ 1300/24965]\n",
      "Training batch loss: 1416.662109 [ 1400/24965]\n",
      "Training batch loss: 1537.976807 [ 1500/24965]\n",
      "Training batch loss: 1392.551514 [ 1600/24965]\n",
      "Training batch loss: 1623.237549 [ 1700/24965]\n",
      "Training batch loss: 1609.322388 [ 1800/24965]\n",
      "Training batch loss: 1602.770264 [ 1900/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1468.256104 [ 2000/24965]\n",
      "Training batch loss: 1622.921509 [ 2100/24965]\n",
      "Training batch loss: 1504.946533 [ 2200/24965]\n",
      "Training batch loss: 1450.439209 [ 2300/24965]\n",
      "Training batch loss: 1436.020142 [ 2400/24965]\n",
      "Training batch loss: 1579.130371 [ 2500/24965]\n",
      "Training batch loss: 1609.826660 [ 2600/24965]\n",
      "Training batch loss: 1625.101440 [ 2700/24965]\n",
      "Training batch loss: 1592.593262 [ 2800/24965]\n",
      "Training batch loss: 1635.536621 [ 2900/24965]\n",
      "Training batch loss: 1554.958862 [ 3000/24965]\n",
      "Training batch loss: 1407.502197 [ 3100/24965]\n",
      "Training batch loss: 1533.423340 [ 3200/24965]\n",
      "Training batch loss: 1619.139282 [ 3300/24965]\n",
      "Training batch loss: 1462.885132 [ 3400/24965]\n",
      "Training batch loss: 1208.569336 [ 3500/24965]\n",
      "Training batch loss: 1465.408081 [ 3600/24965]\n",
      "Training batch loss: 1670.674194 [ 3700/24965]\n",
      "Training batch loss: 1473.441406 [ 3800/24965]\n",
      "Training batch loss: 1563.728149 [ 3900/24965]\n",
      "Training batch loss: 1566.364014 [ 4000/24965]\n",
      "Training batch loss: 1765.178711 [ 4100/24965]\n",
      "Training batch loss: 1268.651367 [ 4200/24965]\n",
      "Training batch loss: 1479.607788 [ 4300/24965]\n",
      "Training batch loss: 1552.511963 [ 4400/24965]\n",
      "Training batch loss: 1708.210693 [ 4500/24965]\n",
      "Training batch loss: 1375.841675 [ 4600/24965]\n",
      "Training batch loss: 1364.888306 [ 4700/24965]\n",
      "Training batch loss: 1255.323853 [ 4800/24965]\n",
      "Training batch loss: 1465.545410 [ 4900/24965]\n",
      "Training batch loss: 1534.090820 [ 5000/24965]\n",
      "Training batch loss: 1311.713989 [ 5100/24965]\n",
      "Training batch loss: 1561.806641 [ 5200/24965]\n",
      "Training batch loss: 1547.627686 [ 5300/24965]\n",
      "Training batch loss: 1592.468262 [ 5400/24965]\n",
      "Training batch loss: 1404.850464 [ 5500/24965]\n",
      "Training batch loss: 1516.197876 [ 5600/24965]\n",
      "Training batch loss: 1513.306274 [ 5700/24965]\n",
      "Training batch loss: 1333.718140 [ 5800/24965]\n",
      "Training batch loss: 1612.750366 [ 5900/24965]\n",
      "Training batch loss: 1692.094849 [ 6000/24965]\n",
      "Training batch loss: 1189.210693 [ 6100/24965]\n",
      "Training batch loss: 1512.302979 [ 6200/24965]\n",
      "Training batch loss: 1343.632202 [ 6300/24965]\n",
      "Training batch loss: 1590.545410 [ 6400/24965]\n",
      "Training batch loss: 1486.837402 [ 6500/24965]\n",
      "Training batch loss: 1456.144409 [ 6600/24965]\n",
      "Training batch loss: 1498.932861 [ 6700/24965]\n",
      "Training batch loss: 1542.910767 [ 6800/24965]\n",
      "Training batch loss: 1427.108154 [ 6900/24965]\n",
      "Training batch loss: 1510.170776 [ 7000/24965]\n",
      "Training batch loss: 1717.462036 [ 7100/24965]\n",
      "Training batch loss: 1396.189453 [ 7200/24965]\n",
      "Training batch loss: 1767.823608 [ 7300/24965]\n",
      "Training batch loss: 1465.096069 [ 7400/24965]\n",
      "Training batch loss: 1437.959839 [ 7500/24965]\n",
      "Training batch loss: 1427.119995 [ 7600/24965]\n",
      "Training batch loss: 1430.167358 [ 7700/24965]\n",
      "Training batch loss: 1644.769531 [ 7800/24965]\n",
      "Training batch loss: 1583.205078 [ 7900/24965]\n",
      "Training batch loss: 1632.349731 [ 8000/24965]\n",
      "Training batch loss: 1359.335327 [ 8100/24965]\n",
      "Training batch loss: 1395.014404 [ 8200/24965]\n",
      "Training batch loss: 1322.499023 [ 8300/24965]\n",
      "Training batch loss: 1545.472900 [ 8400/24965]\n",
      "Training batch loss: 1626.648193 [ 8500/24965]\n",
      "Training batch loss: 1470.945435 [ 8600/24965]\n",
      "Training batch loss: 1618.964355 [ 8700/24965]\n",
      "Training batch loss: 1417.212769 [ 8800/24965]\n",
      "Training batch loss: 1667.177368 [ 8900/24965]\n",
      "Training batch loss: 1395.215088 [ 9000/24965]\n",
      "Training batch loss: 1697.088623 [ 9100/24965]\n",
      "Training batch loss: 1501.655273 [ 9200/24965]\n",
      "Training batch loss: 1450.604370 [ 9300/24965]\n",
      "Training batch loss: 1493.985229 [ 9400/24965]\n",
      "Training batch loss: 1735.639038 [ 9500/24965]\n",
      "Training batch loss: 1399.774414 [ 9600/24965]\n",
      "Training batch loss: 1488.529785 [ 9700/24965]\n",
      "Training batch loss: 1464.419312 [ 9800/24965]\n",
      "Training batch loss: 1388.391113 [ 9900/24965]\n",
      "Training batch loss: 1672.846924 [10000/24965]\n",
      "Training batch loss: 1646.389893 [10100/24965]\n",
      "Training batch loss: 1557.672974 [10200/24965]\n",
      "Training batch loss: 1348.561646 [10300/24965]\n",
      "Training batch loss: 1411.272827 [10400/24965]\n",
      "Training batch loss: 1465.853394 [10500/24965]\n",
      "Training batch loss: 1701.656982 [10600/24965]\n",
      "Training batch loss: 1643.757446 [10700/24965]\n",
      "Training batch loss: 1642.011597 [10800/24965]\n",
      "Training batch loss: 1637.911499 [10900/24965]\n",
      "Training batch loss: 1541.432495 [11000/24965]\n",
      "Training batch loss: 1528.315186 [11100/24965]\n",
      "Training batch loss: 1699.905762 [11200/24965]\n",
      "Training batch loss: 1765.663208 [11300/24965]\n",
      "Training batch loss: 1605.799805 [11400/24965]\n",
      "Training batch loss: 1812.703491 [11500/24965]\n",
      "Training batch loss: 1517.313599 [11600/24965]\n",
      "Training batch loss: 1478.607910 [11700/24965]\n",
      "Training batch loss: 1715.881104 [11800/24965]\n",
      "Training batch loss: 1741.224609 [11900/24965]\n",
      "Training batch loss: 1406.323975 [12000/24965]\n",
      "Training batch loss: 1662.751343 [12100/24965]\n",
      "Training batch loss: 1493.460083 [12200/24965]\n",
      "Training batch loss: 1449.455078 [12300/24965]\n",
      "Training batch loss: 1264.598877 [12400/24965]\n",
      "Training batch loss: 1794.474243 [12500/24965]\n",
      "Training batch loss: 1716.269409 [12600/24965]\n",
      "Training batch loss: 1634.455688 [12700/24965]\n",
      "Training batch loss: 1426.149170 [12800/24965]\n",
      "Training batch loss: 1540.023804 [12900/24965]\n",
      "Training batch loss: 1279.631226 [13000/24965]\n",
      "Training batch loss: 1424.790405 [13100/24965]\n",
      "Training batch loss: 1618.302124 [13200/24965]\n",
      "Training batch loss: 1621.850586 [13300/24965]\n",
      "Training batch loss: 1454.610352 [13400/24965]\n",
      "Training batch loss: 1553.504395 [13500/24965]\n",
      "Training batch loss: 1551.918213 [13600/24965]\n",
      "Training batch loss: 1624.810425 [13700/24965]\n",
      "Training batch loss: 1541.364746 [13800/24965]\n",
      "Training batch loss: 1401.255371 [13900/24965]\n",
      "Training batch loss: 1634.729858 [14000/24965]\n",
      "Training batch loss: 1550.917603 [14100/24965]\n",
      "Training batch loss: 1540.706055 [14200/24965]\n",
      "Training batch loss: 1845.956055 [14300/24965]\n",
      "Training batch loss: 1459.056152 [14400/24965]\n",
      "Training batch loss: 1387.067383 [14500/24965]\n",
      "Training batch loss: 1534.421631 [14600/24965]\n",
      "Training batch loss: 1645.090576 [14700/24965]\n",
      "Training batch loss: 1555.479736 [14800/24965]\n",
      "Training batch loss: 1576.296875 [14900/24965]\n",
      "Training batch loss: 1384.847412 [15000/24965]\n",
      "Training batch loss: 1530.624512 [15100/24965]\n",
      "Training batch loss: 1489.506348 [15200/24965]\n",
      "Training batch loss: 1460.406738 [15300/24965]\n",
      "Training batch loss: 1762.620972 [15400/24965]\n",
      "Training batch loss: 1396.000977 [15500/24965]\n",
      "Training batch loss: 1364.270142 [15600/24965]\n",
      "Training batch loss: 1446.664185 [15700/24965]\n",
      "Training batch loss: 1505.201172 [15800/24965]\n",
      "Training batch loss: 1540.230713 [15900/24965]\n",
      "Training batch loss: 1470.979614 [16000/24965]\n",
      "Training batch loss: 1615.089844 [16100/24965]\n",
      "Training batch loss: 1561.601440 [16200/24965]\n",
      "Training batch loss: 1707.536865 [16300/24965]\n",
      "Training batch loss: 1691.147705 [16400/24965]\n",
      "Training batch loss: 1479.442871 [16500/24965]\n",
      "Training batch loss: 1375.293579 [16600/24965]\n",
      "Training batch loss: 1399.200928 [16700/24965]\n",
      "Training batch loss: 1428.353271 [16800/24965]\n",
      "Training batch loss: 1672.852539 [16900/24965]\n",
      "Training batch loss: 1680.143066 [17000/24965]\n",
      "Training batch loss: 1411.428589 [17100/24965]\n",
      "Training batch loss: 1514.289795 [17200/24965]\n",
      "Training batch loss: 1526.395508 [17300/24965]\n",
      "Training batch loss: 1587.884277 [17400/24965]\n",
      "Training batch loss: 1423.031006 [17500/24965]\n",
      "Training batch loss: 1614.366211 [17600/24965]\n",
      "Training batch loss: 1678.195312 [17700/24965]\n",
      "Training batch loss: 1452.894409 [17800/24965]\n",
      "Training batch loss: 1530.058594 [17900/24965]\n",
      "Training batch loss: 1490.171265 [18000/24965]\n",
      "Training batch loss: 1422.330444 [18100/24965]\n",
      "Training batch loss: 1508.462280 [18200/24965]\n",
      "Training batch loss: 1657.846069 [18300/24965]\n",
      "Training batch loss: 1527.032715 [18400/24965]\n",
      "Training batch loss: 1506.088257 [18500/24965]\n",
      "Training batch loss: 1242.310181 [18600/24965]\n",
      "Training batch loss: 1553.602783 [18700/24965]\n",
      "Training batch loss: 1409.186890 [18800/24965]\n",
      "Training batch loss: 1606.009766 [18900/24965]\n",
      "Training batch loss: 1840.893555 [19000/24965]\n",
      "Training batch loss: 1495.729126 [19100/24965]\n",
      "Training batch loss: 1653.693237 [19200/24965]\n",
      "Training batch loss: 1356.577393 [19300/24965]\n",
      "Training batch loss: 1552.050537 [19400/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1703.772827 [19500/24965]\n",
      "Training batch loss: 1615.294922 [19600/24965]\n",
      "Training batch loss: 1331.990967 [19700/24965]\n",
      "Training batch loss: 1380.929932 [19800/24965]\n",
      "Training batch loss: 1524.980347 [19900/24965]\n",
      "Training batch loss: 1632.619995 [20000/24965]\n",
      "Training batch loss: 1395.432861 [20100/24965]\n",
      "Training batch loss: 1560.114014 [20200/24965]\n",
      "Training batch loss: 1493.000000 [20300/24965]\n",
      "Training batch loss: 1575.111450 [20400/24965]\n",
      "Training batch loss: 1503.574951 [20500/24965]\n",
      "Training batch loss: 1610.295166 [20600/24965]\n",
      "Training batch loss: 1501.185303 [20700/24965]\n",
      "Training batch loss: 1467.336304 [20800/24965]\n",
      "Training batch loss: 1730.845215 [20900/24965]\n",
      "Training batch loss: 1621.004761 [21000/24965]\n",
      "Training batch loss: 1356.439575 [21100/24965]\n",
      "Training batch loss: 1364.284546 [21200/24965]\n",
      "Training batch loss: 1634.645996 [21300/24965]\n",
      "Training batch loss: 1486.981079 [21400/24965]\n",
      "Training batch loss: 1665.983276 [21500/24965]\n",
      "Training batch loss: 1417.603638 [21600/24965]\n",
      "Training batch loss: 1547.424194 [21700/24965]\n",
      "Training batch loss: 1353.505859 [21800/24965]\n",
      "Training batch loss: 1524.037842 [21900/24965]\n",
      "Training batch loss: 1595.254883 [22000/24965]\n",
      "Training batch loss: 1538.585327 [22100/24965]\n",
      "Training batch loss: 1353.669434 [22200/24965]\n",
      "Training batch loss: 1391.546875 [22300/24965]\n",
      "Training batch loss: 1527.258667 [22400/24965]\n",
      "Training batch loss: 1637.915283 [22500/24965]\n",
      "Training batch loss: 1500.499756 [22600/24965]\n",
      "Training batch loss: 1277.258789 [22700/24965]\n",
      "Training batch loss: 1340.349121 [22800/24965]\n",
      "Training batch loss: 1341.559570 [22900/24965]\n",
      "Training batch loss: 1506.619873 [23000/24965]\n",
      "Training batch loss: 1867.973633 [23100/24965]\n",
      "Training batch loss: 1568.253662 [23200/24965]\n",
      "Training batch loss: 1484.482422 [23300/24965]\n",
      "Training batch loss: 1475.895752 [23400/24965]\n",
      "Training batch loss: 1811.624634 [23500/24965]\n",
      "Training batch loss: 1266.808716 [23600/24965]\n",
      "Training batch loss: 1657.155396 [23700/24965]\n",
      "Training batch loss: 1395.441772 [23800/24965]\n",
      "Training batch loss: 1644.654053 [23900/24965]\n",
      "Training batch loss: 1894.051270 [24000/24965]\n",
      "Training batch loss: 1340.479248 [24100/24965]\n",
      "Training batch loss: 1544.874756 [24200/24965]\n",
      "Training batch loss: 1650.649902 [24300/24965]\n",
      "Training batch loss: 1295.087646 [24400/24965]\n",
      "Training batch loss: 1535.554810 [24500/24965]\n",
      "Training batch loss: 1540.798096 [24600/24965]\n",
      "Training batch loss: 1338.938232 [24700/24965]\n",
      "Training batch loss: 1738.154541 [24800/24965]\n",
      "Training batch loss: 1423.385254 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.663118\n",
      "\n",
      "Epoch 13\n",
      "----------------------\n",
      "Training batch loss: 1858.854858 [    0/24965]\n",
      "Training batch loss: 1630.807129 [  100/24965]\n",
      "Training batch loss: 1627.556152 [  200/24965]\n",
      "Training batch loss: 1315.750488 [  300/24965]\n",
      "Training batch loss: 1635.377808 [  400/24965]\n",
      "Training batch loss: 1512.384399 [  500/24965]\n",
      "Training batch loss: 1592.690918 [  600/24965]\n",
      "Training batch loss: 1421.221191 [  700/24965]\n",
      "Training batch loss: 1452.619629 [  800/24965]\n",
      "Training batch loss: 1479.207153 [  900/24965]\n",
      "Training batch loss: 1518.371948 [ 1000/24965]\n",
      "Training batch loss: 1428.046509 [ 1100/24965]\n",
      "Training batch loss: 1519.118042 [ 1200/24965]\n",
      "Training batch loss: 1540.118530 [ 1300/24965]\n",
      "Training batch loss: 1751.311523 [ 1400/24965]\n",
      "Training batch loss: 1507.005127 [ 1500/24965]\n",
      "Training batch loss: 1725.881226 [ 1600/24965]\n",
      "Training batch loss: 1658.102295 [ 1700/24965]\n",
      "Training batch loss: 1360.377441 [ 1800/24965]\n",
      "Training batch loss: 1448.498291 [ 1900/24965]\n",
      "Training batch loss: 1561.424072 [ 2000/24965]\n",
      "Training batch loss: 1565.765747 [ 2100/24965]\n",
      "Training batch loss: 1419.175659 [ 2200/24965]\n",
      "Training batch loss: 1623.908325 [ 2300/24965]\n",
      "Training batch loss: 1380.767578 [ 2400/24965]\n",
      "Training batch loss: 1569.501099 [ 2500/24965]\n",
      "Training batch loss: 1636.730591 [ 2600/24965]\n",
      "Training batch loss: 1548.351196 [ 2700/24965]\n",
      "Training batch loss: 1595.112549 [ 2800/24965]\n",
      "Training batch loss: 1685.203735 [ 2900/24965]\n",
      "Training batch loss: 1493.805908 [ 3000/24965]\n",
      "Training batch loss: 1622.092041 [ 3100/24965]\n",
      "Training batch loss: 1681.726074 [ 3200/24965]\n",
      "Training batch loss: 1545.609619 [ 3300/24965]\n",
      "Training batch loss: 1534.707764 [ 3400/24965]\n",
      "Training batch loss: 1650.973511 [ 3500/24965]\n",
      "Training batch loss: 1391.623657 [ 3600/24965]\n",
      "Training batch loss: 1241.473633 [ 3700/24965]\n",
      "Training batch loss: 1596.624268 [ 3800/24965]\n",
      "Training batch loss: 1562.844482 [ 3900/24965]\n",
      "Training batch loss: 1534.468872 [ 4000/24965]\n",
      "Training batch loss: 1825.240967 [ 4100/24965]\n",
      "Training batch loss: 1679.247803 [ 4200/24965]\n",
      "Training batch loss: 1716.830200 [ 4300/24965]\n",
      "Training batch loss: 1541.728516 [ 4400/24965]\n",
      "Training batch loss: 1767.629883 [ 4500/24965]\n",
      "Training batch loss: 1572.850952 [ 4600/24965]\n",
      "Training batch loss: 1532.925049 [ 4700/24965]\n",
      "Training batch loss: 1940.637573 [ 4800/24965]\n",
      "Training batch loss: 1743.065063 [ 4900/24965]\n",
      "Training batch loss: 1690.012451 [ 5000/24965]\n",
      "Training batch loss: 1540.262085 [ 5100/24965]\n",
      "Training batch loss: 1450.809814 [ 5200/24965]\n",
      "Training batch loss: 1426.277588 [ 5300/24965]\n",
      "Training batch loss: 1643.788330 [ 5400/24965]\n",
      "Training batch loss: 1493.200928 [ 5500/24965]\n",
      "Training batch loss: 1499.538696 [ 5600/24965]\n",
      "Training batch loss: 1349.890747 [ 5700/24965]\n",
      "Training batch loss: 1368.622925 [ 5800/24965]\n",
      "Training batch loss: 1687.120483 [ 5900/24965]\n",
      "Training batch loss: 1582.492310 [ 6000/24965]\n",
      "Training batch loss: 1604.322388 [ 6100/24965]\n",
      "Training batch loss: 1501.684326 [ 6200/24965]\n",
      "Training batch loss: 1529.226318 [ 6300/24965]\n",
      "Training batch loss: 1538.244507 [ 6400/24965]\n",
      "Training batch loss: 1564.644897 [ 6500/24965]\n",
      "Training batch loss: 1444.341919 [ 6600/24965]\n",
      "Training batch loss: 1474.434082 [ 6700/24965]\n",
      "Training batch loss: 1528.223145 [ 6800/24965]\n",
      "Training batch loss: 1294.087158 [ 6900/24965]\n",
      "Training batch loss: 1331.114136 [ 7000/24965]\n",
      "Training batch loss: 1750.759644 [ 7100/24965]\n",
      "Training batch loss: 1706.936890 [ 7200/24965]\n",
      "Training batch loss: 1370.602783 [ 7300/24965]\n",
      "Training batch loss: 1474.536987 [ 7400/24965]\n",
      "Training batch loss: 1750.836548 [ 7500/24965]\n",
      "Training batch loss: 1538.590210 [ 7600/24965]\n",
      "Training batch loss: 1572.866089 [ 7700/24965]\n",
      "Training batch loss: 1396.713623 [ 7800/24965]\n",
      "Training batch loss: 1512.600220 [ 7900/24965]\n",
      "Training batch loss: 1447.286865 [ 8000/24965]\n",
      "Training batch loss: 1580.350220 [ 8100/24965]\n",
      "Training batch loss: 1321.295898 [ 8200/24965]\n",
      "Training batch loss: 1592.142212 [ 8300/24965]\n",
      "Training batch loss: 1404.377808 [ 8400/24965]\n",
      "Training batch loss: 1604.873901 [ 8500/24965]\n",
      "Training batch loss: 1591.253418 [ 8600/24965]\n",
      "Training batch loss: 1697.537109 [ 8700/24965]\n",
      "Training batch loss: 1644.587646 [ 8800/24965]\n",
      "Training batch loss: 1307.725464 [ 8900/24965]\n",
      "Training batch loss: 1653.844727 [ 9000/24965]\n",
      "Training batch loss: 1649.465942 [ 9100/24965]\n",
      "Training batch loss: 1565.606934 [ 9200/24965]\n",
      "Training batch loss: 1503.758179 [ 9300/24965]\n",
      "Training batch loss: 1298.012817 [ 9400/24965]\n",
      "Training batch loss: 1467.864502 [ 9500/24965]\n",
      "Training batch loss: 1479.866943 [ 9600/24965]\n",
      "Training batch loss: 1456.358765 [ 9700/24965]\n",
      "Training batch loss: 1613.916748 [ 9800/24965]\n",
      "Training batch loss: 1529.588623 [ 9900/24965]\n",
      "Training batch loss: 1499.955200 [10000/24965]\n",
      "Training batch loss: 1633.132202 [10100/24965]\n",
      "Training batch loss: 1698.571167 [10200/24965]\n",
      "Training batch loss: 1526.652710 [10300/24965]\n",
      "Training batch loss: 1411.208496 [10400/24965]\n",
      "Training batch loss: 1491.435425 [10500/24965]\n",
      "Training batch loss: 1481.674927 [10600/24965]\n",
      "Training batch loss: 1729.894287 [10700/24965]\n",
      "Training batch loss: 1612.710083 [10800/24965]\n",
      "Training batch loss: 1446.166626 [10900/24965]\n",
      "Training batch loss: 1604.793579 [11000/24965]\n",
      "Training batch loss: 1655.578491 [11100/24965]\n",
      "Training batch loss: 1461.783447 [11200/24965]\n",
      "Training batch loss: 1549.989014 [11300/24965]\n",
      "Training batch loss: 1396.603394 [11400/24965]\n",
      "Training batch loss: 1430.548950 [11500/24965]\n",
      "Training batch loss: 1536.147095 [11600/24965]\n",
      "Training batch loss: 1769.619873 [11700/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1610.632080 [11800/24965]\n",
      "Training batch loss: 1539.104248 [11900/24965]\n",
      "Training batch loss: 1510.541260 [12000/24965]\n",
      "Training batch loss: 1494.773315 [12100/24965]\n",
      "Training batch loss: 1545.222534 [12200/24965]\n",
      "Training batch loss: 1461.443726 [12300/24965]\n",
      "Training batch loss: 1614.707275 [12400/24965]\n",
      "Training batch loss: 1450.517822 [12500/24965]\n",
      "Training batch loss: 1390.133057 [12600/24965]\n",
      "Training batch loss: 1664.048218 [12700/24965]\n",
      "Training batch loss: 1532.425537 [12800/24965]\n",
      "Training batch loss: 1579.309204 [12900/24965]\n",
      "Training batch loss: 1521.003784 [13000/24965]\n",
      "Training batch loss: 1634.213135 [13100/24965]\n",
      "Training batch loss: 1353.768677 [13200/24965]\n",
      "Training batch loss: 1469.040771 [13300/24965]\n",
      "Training batch loss: 1618.965088 [13400/24965]\n",
      "Training batch loss: 1658.867676 [13500/24965]\n",
      "Training batch loss: 1308.738281 [13600/24965]\n",
      "Training batch loss: 1360.612793 [13700/24965]\n",
      "Training batch loss: 1673.206055 [13800/24965]\n",
      "Training batch loss: 1375.543213 [13900/24965]\n",
      "Training batch loss: 1756.103027 [14000/24965]\n",
      "Training batch loss: 1617.126587 [14100/24965]\n",
      "Training batch loss: 1528.953003 [14200/24965]\n",
      "Training batch loss: 1518.896362 [14300/24965]\n",
      "Training batch loss: 1596.494141 [14400/24965]\n",
      "Training batch loss: 1363.350830 [14500/24965]\n",
      "Training batch loss: 1453.232666 [14600/24965]\n",
      "Training batch loss: 1305.031738 [14700/24965]\n",
      "Training batch loss: 1360.811401 [14800/24965]\n",
      "Training batch loss: 1738.929199 [14900/24965]\n",
      "Training batch loss: 1315.968628 [15000/24965]\n",
      "Training batch loss: 1470.704956 [15100/24965]\n",
      "Training batch loss: 1470.275635 [15200/24965]\n",
      "Training batch loss: 1831.379028 [15300/24965]\n",
      "Training batch loss: 1486.933350 [15400/24965]\n",
      "Training batch loss: 1448.033691 [15500/24965]\n",
      "Training batch loss: 1374.152710 [15600/24965]\n",
      "Training batch loss: 1570.569458 [15700/24965]\n",
      "Training batch loss: 1502.671265 [15800/24965]\n",
      "Training batch loss: 1555.636353 [15900/24965]\n",
      "Training batch loss: 1464.447876 [16000/24965]\n",
      "Training batch loss: 1406.433350 [16100/24965]\n",
      "Training batch loss: 1583.385498 [16200/24965]\n",
      "Training batch loss: 1441.739868 [16300/24965]\n",
      "Training batch loss: 1427.066895 [16400/24965]\n",
      "Training batch loss: 1509.313354 [16500/24965]\n",
      "Training batch loss: 1441.957886 [16600/24965]\n",
      "Training batch loss: 1660.083374 [16700/24965]\n",
      "Training batch loss: 1596.466187 [16800/24965]\n",
      "Training batch loss: 1571.479736 [16900/24965]\n",
      "Training batch loss: 1399.573486 [17000/24965]\n",
      "Training batch loss: 1314.804810 [17100/24965]\n",
      "Training batch loss: 1531.890137 [17200/24965]\n",
      "Training batch loss: 1539.376221 [17300/24965]\n",
      "Training batch loss: 1446.368286 [17400/24965]\n",
      "Training batch loss: 1537.757812 [17500/24965]\n",
      "Training batch loss: 1820.726318 [17600/24965]\n",
      "Training batch loss: 1651.740845 [17700/24965]\n",
      "Training batch loss: 1571.202881 [17800/24965]\n",
      "Training batch loss: 1595.711182 [17900/24965]\n",
      "Training batch loss: 1273.430664 [18000/24965]\n",
      "Training batch loss: 1192.305908 [18100/24965]\n",
      "Training batch loss: 1326.073364 [18200/24965]\n",
      "Training batch loss: 1826.587280 [18300/24965]\n",
      "Training batch loss: 1738.598389 [18400/24965]\n",
      "Training batch loss: 1343.105713 [18500/24965]\n",
      "Training batch loss: 1603.653198 [18600/24965]\n",
      "Training batch loss: 1412.637695 [18700/24965]\n",
      "Training batch loss: 1592.183594 [18800/24965]\n",
      "Training batch loss: 1687.112305 [18900/24965]\n",
      "Training batch loss: 1612.930420 [19000/24965]\n",
      "Training batch loss: 1486.189453 [19100/24965]\n",
      "Training batch loss: 1597.004028 [19200/24965]\n",
      "Training batch loss: 1618.294800 [19300/24965]\n",
      "Training batch loss: 1498.606567 [19400/24965]\n",
      "Training batch loss: 1615.527710 [19500/24965]\n",
      "Training batch loss: 1484.447876 [19600/24965]\n",
      "Training batch loss: 1617.186768 [19700/24965]\n",
      "Training batch loss: 1292.469482 [19800/24965]\n",
      "Training batch loss: 1418.093018 [19900/24965]\n",
      "Training batch loss: 1478.941406 [20000/24965]\n",
      "Training batch loss: 1671.323608 [20100/24965]\n",
      "Training batch loss: 1444.686157 [20200/24965]\n",
      "Training batch loss: 1531.061646 [20300/24965]\n",
      "Training batch loss: 1252.301025 [20400/24965]\n",
      "Training batch loss: 1575.055664 [20500/24965]\n",
      "Training batch loss: 1473.091919 [20600/24965]\n",
      "Training batch loss: 1504.744995 [20700/24965]\n",
      "Training batch loss: 1718.599365 [20800/24965]\n",
      "Training batch loss: 1659.093018 [20900/24965]\n",
      "Training batch loss: 1442.028809 [21000/24965]\n",
      "Training batch loss: 1573.084106 [21100/24965]\n",
      "Training batch loss: 1570.937988 [21200/24965]\n",
      "Training batch loss: 1448.838135 [21300/24965]\n",
      "Training batch loss: 1446.057983 [21400/24965]\n",
      "Training batch loss: 1390.006836 [21500/24965]\n",
      "Training batch loss: 1558.888916 [21600/24965]\n",
      "Training batch loss: 1517.938477 [21700/24965]\n",
      "Training batch loss: 1437.782349 [21800/24965]\n",
      "Training batch loss: 1736.338623 [21900/24965]\n",
      "Training batch loss: 1572.046387 [22000/24965]\n",
      "Training batch loss: 1461.397461 [22100/24965]\n",
      "Training batch loss: 1659.053467 [22200/24965]\n",
      "Training batch loss: 1800.920166 [22300/24965]\n",
      "Training batch loss: 1596.760254 [22400/24965]\n",
      "Training batch loss: 1527.307251 [22500/24965]\n",
      "Training batch loss: 1667.784912 [22600/24965]\n",
      "Training batch loss: 1579.705811 [22700/24965]\n",
      "Training batch loss: 1575.545166 [22800/24965]\n",
      "Training batch loss: 1469.959717 [22900/24965]\n",
      "Training batch loss: 1466.986206 [23000/24965]\n",
      "Training batch loss: 1566.365845 [23100/24965]\n",
      "Training batch loss: 1508.746582 [23200/24965]\n",
      "Training batch loss: 1354.946045 [23300/24965]\n",
      "Training batch loss: 1533.628296 [23400/24965]\n",
      "Training batch loss: 1579.921265 [23500/24965]\n",
      "Training batch loss: 1443.621338 [23600/24965]\n",
      "Training batch loss: 1498.159180 [23700/24965]\n",
      "Training batch loss: 1407.144531 [23800/24965]\n",
      "Training batch loss: 1750.079224 [23900/24965]\n",
      "Training batch loss: 1510.454590 [24000/24965]\n",
      "Training batch loss: 1604.298340 [24100/24965]\n",
      "Training batch loss: 1534.546265 [24200/24965]\n",
      "Training batch loss: 1724.788574 [24300/24965]\n",
      "Training batch loss: 1645.755371 [24400/24965]\n",
      "Training batch loss: 1651.115112 [24500/24965]\n",
      "Training batch loss: 1868.789429 [24600/24965]\n",
      "Training batch loss: 1475.482056 [24700/24965]\n",
      "Training batch loss: 1506.622070 [24800/24965]\n",
      "Training batch loss: 1592.209473 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.688629\n",
      "\n",
      "Epoch 14\n",
      "----------------------\n",
      "Training batch loss: 1476.605713 [    0/24965]\n",
      "Training batch loss: 1477.287964 [  100/24965]\n",
      "Training batch loss: 1720.233887 [  200/24965]\n",
      "Training batch loss: 1576.339111 [  300/24965]\n",
      "Training batch loss: 1708.795898 [  400/24965]\n",
      "Training batch loss: 1510.528076 [  500/24965]\n",
      "Training batch loss: 1482.051147 [  600/24965]\n",
      "Training batch loss: 1482.013184 [  700/24965]\n",
      "Training batch loss: 1456.447021 [  800/24965]\n",
      "Training batch loss: 1601.503418 [  900/24965]\n",
      "Training batch loss: 1667.724121 [ 1000/24965]\n",
      "Training batch loss: 1483.488037 [ 1100/24965]\n",
      "Training batch loss: 1707.928345 [ 1200/24965]\n",
      "Training batch loss: 1752.821411 [ 1300/24965]\n",
      "Training batch loss: 1461.658081 [ 1400/24965]\n",
      "Training batch loss: 1383.661011 [ 1500/24965]\n",
      "Training batch loss: 1346.490234 [ 1600/24965]\n",
      "Training batch loss: 1482.687134 [ 1700/24965]\n",
      "Training batch loss: 1539.125488 [ 1800/24965]\n",
      "Training batch loss: 1576.419189 [ 1900/24965]\n",
      "Training batch loss: 1427.048584 [ 2000/24965]\n",
      "Training batch loss: 1287.096313 [ 2100/24965]\n",
      "Training batch loss: 1519.365967 [ 2200/24965]\n",
      "Training batch loss: 1363.346558 [ 2300/24965]\n",
      "Training batch loss: 1328.525391 [ 2400/24965]\n",
      "Training batch loss: 1587.397827 [ 2500/24965]\n",
      "Training batch loss: 1526.904297 [ 2600/24965]\n",
      "Training batch loss: 1488.137939 [ 2700/24965]\n",
      "Training batch loss: 1434.427124 [ 2800/24965]\n",
      "Training batch loss: 1620.747681 [ 2900/24965]\n",
      "Training batch loss: 1546.210083 [ 3000/24965]\n",
      "Training batch loss: 1307.114502 [ 3100/24965]\n",
      "Training batch loss: 1321.502563 [ 3200/24965]\n",
      "Training batch loss: 1352.412598 [ 3300/24965]\n",
      "Training batch loss: 1321.165527 [ 3400/24965]\n",
      "Training batch loss: 1499.824951 [ 3500/24965]\n",
      "Training batch loss: 1560.634033 [ 3600/24965]\n",
      "Training batch loss: 1469.248047 [ 3700/24965]\n",
      "Training batch loss: 1449.266357 [ 3800/24965]\n",
      "Training batch loss: 1545.475830 [ 3900/24965]\n",
      "Training batch loss: 1774.532837 [ 4000/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1631.617188 [ 4100/24965]\n",
      "Training batch loss: 1641.433228 [ 4200/24965]\n",
      "Training batch loss: 1584.925781 [ 4300/24965]\n",
      "Training batch loss: 1526.925659 [ 4400/24965]\n",
      "Training batch loss: 1396.666870 [ 4500/24965]\n",
      "Training batch loss: 1242.057739 [ 4600/24965]\n",
      "Training batch loss: 1660.438843 [ 4700/24965]\n",
      "Training batch loss: 1497.468384 [ 4800/24965]\n",
      "Training batch loss: 1501.564453 [ 4900/24965]\n",
      "Training batch loss: 1505.325195 [ 5000/24965]\n",
      "Training batch loss: 1508.386963 [ 5100/24965]\n",
      "Training batch loss: 1561.161743 [ 5200/24965]\n",
      "Training batch loss: 1545.519043 [ 5300/24965]\n",
      "Training batch loss: 1452.749756 [ 5400/24965]\n",
      "Training batch loss: 1504.237549 [ 5500/24965]\n",
      "Training batch loss: 1488.012939 [ 5600/24965]\n",
      "Training batch loss: 1568.400024 [ 5700/24965]\n",
      "Training batch loss: 1608.726807 [ 5800/24965]\n",
      "Training batch loss: 1481.857788 [ 5900/24965]\n",
      "Training batch loss: 1537.159180 [ 6000/24965]\n",
      "Training batch loss: 1610.009033 [ 6100/24965]\n",
      "Training batch loss: 1756.992798 [ 6200/24965]\n",
      "Training batch loss: 1396.374390 [ 6300/24965]\n",
      "Training batch loss: 1548.331177 [ 6400/24965]\n",
      "Training batch loss: 1432.387207 [ 6500/24965]\n",
      "Training batch loss: 1505.438965 [ 6600/24965]\n",
      "Training batch loss: 1571.734741 [ 6700/24965]\n",
      "Training batch loss: 1639.504028 [ 6800/24965]\n",
      "Training batch loss: 1541.867432 [ 6900/24965]\n",
      "Training batch loss: 1725.764648 [ 7000/24965]\n",
      "Training batch loss: 1390.345703 [ 7100/24965]\n",
      "Training batch loss: 1626.253906 [ 7200/24965]\n",
      "Training batch loss: 1500.057861 [ 7300/24965]\n",
      "Training batch loss: 1719.648071 [ 7400/24965]\n",
      "Training batch loss: 1413.771729 [ 7500/24965]\n",
      "Training batch loss: 1440.726074 [ 7600/24965]\n",
      "Training batch loss: 1484.536621 [ 7700/24965]\n",
      "Training batch loss: 1553.599365 [ 7800/24965]\n",
      "Training batch loss: 1567.908081 [ 7900/24965]\n",
      "Training batch loss: 1655.449219 [ 8000/24965]\n",
      "Training batch loss: 1510.331421 [ 8100/24965]\n",
      "Training batch loss: 1622.352539 [ 8200/24965]\n",
      "Training batch loss: 1477.854858 [ 8300/24965]\n",
      "Training batch loss: 1730.419067 [ 8400/24965]\n",
      "Training batch loss: 1420.468872 [ 8500/24965]\n",
      "Training batch loss: 1433.963989 [ 8600/24965]\n",
      "Training batch loss: 1635.181030 [ 8700/24965]\n",
      "Training batch loss: 1389.550415 [ 8800/24965]\n",
      "Training batch loss: 1538.342407 [ 8900/24965]\n",
      "Training batch loss: 1426.416870 [ 9000/24965]\n",
      "Training batch loss: 1348.234131 [ 9100/24965]\n",
      "Training batch loss: 1453.310303 [ 9200/24965]\n",
      "Training batch loss: 1423.755981 [ 9300/24965]\n",
      "Training batch loss: 1596.930054 [ 9400/24965]\n",
      "Training batch loss: 1346.690308 [ 9500/24965]\n",
      "Training batch loss: 1773.088013 [ 9600/24965]\n",
      "Training batch loss: 1423.022827 [ 9700/24965]\n",
      "Training batch loss: 1479.022095 [ 9800/24965]\n",
      "Training batch loss: 1452.132568 [ 9900/24965]\n",
      "Training batch loss: 1488.960815 [10000/24965]\n",
      "Training batch loss: 1497.838135 [10100/24965]\n",
      "Training batch loss: 1579.694946 [10200/24965]\n",
      "Training batch loss: 1414.499146 [10300/24965]\n",
      "Training batch loss: 1540.230469 [10400/24965]\n",
      "Training batch loss: 1505.030518 [10500/24965]\n",
      "Training batch loss: 1470.904663 [10600/24965]\n",
      "Training batch loss: 1586.634399 [10700/24965]\n",
      "Training batch loss: 1490.537842 [10800/24965]\n",
      "Training batch loss: 1566.089722 [10900/24965]\n",
      "Training batch loss: 1570.965210 [11000/24965]\n",
      "Training batch loss: 1679.721680 [11100/24965]\n",
      "Training batch loss: 1702.192383 [11200/24965]\n",
      "Training batch loss: 1399.418823 [11300/24965]\n",
      "Training batch loss: 1658.615601 [11400/24965]\n",
      "Training batch loss: 1551.453857 [11500/24965]\n",
      "Training batch loss: 1610.926147 [11600/24965]\n",
      "Training batch loss: 1371.449829 [11700/24965]\n",
      "Training batch loss: 1712.213501 [11800/24965]\n",
      "Training batch loss: 1481.055908 [11900/24965]\n",
      "Training batch loss: 1621.972778 [12000/24965]\n",
      "Training batch loss: 1543.049072 [12100/24965]\n",
      "Training batch loss: 1494.377686 [12200/24965]\n",
      "Training batch loss: 1528.508179 [12300/24965]\n",
      "Training batch loss: 1596.282349 [12400/24965]\n",
      "Training batch loss: 1677.750488 [12500/24965]\n",
      "Training batch loss: 1160.708252 [12600/24965]\n",
      "Training batch loss: 1493.706543 [12700/24965]\n",
      "Training batch loss: 1670.704346 [12800/24965]\n",
      "Training batch loss: 1252.154175 [12900/24965]\n",
      "Training batch loss: 1576.203003 [13000/24965]\n",
      "Training batch loss: 1576.251465 [13100/24965]\n",
      "Training batch loss: 1410.161987 [13200/24965]\n",
      "Training batch loss: 1399.986572 [13300/24965]\n",
      "Training batch loss: 1818.508789 [13400/24965]\n",
      "Training batch loss: 1542.844360 [13500/24965]\n",
      "Training batch loss: 1374.098633 [13600/24965]\n",
      "Training batch loss: 1576.134644 [13700/24965]\n",
      "Training batch loss: 1331.534424 [13800/24965]\n",
      "Training batch loss: 1448.339355 [13900/24965]\n",
      "Training batch loss: 1401.392212 [14000/24965]\n",
      "Training batch loss: 1347.988159 [14100/24965]\n",
      "Training batch loss: 1502.094727 [14200/24965]\n",
      "Training batch loss: 1459.096680 [14300/24965]\n",
      "Training batch loss: 1644.619873 [14400/24965]\n",
      "Training batch loss: 1336.503784 [14500/24965]\n",
      "Training batch loss: 1712.982178 [14600/24965]\n",
      "Training batch loss: 1495.388672 [14700/24965]\n",
      "Training batch loss: 1529.252930 [14800/24965]\n",
      "Training batch loss: 1493.963501 [14900/24965]\n",
      "Training batch loss: 1560.947510 [15000/24965]\n",
      "Training batch loss: 1444.773682 [15100/24965]\n",
      "Training batch loss: 1532.799805 [15200/24965]\n",
      "Training batch loss: 1517.565796 [15300/24965]\n",
      "Training batch loss: 1546.581177 [15400/24965]\n",
      "Training batch loss: 1645.766724 [15500/24965]\n",
      "Training batch loss: 1325.095459 [15600/24965]\n",
      "Training batch loss: 1489.229614 [15700/24965]\n",
      "Training batch loss: 1260.307617 [15800/24965]\n",
      "Training batch loss: 1791.566895 [15900/24965]\n",
      "Training batch loss: 1652.378662 [16000/24965]\n",
      "Training batch loss: 1646.524780 [16100/24965]\n",
      "Training batch loss: 1475.579346 [16200/24965]\n",
      "Training batch loss: 1555.410645 [16300/24965]\n",
      "Training batch loss: 1499.250977 [16400/24965]\n",
      "Training batch loss: 1600.241455 [16500/24965]\n",
      "Training batch loss: 1493.806030 [16600/24965]\n",
      "Training batch loss: 1437.071045 [16700/24965]\n",
      "Training batch loss: 1510.296265 [16800/24965]\n",
      "Training batch loss: 1661.403564 [16900/24965]\n",
      "Training batch loss: 1448.742065 [17000/24965]\n",
      "Training batch loss: 1314.120117 [17100/24965]\n",
      "Training batch loss: 1283.235718 [17200/24965]\n",
      "Training batch loss: 1678.039307 [17300/24965]\n",
      "Training batch loss: 1674.267090 [17400/24965]\n",
      "Training batch loss: 1328.841797 [17500/24965]\n",
      "Training batch loss: 1479.289673 [17600/24965]\n",
      "Training batch loss: 1568.661621 [17700/24965]\n",
      "Training batch loss: 1552.268921 [17800/24965]\n",
      "Training batch loss: 1422.591309 [17900/24965]\n",
      "Training batch loss: 1659.273193 [18000/24965]\n",
      "Training batch loss: 1525.854492 [18100/24965]\n",
      "Training batch loss: 1359.946777 [18200/24965]\n",
      "Training batch loss: 1503.895020 [18300/24965]\n",
      "Training batch loss: 1668.036255 [18400/24965]\n",
      "Training batch loss: 1466.413940 [18500/24965]\n",
      "Training batch loss: 1818.136475 [18600/24965]\n",
      "Training batch loss: 1534.007935 [18700/24965]\n",
      "Training batch loss: 1638.363037 [18800/24965]\n",
      "Training batch loss: 1517.881226 [18900/24965]\n",
      "Training batch loss: 1367.043701 [19000/24965]\n",
      "Training batch loss: 1668.407715 [19100/24965]\n",
      "Training batch loss: 1458.408813 [19200/24965]\n",
      "Training batch loss: 1453.124390 [19300/24965]\n",
      "Training batch loss: 1516.778564 [19400/24965]\n",
      "Training batch loss: 1637.157959 [19500/24965]\n",
      "Training batch loss: 1371.129395 [19600/24965]\n",
      "Training batch loss: 1404.723022 [19700/24965]\n",
      "Training batch loss: 1453.245728 [19800/24965]\n",
      "Training batch loss: 1552.762451 [19900/24965]\n",
      "Training batch loss: 1494.959717 [20000/24965]\n",
      "Training batch loss: 1491.713745 [20100/24965]\n",
      "Training batch loss: 1545.593506 [20200/24965]\n",
      "Training batch loss: 1634.053833 [20300/24965]\n",
      "Training batch loss: 1534.942017 [20400/24965]\n",
      "Training batch loss: 1643.085083 [20500/24965]\n",
      "Training batch loss: 1801.300049 [20600/24965]\n",
      "Training batch loss: 1624.452515 [20700/24965]\n",
      "Training batch loss: 1577.002563 [20800/24965]\n",
      "Training batch loss: 1512.645874 [20900/24965]\n",
      "Training batch loss: 1457.984375 [21000/24965]\n",
      "Training batch loss: 1606.914673 [21100/24965]\n",
      "Training batch loss: 1533.517822 [21200/24965]\n",
      "Training batch loss: 1708.878174 [21300/24965]\n",
      "Training batch loss: 1493.335205 [21400/24965]\n",
      "Training batch loss: 1439.072510 [21500/24965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 1429.528442 [21600/24965]\n",
      "Training batch loss: 1841.033813 [21700/24965]\n",
      "Training batch loss: 1626.209473 [21800/24965]\n",
      "Training batch loss: 1340.996216 [21900/24965]\n",
      "Training batch loss: 1610.324463 [22000/24965]\n",
      "Training batch loss: 1452.061279 [22100/24965]\n",
      "Training batch loss: 1600.983887 [22200/24965]\n",
      "Training batch loss: 1647.770508 [22300/24965]\n",
      "Training batch loss: 1415.121338 [22400/24965]\n",
      "Training batch loss: 1481.910278 [22500/24965]\n",
      "Training batch loss: 1481.324585 [22600/24965]\n",
      "Training batch loss: 1611.178101 [22700/24965]\n",
      "Training batch loss: 1400.018555 [22800/24965]\n",
      "Training batch loss: 1375.686646 [22900/24965]\n",
      "Training batch loss: 1478.522095 [23000/24965]\n",
      "Training batch loss: 1407.957031 [23100/24965]\n",
      "Training batch loss: 1466.532471 [23200/24965]\n",
      "Training batch loss: 1640.520874 [23300/24965]\n",
      "Training batch loss: 1582.032593 [23400/24965]\n",
      "Training batch loss: 1367.555786 [23500/24965]\n",
      "Training batch loss: 1635.899414 [23600/24965]\n",
      "Training batch loss: 1476.178589 [23700/24965]\n",
      "Training batch loss: 1604.626221 [23800/24965]\n",
      "Training batch loss: 1207.365967 [23900/24965]\n",
      "Training batch loss: 1587.569092 [24000/24965]\n",
      "Training batch loss: 1702.501221 [24100/24965]\n",
      "Training batch loss: 1569.270996 [24200/24965]\n",
      "Training batch loss: 1564.643188 [24300/24965]\n",
      "Training batch loss: 1505.434082 [24400/24965]\n",
      "Training batch loss: 1573.275513 [24500/24965]\n",
      "Training batch loss: 1435.680664 [24600/24965]\n",
      "Training batch loss: 1532.606079 [24700/24965]\n",
      "Training batch loss: 1362.454590 [24800/24965]\n",
      "Training batch loss: 1520.220703 [24900/24965]\n",
      "\n",
      "Valdidation average loss: 1512.696135\n",
      "\n",
      "Early stopping triggered after 14 epochs\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "indicies = np.arange(len(train_dataset))\n",
    "train_indicies, val_indicies = train_test_split(indicies, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indicies)\n",
    "val_sampler = SubsetRandomSampler(val_indicies)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "my_model = BackpackPriceNet(train_dataset.num_categories)\n",
    "device = torch.device('cpu')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.2, patience=2)\n",
    "history = defaultdict(list)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_model.pth\"\n",
    "\n",
    "patience = 3\n",
    "early_stopping_counter = 0\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n----------------------\")\n",
    "    train_loss = train_loop(train_loader, my_model, loss_fn, optimizer)\n",
    "    val_loss = val_loop(val_loader, my_model, loss_fn)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        early_stopping_counter = 0\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(my_model.state_dict(), best_model_path)\n",
    "        print(f\"Saved best new model with val_loss: {val_loss:.4f}\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.load_state_dict(torch.load(best_model_path))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "predictions = predict(test_loader, my_model)\n",
    "test_predictions_df = pd.DataFrame({\n",
    "    'id': test_dataset.data['id'],\n",
    "    'Price': predictions.flatten()\n",
    "})\n",
    "test_predictions_df.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "-.venv",
   "language": "python",
   "name": "-.venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
